{"fetched_at": "2025-07-28T18:20:02Z", "published": "Mon, 28 Jul 2025 18:19:49 GMT", "package": "corvic-engine", "version": "0.3.0rc104", "json": {"info": {"author": null, "author_email": "Corvic Team <contact@corvic.ai>", "bugtrack_url": null, "classifiers": ["Environment :: Console", "License :: Other/Proprietary License", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy", "Programming Language :: Rust", "Topic :: Scientific/Engineering"], "description": "# Corvic Engine [![CI](https://github.com/corvicai/corvic-engine/actions/workflows/ci.yaml/badge.svg)](https://github.com/corvicai/corvic-engine/actions/workflows/ci.yaml) [![Python Version](https://img.shields.io/badge/python-3.11%20|%203.12-blue)](https://www.python.org/downloads/release/python-3110/)\n\n", "description_content_type": "text/markdown; charset=UTF-8; variant=GFM", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "corvic-engine", "package_url": "https://pypi.org/project/corvic-engine/", "platform": null, "project_url": "https://pypi.org/project/corvic-engine/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/corvic-engine/0.3.0rc104/", "requires_dist": ["aiosqlite>=0.21", "cachetools>=5", "duckdb>=1.0.0", "more-itertools>=10", "numpy>=1.26", "polars>=1.7.1", "protobuf>=4.25", "protovalidate>=0.3", "pyarrow>=20", "sqlalchemy>=2", "sqlglot<27,>=26", "structlog>=24", "tqdm", "umap-learn>=0.5.5; extra == \"ml\"", "pillow>=10.0.0; extra == \"ml\"", "scikit-learn>=1.4.0; extra == \"ml\"", "transformers[torch]>=4.45.0; extra == \"ml\"", "sentencepiece>=0.2.0; extra == \"ml\"", "torchvision==0.22.1; extra == \"ml\"", "opentelemetry-api>=1.20.0; extra == \"telemetry\"", "opentelemetry-sdk>=1.20.0; extra == \"telemetry\""], "requires_python": ">=3.11", "summary": "Seamless embedding generation and retrieval.", "version": "0.3.0rc104", "yanked": false, "yanked_reason": null}, "last_serial": 30388610, "urls": [{"comment_text": null, "digests": {"blake2b_256": "f1987343d4bcba793d962410b2a17f6ba642c1c95b0f1891c91843906997c531", "md5": "c1a450f71c4d74ce52e44252388a4d01", "sha256": "a48ff5055eecf9fdf0926e8b64cffa9bbf0786815237f69f8162185db37f0cfe"}, "downloads": -1, "filename": "corvic_engine-0.3.0rc104-cp38-abi3-macosx_10_12_x86_64.whl", "has_sig": false, "md5_digest": "c1a450f71c4d74ce52e44252388a4d01", "packagetype": "bdist_wheel", "python_version": "cp38", "requires_python": ">=3.11", "size": 610158, "upload_time": "2025-07-28T18:19:49", "upload_time_iso_8601": "2025-07-28T18:19:49.376992Z", "url": "https://files.pythonhosted.org/packages/f1/98/7343d4bcba793d962410b2a17f6ba642c1c95b0f1891c91843906997c531/corvic_engine-0.3.0rc104-cp38-abi3-macosx_10_12_x86_64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "9dcbd2b125292196d2602ad1eaa907cc8e5a8e5f3452b09bee4787864d3e25b4", "md5": "de6c96c22686b7a4fe2a9a29fdc8ffb5", "sha256": "e6468f117d424c80763ff7dd4c9b582aa26291a867cbe3ea1c38a4efe79ce268"}, "downloads": -1, "filename": "corvic_engine-0.3.0rc104-cp38-abi3-macosx_11_0_arm64.whl", "has_sig": false, "md5_digest": "de6c96c22686b7a4fe2a9a29fdc8ffb5", "packagetype": "bdist_wheel", "python_version": "cp38", "requires_python": ">=3.11", "size": 600844, "upload_time": "2025-07-28T18:19:51", "upload_time_iso_8601": "2025-07-28T18:19:51.435499Z", "url": "https://files.pythonhosted.org/packages/9d/cb/d2b125292196d2602ad1eaa907cc8e5a8e5f3452b09bee4787864d3e25b4/corvic_engine-0.3.0rc104-cp38-abi3-macosx_11_0_arm64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "4277bc7c58e062f24b85e4b69fd1831131e81d8f08894bd980edc747f6b53428", "md5": "df98356ff8b0d04e4ee14f18ad1dce77", "sha256": "95ca1efc55f3474b47ef22e5f52ba41587ceedcd8870b70888166610224a79aa"}, "downloads": -1, "filename": "corvic_engine-0.3.0rc104-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", "has_sig": false, "md5_digest": "df98356ff8b0d04e4ee14f18ad1dce77", "packagetype": "bdist_wheel", "python_version": "cp38", "requires_python": ">=3.11", "size": 649337, "upload_time": "2025-07-28T18:19:52", "upload_time_iso_8601": "2025-07-28T18:19:52.692170Z", "url": "https://files.pythonhosted.org/packages/42/77/bc7c58e062f24b85e4b69fd1831131e81d8f08894bd980edc747f6b53428/corvic_engine-0.3.0rc104-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "2047a768b6473d878d90d9f539f72f27273c4efc12ce7ce5737bda697aa31c2f", "md5": "847b8222336f99efa8263739e74163f4", "sha256": "0c66d030ab17c1989e07ef4a58fb792ca8a9cb9533b011b339d1416a2ddabeda"}, "downloads": -1, "filename": "corvic_engine-0.3.0rc104-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", "has_sig": false, "md5_digest": "847b8222336f99efa8263739e74163f4", "packagetype": "bdist_wheel", "python_version": "cp38", "requires_python": ">=3.11", "size": 642480, "upload_time": "2025-07-28T18:19:54", "upload_time_iso_8601": "2025-07-28T18:19:54.170856Z", "url": "https://files.pythonhosted.org/packages/20/47/a768b6473d878d90d9f539f72f27273c4efc12ce7ce5737bda697aa31c2f/corvic_engine-0.3.0rc104-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "a96c129aad6a15c13d9c870a5d90bf9e9fff5083c113dc81fed1921523a4ff95", "md5": "919c336787e73f62ba92221de412cc21", "sha256": "a43f02a29051c39ed539d36edd47c6b8634617f288670aa1e3dbd15d663bd552"}, "downloads": -1, "filename": "corvic_engine-0.3.0rc104-cp38-abi3-win_amd64.whl", "has_sig": false, "md5_digest": "919c336787e73f62ba92221de412cc21", "packagetype": "bdist_wheel", "python_version": "cp38", "requires_python": ">=3.11", "size": 513838, "upload_time": "2025-07-28T18:19:55", "upload_time_iso_8601": "2025-07-28T18:19:55.390497Z", "url": "https://files.pythonhosted.org/packages/a9/6c/129aad6a15c13d9c870a5d90bf9e9fff5083c113dc81fed1921523a4ff95/corvic_engine-0.3.0rc104-cp38-abi3-win_amd64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "e13880896d273908f35f5a9ca105b2c81a2a0043dc1a01b96c9af73b9beeb393", "md5": "bd09940e7b56d202a1ec8639e6668b6c", "sha256": "cc57c138e40c5d2f2c5d561b76efee0a795373c6fd36f5f2369c72f037c1827b"}, "downloads": -1, "filename": "corvic_engine-0.3.0rc104.tar.gz", "has_sig": false, "md5_digest": "bd09940e7b56d202a1ec8639e6668b6c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.11", "size": 241090, "upload_time": "2025-07-28T18:19:56", "upload_time_iso_8601": "2025-07-28T18:19:56.503417Z", "url": "https://files.pythonhosted.org/packages/e1/38/80896d273908f35f5a9ca105b2c81a2a0043dc1a01b96c9af73b9beeb393/corvic_engine-0.3.0rc104.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:19:45 GMT", "package": "pilot-platform-common", "version": "2.12.3", "json": {"info": {"author": "Indoc Systems", "author_email": "support@indocsystems.com", "bugtrack_url": null, "classifiers": ["Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12"], "description": "# common\n\n[![Run Tests](https://github.com/PilotDataPlatform/common/actions/workflows/run-tests.yml/badge.svg?branch=develop)](https://github.com/PilotDataPlatform/common/actions/workflows/run-tests.yml)\n[![Python](https://img.shields.io/badge/python-3.12-brightgreen.svg)](https://www.python.org/)\n[![PyPI](https://img.shields.io/pypi/v/pilot-platform-common.svg)](https://pypi.org/project/pilot-platform-common/)\n\nImportable package responsible for cross-service tasks within the Pilot Platform (e.g. logging, Vault connection, etc.).\n\n\n## Getting Started\n\n### Installation & Quick Start\nThe latest version of the common package is available on [PyPi](https://pypi.org/project/pilot-platform-common/) and can be installed into another service via Pip.\n\nPip install from PyPi:\n```\npip install pilot-platform-common\n```\n\nIn `pyproject.toml`:\n```\npilot-platform-common = \"^<VERSION>\"\n```\n\nPip install from a local `.whl` file:\n```\npip install pilot_platform_common-<VERSION>-py3-none-any.whl\n```\n\n## Contribution\n\nYou can contribute the project in following ways:\n\n* Report a bug.\n* Suggest a feature.\n* Open a pull request for fixing issues or adding functionality. Please consider using [pre-commit](https://pre-commit.com) in this case.\n* For general guidelines on how to contribute to the project, please take a look at the [contribution guide](CONTRIBUTING.md).\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "pilot-platform-common", "package_url": "https://pypi.org/project/pilot-platform-common/", "platform": null, "project_url": "https://pypi.org/project/pilot-platform-common/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/pilot-platform-common/2.12.3/", "requires_dist": ["aioboto3<15.0.0,>=14.3.0", "cryptography<45.0.0,>=44.0.2", "dicttoxml<2.0.0,>=1.7.16", "httpx<0.29.0,>=0.28.1", "minio<8.0.0,>=7.2.15", "opentelemetry-instrumentation-fastapi<0.54,>=0.53b1", "opentelemetry-sdk<2.0.0,>=1.32.1", "pydantic<3.0.0,>=2.7.1", "pyjwt<3.0.0,>=2.10.1", "python-dotenv>=0.19.1", "python-json-logger<=2.02,>=0.1.11", "redis<7.0.0,>=4.5.0", "starlette<0.47.0,>=0.40.0", "xmltodict<0.15.0,>=0.14.2"], "requires_python": "<3.13,>=3.10", "summary": "Common package used by various Pilot microservices.", "version": "2.12.3", "yanked": false, "yanked_reason": null}, "last_serial": 30388596, "urls": [{"comment_text": null, "digests": {"blake2b_256": "58a0284ea58e302e07771e8d12cebafb372d66b2c987842d0a3c971f3c168ac2", "md5": "7bda1dfceaa18f19ca6df36af705cd23", "sha256": "fba84ea473f6e4b4d8f71168414dd09469f9e8584689034097763687e64d8100"}, "downloads": -1, "filename": "pilot_platform_common-2.12.3-py3-none-any.whl", "has_sig": false, "md5_digest": "7bda1dfceaa18f19ca6df36af705cd23", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<3.13,>=3.10", "size": 31221, "upload_time": "2025-07-28T18:19:45", "upload_time_iso_8601": "2025-07-28T18:19:45.815067Z", "url": "https://files.pythonhosted.org/packages/58/a0/284ea58e302e07771e8d12cebafb372d66b2c987842d0a3c971f3c168ac2/pilot_platform_common-2.12.3-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "659d2b032066a910d944f1f6177136453aaafa756f8eef49c8820b5f03f1d6c8", "md5": "611b3b98517704e2493fc20412835117", "sha256": "0fd7ff9ea3d5305cfdfd862ff80fc4c2c891fe54d5edde9d3a505770bb0099e3"}, "downloads": -1, "filename": "pilot_platform_common-2.12.3.tar.gz", "has_sig": false, "md5_digest": "611b3b98517704e2493fc20412835117", "packagetype": "sdist", "python_version": "source", "requires_python": "<3.13,>=3.10", "size": 19576, "upload_time": "2025-07-28T18:19:46", "upload_time_iso_8601": "2025-07-28T18:19:46.723505Z", "url": "https://files.pythonhosted.org/packages/65/9d/2b032066a910d944f1f6177136453aaafa756f8eef49c8820b5f03f1d6c8/pilot_platform_common-2.12.3.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:19:34 GMT", "package": "amazon-orders", "version": "4.0.13", "json": {"info": {"author": null, "author_email": "Alex Laird <contact@alexlaird.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Console", "Environment :: Web Environment", "Intended Audience :: Developers", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.9", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "<p align=\"center\"><img alt=\"amazon-orders - A Python libray (and CLI) for Amazon order history\" src=\"https://amazon-orders.readthedocs.io/_images/logo.png\" /></p>\n\n[![Version](https://img.shields.io/pypi/v/amazon-orders)](https://pypi.org/project/amazon-orders)\n[![Python Versions](https://img.shields.io/pypi/pyversions/amazon-orders.svg)](https://pypi.org/project/amazon-orders)\n[![Coverage](https://img.shields.io/codecov/c/github/alexdlaird/amazon-orders)](https://codecov.io/gh/alexdlaird/amazon-orders)\n[![Build](https://img.shields.io/github/actions/workflow/status/alexdlaird/amazon-orders/build.yml)](https://github.com/alexdlaird/amazon-orders/actions/workflows/build.yml)\n[![Docs](https://img.shields.io/readthedocs/amazon-orders)](https://amazon-orders.readthedocs.io)\n[![GitHub License](https://img.shields.io/github/license/alexdlaird/amazon-orders)](https://github.com/alexdlaird/amazon-orders/blob/main/LICENSE)\n\n`amazon-orders` is an unofficial library that provides a Python API (and CLI) for Amazon order history.\n\nThis package works by parsing data from Amazon's consumer-facing website. A periodic build validates functionality\nto ensure its stability, but as Amazon provides no official API to use, this package may break at any time (so check\noften to ensure you're on the latest version).\n\nOnly the English, `.com` version of Amazon is officially supported.\n\n## Installation\n\n`amazon-orders` is available on [PyPI](https://pypi.org/project/amazon-orders/) and can be installed and/or upgraded using `pip`:\n\n```sh\npip install amazon-orders --upgrade\n```\n\nThat's it! `amazon-orders` is now available as a package to your Python projects and from the command line.\n\nIf pinning, be sure to use a wildcard for the [minor version](https://semver.org/) (ex. `==4.0.*`, not `==4.0.13`) to\nensure you always get the latest stable release.\n\n## Basic Usage\n\nYou'll use [`AmazonSession`](https://amazon-orders.readthedocs.io/api.html#amazonorders.session.AmazonSession) to\nauthenticate your Amazon account, then [`AmazonOrders`](https://amazon-orders.readthedocs.io/api.html#amazonorders.orders.AmazonOrders)\nand [`AmazonTransactions`](https://amazon-orders.readthedocs.io/api.html#amazonorders.transactions.AmazonTransactions)\nto interact with account data. [`get_order_history`](https://amazon-orders.readthedocs.io/api.html#amazonorders.orders.AmazonOrders.get_order_history)\nand [`get_order`](https://amazon-orders.readthedocs.io/api.html#amazonorders.orders.AmazonOrders.get_order) are good places to start.\n\n```python\nfrom amazonorders.session import AmazonSession\nfrom amazonorders.orders import AmazonOrders\n\namazon_session = AmazonSession(\"<AMAZON_EMAIL>\",\n                               \"<AMAZON_PASSWORD>\")\namazon_session.login()\n\namazon_orders = AmazonOrders(amazon_session)\norders = amazon_orders.get_order_history(year=2023)\n\nfor order in orders:\n    print(f\"{order.order_number} - {order.grand_total}\")\n```\n\nIf the fields you're looking for aren't populated with the above, set `full_details=True` (or pass `--full-details` to\nthe `history` CLI command), since by default it is `False` (enabling it slows down querying, since an additional\nrequest for each order is necessary). Have a look at the [Order](https://amazon-orders.readthedocs.io/api.html#amazonorders.entity.order.Order) entity's docs to see what fields are only\npopulated with full details.\n\n### Command Line Usage\n\nYou can also run any command available to the main Python interface from the command line:\n\n```sh\namazon-orders login\namazon-orders history --year 2023\n```\n\n### Automating Authentication\n\nAuthentication can be automated by (in order of precedence) storing credentials in environment variables, passing them\nto [`AmazonSession`](https://amazon-orders.readthedocs.io/api.html#amazonorders.session.AmazonSession), or storing them\nin [`AmazonOrdersConfig`](https://amazon-orders.readthedocs.io/api.html#amazonorders.conf.AmazonOrdersConfig). The\nenvironment variables `amazon-orders` looks for are:\n\n- `AMAZON_USERNAME`\n- `AMAZON_PASSWORD`\n- `AMAZON_OTP_SECRET_KEY` (see [docs for usage](https://amazon-orders.readthedocs.io/api.html#amazonorders.session.AmazonSession.otp_secret_key))\n\n## Documentation\n\nFor more advanced usage, `amazon-orders`'s official documentation is available\nat [Read the Docs](http://amazon-orders.readthedocs.io).\n\n## Contributing\n\nIf you would like to get involved, be sure to review\nthe [Contribution Guide](https://github.com/alexdlaird/amazon-orders/blob/main/CONTRIBUTING.rst).\n\nWant to contribute financially? If you've found `amazon-orders`\nuseful, [sponsorship](https://github.com/sponsors/alexdlaird) would\nalso be greatly appreciated!\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": null, "license": null, "license_expression": "MIT", "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": "Alex Laird <contact@alexlaird.com>", "name": "amazon-orders", "package_url": "https://pypi.org/project/amazon-orders/", "platform": null, "project_url": "https://pypi.org/project/amazon-orders/", "project_urls": {"Changelog": "https://github.com/alexdlaird/amazon-orders/blob/main/CHANGELOG.md", "Documentation": "https://amazon-orders.readthedocs.io", "Source Code": "https://github.com/alexdlaird/amazon-orders", "Sponsor": "https://github.com/sponsors/alexdlaird"}, "provides_extra": ["dev", "integration", "docs"], "release_url": "https://pypi.org/project/amazon-orders/4.0.13/", "requires_dist": ["click>=7.1", "requests>=2.23", "amazoncaptcha>=0.4", "beautifulsoup4>=4.12", "PyYAML>=5.1", "python-dateutil>=2.8", "pyotp>=2.9", "pytest; extra == \"dev\"", "coverage[toml]; extra == \"dev\"", "flake8; extra == \"dev\"", "flake8-pyproject; extra == \"dev\"", "pep8-naming; extra == \"dev\"", "responses; extra == \"dev\"", "lxml; extra == \"dev\"", "pytest-rerunfailures; extra == \"integration\"", "parameterized; extra == \"integration\"", "Sphinx; extra == \"docs\"", "sphinx-notfound-page; extra == \"docs\"", "sphinx_autodoc_typehints; extra == \"docs\"", "mypy; extra == \"docs\"", "types-requests; extra == \"docs\"", "types-beautifulsoup4; extra == \"docs\"", "types-Pillow; extra == \"docs\"", "types-PyYAML; extra == \"docs\"", "types-python-dateutil; extra == \"docs\""], "requires_python": ">=3.9", "summary": "A Python libray (and CLI) for Amazon order history", "version": "4.0.13", "yanked": false, "yanked_reason": null}, "last_serial": 30388586, "urls": [{"comment_text": null, "digests": {"blake2b_256": "fa2d60800681ca4c550e610fc7022297031b589cb50da5a689a199586da5bab4", "md5": "92a4e4a62f8baa38516adde0d41b8a68", "sha256": "63351317185a778d1309713672d953e382fd5109723493fb7f4a7c3d2f658e6f"}, "downloads": -1, "filename": "amazon_orders-4.0.13-py3-none-any.whl", "has_sig": false, "md5_digest": "92a4e4a62f8baa38516adde0d41b8a68", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 40665, "upload_time": "2025-07-28T18:19:34", "upload_time_iso_8601": "2025-07-28T18:19:34.583925Z", "url": "https://files.pythonhosted.org/packages/fa/2d/60800681ca4c550e610fc7022297031b589cb50da5a689a199586da5bab4/amazon_orders-4.0.13-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "fdfd73a5d0aa66af3a266ccce53efc954516de64e6d651a4c2cfefdb2decb43a", "md5": "89c19f966c3cef61135b03c0a5dcb6d4", "sha256": "a06d0cdaafd9adbad6af11a882e9f8a3d698e5b55c0173b52c92a3e3302ff3f4"}, "downloads": -1, "filename": "amazon_orders-4.0.13.tar.gz", "has_sig": false, "md5_digest": "89c19f966c3cef61135b03c0a5dcb6d4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 40783, "upload_time": "2025-07-28T18:19:35", "upload_time_iso_8601": "2025-07-28T18:19:35.720305Z", "url": "https://files.pythonhosted.org/packages/fd/fd/73a5d0aa66af3a266ccce53efc954516de64e6d651a4c2cfefdb2decb43a/amazon_orders-4.0.13.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:19:13 GMT", "package": "aws-cdk.asset-awscli-v1", "version": "2.2.247", "json": {"info": {"author": "Amazon Web Services<aws-cdk-dev@amazon.com>", "author_email": null, "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved", "Operating System :: OS Independent", "Programming Language :: JavaScript", "Programming Language :: Python :: 3 :: Only", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.9", "Typing :: Typed"], "description": "aws-cdk.asset-awscli-v1\n=======================\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://github.com/cdklabs/awscdk-asset-awscli#readme", "keywords": null, "license": "Apache-2.0", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "aws-cdk.asset-awscli-v1", "package_url": "https://pypi.org/project/aws-cdk.asset-awscli-v1/", "platform": null, "project_url": "https://pypi.org/project/aws-cdk.asset-awscli-v1/", "project_urls": {"Homepage": "https://github.com/cdklabs/awscdk-asset-awscli#readme", "Source": "https://github.com/cdklabs/awscdk-asset-awscli.git"}, "provides_extra": null, "release_url": "https://pypi.org/project/aws-cdk.asset-awscli-v1/2.2.247/", "requires_dist": ["jsii<2.0.0,>=1.112.0", "publication>=0.0.3", "typeguard<4.3.0,>=2.13.3"], "requires_python": "~=3.9", "summary": "A library that contains the AWS CLI for use in Lambda Layers", "version": "2.2.247", "yanked": false, "yanked_reason": null}, "last_serial": 30388571, "urls": [{"comment_text": null, "digests": {"blake2b_256": "6d209c04819490cd21802979b9274aca9920953685c9ac794c8ed4427eb0fb9c", "md5": "74b8e1b1b3b144322478ef3c82cd15c2", "sha256": "ee1ad982a6311dd74d735b34107b4166d87b9c284234308605a99b749048a3cd"}, "downloads": -1, "filename": "aws_cdk_asset_awscli_v1-2.2.247-py3-none-any.whl", "has_sig": false, "md5_digest": "74b8e1b1b3b144322478ef3c82cd15c2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "~=3.9", "size": 19495193, "upload_time": "2025-07-28T18:19:13", "upload_time_iso_8601": "2025-07-28T18:19:13.900495Z", "url": "https://files.pythonhosted.org/packages/6d/20/9c04819490cd21802979b9274aca9920953685c9ac794c8ed4427eb0fb9c/aws_cdk_asset_awscli_v1-2.2.247-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "c01835e9a37f55ff5019ba633aff385a4994e8b9251b2dfb020771712eb8c933", "md5": "c2a8bd8bd60a8a6205e88f051457588e", "sha256": "8fc515388f68b0e3549de30e1db8eec1bf97d6b6bdfa541258fa5d50937f326b"}, "downloads": -1, "filename": "aws_cdk_asset_awscli_v1-2.2.247.tar.gz", "has_sig": false, "md5_digest": "c2a8bd8bd60a8a6205e88f051457588e", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.9", "size": 19496788, "upload_time": "2025-07-28T18:19:16", "upload_time_iso_8601": "2025-07-28T18:19:16.739014Z", "url": "https://files.pythonhosted.org/packages/c0/18/35e9a37f55ff5019ba633aff385a4994e8b9251b2dfb020771712eb8c933/aws_cdk_asset_awscli_v1-2.2.247.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:19:00 GMT", "package": "bhp-pro", "version": "0.1.9", "json": {"info": {"author": "ssskingsss12", "author_email": "smalls3000i@gmail.com", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3"], "description": "Run the script using bhp_pro follow the on screen instructions for the first time installing, after install  it will take you to the main menu, the script is 40 tools combined\r\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Requires-Python", "Summary"], "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "bhp-pro", "package_url": "https://pypi.org/project/bhp-pro/", "platform": null, "project_url": "https://pypi.org/project/bhp-pro/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/bhp-pro/0.1.9/", "requires_dist": null, "requires_python": ">=3.12", "summary": "Web Enumeration Tool", "version": "0.1.9", "yanked": false, "yanked_reason": null}, "last_serial": 30388565, "urls": [{"comment_text": null, "digests": {"blake2b_256": "a16b34f33934f438348ad11a3f400d78799ea71c0c13a5a050a2c2c9f051a486", "md5": "7663eb407e4fe7a7316a80d913c04653", "sha256": "7a68ed93ef6c7d6fbf1a61a8b23773e1daf23ca146d3d461a58bdeb5f1847e6c"}, "downloads": -1, "filename": "bhp_pro-0.1.9-py3-none-any.whl", "has_sig": false, "md5_digest": "7663eb407e4fe7a7316a80d913c04653", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.12", "size": 88387, "upload_time": "2025-07-28T18:19:00", "upload_time_iso_8601": "2025-07-28T18:19:00.544268Z", "url": "https://files.pythonhosted.org/packages/a1/6b/34f33934f438348ad11a3f400d78799ea71c0c13a5a050a2c2c9f051a486/bhp_pro-0.1.9-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "6eb49336d17f76bead6345228dbc410eb61a85a889b69f99d134c35b97bd3720", "md5": "a51f019e5f50e20bf4e5353f767c5582", "sha256": "16b3e3b13d8f46ccfcdde20ecf02acacf2cfbaf58c1ef40e5339e3e86c3b8988"}, "downloads": -1, "filename": "bhp_pro-0.1.9.tar.gz", "has_sig": false, "md5_digest": "a51f019e5f50e20bf4e5353f767c5582", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.12", "size": 87241, "upload_time": "2025-07-28T18:19:01", "upload_time_iso_8601": "2025-07-28T18:19:01.781675Z", "url": "https://files.pythonhosted.org/packages/6e/b4/9336d17f76bead6345228dbc410eb61a85a889b69f99d134c35b97bd3720/bhp_pro-0.1.9.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:18:49 GMT", "package": "robo-lib", "version": "1.0.5", "json": {"info": {"author": null, "author_email": "Erik Papp <erik3papp@gmail.com>", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# robo-lib\n\nprovides tools for creating, configuring, and training custom transformer models on any data available to you.\n\n## Main features:\n- Customize and train tokenizers using an implementation of the features from the [tokenizers](https://pypi.org/project/tokenizers/#description) library.\n- Customize data processor to process data into individual tensors, ready to be used to train transformers without further processing.\n- Configure transformer models to fit specific requirements/specifications without having to write the internal logic.\n- Use the 3 components to create, train, and use custom transformers in different applications.\n\n## Installation\n\n```bash\npip install robo-lib\n```\n\n## using robo-lib\n\nDocumentation can be found [here](https://github.com/hamburgerfish/robo_pack/wiki).\n\n### Language translation example\n- In this example, an encoder-decoder transformer is created for language translation, from English to French.\n- This example uses two .txt files for training, one with English, and the other with the equivalent French sentence in each line (delimited by \"\\n\").\n- Create, train, and save tokenizers using `TokenizerConstructor`.\n- In this example, the WordLevel tokenizer is used, along with the detault arguments of `TokenizerConstructor`.\n\n```python\nimport robo_lib as rl\n\nencoder_tok = rl.TokenizerConstructor(tokenizer_type=\"WordLevel\")\nencoder_tok.train(\"english_data.txt\")\n\ndecoder_tok = rl.TokenizerConstructor(tokenizer_type=\"WordLevel\")\ndecoder_tok.train(\"french_data.txt\")\n\nrl.save_component(encoder_tok, \"tokenizers/encoder_tok.pkl\")\nrl.save_component(decoder_tok, \"tokenizers/decoder_tok.pkl\")\n```\n\n- The `DataProcessor` can be used to automatically process the data into a single torch.tensor, easily useable by the transformer for training.\n- The tokenizer(s) must be specified when initialising a DataProcessor. In this case the dec_tokenizer, and enc_tokenizer is both specified for an encoder-decoder transformer.\n- The `process_list` method processes lists of string data, so our .txt files are read into lists to be processed by `process_list`.\n- In this example, we are splitting the data 90% : 10% for training and validation.\n\n```python\nproc = rl.DataProcessor(dec_tokenizer=decoder_tok, enc_tokenizer=encoder_tok)\n\n# read training .txt files into lists\nwith open(\"english_data.txt\", \"r\") as file:\n    english_list = file.read().split(\"\\n\")\n\nwith open(\"french_data.txt\", \"r\") as file:\n    french_list = file.read().split(\"\\n\")\n\n# splitting lists into train and validation sets\nsplit = 0.9\nn = int(len(english_list) * split)\nenglish_train = english_list[:n]\nfrench_train = french_list[:n]\nenglish_val = english_list[n:]\nfrench_val = french_list[n:]\n\n# process and save training data as data/training*.pt\n# block_size_exceeded_policy=\"skip\" removes training data larger than specified block size\nproc.process_list(\n    save_path=\"data/training\",\n    dec_data=french_train,\n    dec_max_block_size=100,\n    enc_data=english_train,\n    enc_max_block_size=100\n)\n\n# process and save validation data as data/validation*.pt\nproc.process_list(\n    save_path=\"data/validation\",\n    dec_data=french_val,\n    dec_max_block_size=100,\n    enc_data=english_val,\n    enc_max_block_size=100\n)\n```\n- The `RoboConstructor` class is used to create and configure transformer models before trainin.\n- A separate .py file is recommended for training.\n- If device is not specified, `RoboConstructor` will take the first available one out of (\"cuda\", \"mps\", \"cpu\"). Torch cuda is not part of the dependencies when installing robo-lib, so it is highly recommended to install it, using this [link](https://pytorch.org/get-started/locally/), if you have a CUDA compatible device.\n- The `train` method is used to train the transformer and save it to `save_path` every `eval_interval` iterations.\n- If a non-`TokenizerConstructor` token is used, the pad token if your tokenizer can be specified instead of the dec_tokenizer parameter.\n\n```python\nimport robo_lib as rl\n\nencoder_tok = rl.load_component(\"tokenizers/encoder_tok.pkl\")\ndecoder_tok = rl.load_component(\"tokenizers/decoder_tok.pkl\")\n\nrobo = rl.RoboConstructor(\n    n_embed=512,\n    dec_n_blocks=6,\n    dec_n_head=8,\n    dec_vocab_size=decoder_tok.vocab_size,\n    dec_block_size=100,\n    enc_n_blocks=6,\n    enc_n_head=8,\n    enc_vocab_size=encoder_tok.vocab_size,\n    enc_block_size=100\n)\n\nrobo.train_robo(\n    max_iters=20000,\n    eval_interval=200,\n    batch_size=128,\n    training_dir_path=\"data/training\",\n    eval_dir_path=\"data/validation\",\n    dec_tokenizer=decoder_tok,\n    save_path=\"models/eng_to_fr_robo.pkl\"\n)\n```\n\n- For language translation, a loss of around 3 already shows good results.\n- To use the trained transformer, the `generate` method can be employed.\n- The temperature, top_k and top_p values can be specified for this method, along with the tokenizers used.\n- If a non-`TokenizerConstructor` tokenizer is used, the start, end, separator (decoder-only), and new-line tokens can be specified of your tokenizer.\n- In this example, a simple script is created to interact with the user on the command-line, where the user's English input will be translated by the transformer and printed out onto the console in French.\n\n```python\nimport robo_lib as rl\n\nrobo = rc.load_component(\"models/eng_to_fr_robo.pkl\")\nencoder_tok = rl.load_component(\"tokenizers/encoder_tok.pkl\")\ndecoder_tok = rl.load_component(\"tokenizers/decoder_tok.pkl\")\n\nWhile True:\n    query = input()\n    print(robo.generate(query, dec_tokenizer=decoder_tok, enc_tokenizer=encoder_tok))\n```\n\n### Shakespeare dialogue generator example\n- In this example, a decoder-only transformer is created and trained on a file containing all the dialogue written by William Shakespeare in his plays.\n- The training data is in the form of a single .txt file containing the dialogue.\n- The default BPE tokenizer is used in this case, so no argument is specified for `TokenizerConstructor`.\n\n```python\nimport robo_lib as rl\n\ntok = rl.TokenizerConstructor()\ntok.train(\"shakespeare_dialogues.txt\")\n\nrl.save_component(tok, \"tokenizers/shakespeare_tok.pkl\")\n```\n\n- In this example, instead of having multiple pieces of training data, we have one large text file, from which random chunks of length `block_size` can be used for training. Therefore, a single large string is input into the DataProcessor instead of a list of strings.\n- Since this is a decoder-only transformer, encoder arguments are not given.\n- Since the entire string should be processed as is, instead of creating blocks of training data, block_size is not specified.\n- dec_create_masks is set to False, as there will be no padding in the training data.\n\n```python\nproc = rl.DataProcessor(dec_tokenizer=tok)\n\n# read training .txt file\nwith open(\"shakespeare_dialogues.txt\", \"r\") as file:\n    dialogues_str = file.read()\n\n# splitting string into train and validation sets\nsplit = 0.9\nn = int(len(dialogues_str) * split)\ntrain_data = dialogues_str[:n]\nval_data = dialogues_str[n:]\n\n# process and save training data as data/shakespeare_train*.pt\nproc.process_list(\n    save_path=\"data/shakespeare_train\",\n    dec_data=train_data,\n    dec_create_masks=False\n    )\n\n# process and save validation data as data/validation*.pt\nproc.process_list(\n    save_path=\"data/shakespeare_valid\",\n    dec_data=val_data,\n    dec_create_masks=False\n)\n```\n- Training the transformer.\n```python\nimport robo_lib as rl\n\ntok = rl.load_component(\"tokenizers/shakespeare_tok.pkl\")\n\nrobo = rl.RoboConstructor(\n    n_embed=1024,\n    dec_n_blocks=8,\n    dec_n_head=8,\n    dec_vocab_size=tok.vocab_size,\n    dec_block_size=200\n)\n\nrobo.train(\n    max_iters=20000,\n    eval_interval=200,\n    batch_size=64,\n    training_dir_path=\"data/shakespeare_train\",\n    eval_dir_path=\"data/shakespeare_valid\",\n    dec_tokenizer=tok,\n    save_path=\"models/shakespeare_robo.pkl\"\n)\n```\n- In this example, the user can specify the start of the generated Shakespeare play and the transformer will generate and print the rest, until `max_new_tokens` (1000) tokens are generated.\n- Temperature and top_k are set to 1.2 and 2 respectively to generate a more \"creative\" output.\n```python\nimport robo_lib as rl\n\nrobo = rc.load_component(\"models/shakespeare_robo.pkl\")\ntok = rl.load_component(\"tokenizers/shakespeare_tok.pkl\")\n\nWhile True:\n    start = input()\n    print(robo.generate(start, max_new_tokens=1000, dec_tokenizer=tok, temperature=1.2, top_k=2))\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "robo-lib", "package_url": "https://pypi.org/project/robo-lib/", "platform": null, "project_url": "https://pypi.org/project/robo-lib/", "project_urls": {"Homepage": "https://github.com/hamburgerfish/robo_pack", "Issues": "https://github.com/hamburgerfish/robo_pack/issues"}, "provides_extra": null, "release_url": "https://pypi.org/project/robo-lib/1.0.5/", "requires_dist": ["numpy", "tokenizers", "torch", "typing"], "requires_python": ">=3.8", "summary": "A package to create, configure, and train transformer models.", "version": "1.0.5", "yanked": false, "yanked_reason": null}, "last_serial": 30388562, "urls": [{"comment_text": "", "digests": {"blake2b_256": "1f0881dc2a07773cef11e7b08a29b8b486fb5dffeb7286bcad07e5528c8bd381", "md5": "28a09ecf866b8028b29c8e129c8b9ad8", "sha256": "e79863025e504640afc4a636cc652a6e06f0b53ee07689dda05808452aa08877"}, "downloads": -1, "filename": "robo_lib-1.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "28a09ecf866b8028b29c8e129c8b9ad8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 13977, "upload_time": "2025-07-28T18:18:49", "upload_time_iso_8601": "2025-07-28T18:18:49.766346Z", "url": "https://files.pythonhosted.org/packages/1f/08/81dc2a07773cef11e7b08a29b8b486fb5dffeb7286bcad07e5528c8bd381/robo_lib-1.0.5-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "4c3402454801dbc9cb3a030167c832d57d34b123f2cea5f272f046d20d3b057c", "md5": "43c03015e8a9bf654eaf453642238bdc", "sha256": "e0564ba4ac7b9ec8e9f8d07936f67f3649160a497d82d710cabefd7bbfb825c9"}, "downloads": -1, "filename": "robo_lib-1.0.5.tar.gz", "has_sig": false, "md5_digest": "43c03015e8a9bf654eaf453642238bdc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 16988, "upload_time": "2025-07-28T18:18:51", "upload_time_iso_8601": "2025-07-28T18:18:51.999633Z", "url": "https://files.pythonhosted.org/packages/4c/34/02454801dbc9cb3a030167c832d57d34b123f2cea5f272f046d20d3b057c/robo_lib-1.0.5.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:18:49 GMT", "package": "liger-kernel-nightly", "version": "0.6.1.dev20250728181841", "json": {"info": {"author": null, "author_email": null, "bugtrack_url": null, "classifiers": [], "description": "<a name=\"readme-top\"></a>\n\n# Liger Kernel: Efficient Triton Kernels for LLM Training\n\n\n<table style=\"width: 100%; text-align: center; border-collapse: collapse;\">\n    <tr>\n        <th style=\"padding: 10px;\" colspan=\"2\">Stable</th>\n        <th style=\"padding: 10px;\" colspan=\"2\">Nightly</th>\n        <th style=\"padding: 10px;\">Discord</th>\n    </tr>\n    <tr>\n        <td style=\"padding: 10px;\">\n            <a href=\"https://pepy.tech/project/liger-kernel\">\n                <img src=\"https://static.pepy.tech/badge/liger-kernel\" alt=\"Downloads (Stable)\">\n            </a>\n        </td>\n        <td style=\"padding: 10px;\">\n            <a href=\"https://pypi.org/project/liger-kernel\">\n                <img alt=\"PyPI - Version\" src=\"https://img.shields.io/pypi/v/liger-kernel?color=green\">\n            </a>\n        </td>\n        <td style=\"padding: 10px;\">\n            <a href=\"https://pepy.tech/project/liger-kernel-nightly\">\n                <img src=\"https://static.pepy.tech/badge/liger-kernel-nightly\" alt=\"Downloads (Nightly)\">\n            </a>\n        </td>\n        <td style=\"padding: 10px;\">\n            <a href=\"https://pypi.org/project/liger-kernel-nightly\">\n                <img alt=\"PyPI - Version\" src=\"https://img.shields.io/pypi/v/liger-kernel-nightly?color=green\">\n            </a>\n        </td>\n        <td style=\"padding: 10px;\">\n            <a href=\"https://discord.gg/gpumode\">\n                <img src=\"https://dcbadge.limes.pink/api/server/gpumode?style=flat\" alt=\"Join Our Discord\">\n            </a>\n        </td>\n    </tr>\n</table>\n\n\n\n<img src=\"https://raw.githubusercontent.com/linkedin/Liger-Kernel/main/docs/images/logo-banner.png\">\n\n[Installation](#installation) | [Getting Started](#getting-started) | [Examples](#examples) | [High-level APIs](#high-level-apis) | [Low-level APIs](#low-level-apis) | [Cite our work](#cite-this-work)\n\n<details>\n  <summary>Latest News \ud83d\udd25</summary>\n\n  - [2025/03/06] We release a joint blog post on TorchTune \u00d7 Liger - [Peak Performance, Minimized Memory: Optimizing torchtune\u2019s performance with torch.compile & Liger Kernel](https://pytorch.org/blog/peak-performance-minimized-memory/)\n  - [2024/12/11] We release [v0.5.0](https://github.com/linkedin/Liger-Kernel/releases/tag/v0.5.0): 80% more memory efficient post training losses (DPO, ORPO, CPO, etc)!\n  - [2024/12/5] We release LinkedIn Engineering Blog - [Liger-Kernel: Empowering an open source ecosystem of Triton Kernels for Efficient LLM Training](https://www.linkedin.com/blog/engineering/open-source/liger-kernel-open-source-ecosystem-for-efficient-llm-training)\n  - [2024/11/6] We release [v0.4.0](https://github.com/linkedin/Liger-Kernel/releases/tag/v0.4.0): Full AMD support, Tech Report, Modal CI, Llama-3.2-Vision!\n  - [2024/10/21] We have released the tech report of Liger Kernel on Arxiv: https://arxiv.org/pdf/2410.10989\n  - [2024/9/6] We release v0.2.1 ([X post](https://x.com/liger_kernel/status/1832168197002510649)). 2500+ Stars, 10+ New Contributors, 50+ PRs, 50k Downloads in two weeks!\n  - [2024/8/31] CUDA MODE talk, [Liger-Kernel: Real-world Triton kernel for LLM Training](https://youtu.be/gWble4FreV4?si=dxPeIchhkJ36Mbns), [Slides](https://github.com/cuda-mode/lectures?tab=readme-ov-file#lecture-28-liger-kernel)\n  - [2024/8/23] Official release: check out our [X post](https://x.com/hsu_byron/status/1827072737673982056)\n\n</details>\n\n\n**Liger Kernel** is a collection of Triton kernels designed specifically for LLM training. It can effectively increase multi-GPU **training throughput by 20%** and reduces **memory usage by 60%**. We have implemented **Hugging Face Compatible** `RMSNorm`, `RoPE`, `SwiGLU`, `CrossEntropy`, `FusedLinearCrossEntropy`, and more to come. The kernel works out of the box with [Flash Attention](https://github.com/Dao-AILab/flash-attention), [PyTorch FSDP](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html), and [Microsoft DeepSpeed](https://github.com/microsoft/DeepSpeed). We welcome contributions from the community to gather the best kernels for LLM training.\n\nWe've also added optimized Post-Training kernels that deliver **up to 80% memory savings** for alignment and distillation tasks. We support losses like DPO, CPO, ORPO, SimPO, KTO, JSD, and many more. Check out [how we optimize the memory](https://x.com/hsu_byron/status/1866577403918917655).\n\nYou can view the documentation site for additional installation, usage examples, and API references:https://linkedin.github.io/Liger-Kernel/\n\n## Supercharge Your Model with Liger Kernel\n\n![Banner](https://raw.githubusercontent.com/linkedin/Liger-Kernel/main/docs/images/banner.GIF)\n\nWith one line of code, Liger Kernel can increase throughput by more than 20% and reduce memory usage by 60%, thereby enabling longer context lengths, larger batch sizes, and massive vocabularies.\n\n\n| Speed Up                 | Memory Reduction        |\n|--------------------------|-------------------------|\n| ![Speed up](https://raw.githubusercontent.com/linkedin/Liger-Kernel/main/docs/images/e2e-tps.png) | ![Memory](https://raw.githubusercontent.com/linkedin/Liger-Kernel/main/docs/images/e2e-memory.png) |\n\n> **Note:**\n> - Benchmark conditions: LLaMA 3-8B, Batch Size = 8, Data Type = `bf16`, Optimizer = AdamW, Gradient Checkpointing = True, Distributed Strategy = FSDP1 on 8 A100s.\n> - Hugging Face models start to OOM at a 4K context length, whereas Hugging Face + Liger Kernel scales up to 16K.\n\n## Optimize Post Training with Liger Kernel\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/linkedin/Liger-Kernel/main/docs/images/post-training.png\" width=\"50%\" alt=\"Post Training\">\n</p>\n\nWe provide optimized post training kernels like DPO, ORPO, SimPO, and more which can reduce memory usage by up to 80%. You can easily use them as python modules.\n\n```python\nfrom liger_kernel.chunked_loss import LigerFusedLinearORPOLoss\norpo_loss = LigerFusedLinearORPOLoss()\ny = orpo_loss(lm_head.weight, x, target)\n```\n\n\n## Examples\n\n| **Use Case**                                    | **Description**                                                                                   |\n|------------------------------------------------|---------------------------------------------------------------------------------------------------|\n| [**Hugging Face Trainer**](https://github.com/linkedin/Liger-Kernel/tree/main/examples/huggingface)      | Train LLaMA 3-8B ~20% faster with over 40% memory reduction on Alpaca dataset using 4 A100s with FSDP |\n| [**Lightning Trainer**](https://github.com/linkedin/Liger-Kernel/tree/main/examples/lightning)         | Increase 15% throughput and reduce memory usage by 40% with LLaMA3-8B on MMLU dataset using 8 A100s with DeepSpeed ZeRO3 |\n| [**Medusa Multi-head LLM (Retraining Phase)**](https://github.com/linkedin/Liger-Kernel/tree/main/examples/medusa)        | Reduce memory usage by 80% with 5 LM heads and improve throughput by 40% using 8 A100s with FSDP |\n| [**Vision-Language Model SFT**](https://github.com/linkedin/Liger-Kernel/tree/main/examples/huggingface/run_qwen2_vl.sh)      | Finetune Qwen2-VL on image-text data using 4 A100s with FSDP |\n| [**Liger ORPO Trainer**](https://github.com/linkedin/Liger-Kernel/blob/main/examples/alignment/run_orpo.py)      | Align Llama 3.2 using Liger ORPO Trainer with FSDP with 50% memory reduction |\n\n## Key Features\n\n- **Ease of use:** Simply patch your Hugging Face model with one line of code, or compose your own model using our Liger Kernel modules.\n- **Time and memory efficient:** In the same spirit as Flash-Attn, but for layers like **RMSNorm**, **RoPE**, **SwiGLU**, and **CrossEntropy**! Increases multi-GPU training throughput by 20% and reduces memory usage by 60% with **kernel fusion**, **in-place replacement**, and **chunking** techniques.\n- **Exact:** Computation is exact\u2014no approximations! Both forward and backward passes are implemented with rigorous unit tests and undergo convergence testing against training runs without Liger Kernel to ensure accuracy.\n- **Lightweight:** Liger Kernel has minimal dependencies, requiring only Torch and Triton\u2014no extra libraries needed! Say goodbye to dependency headaches!\n- **Multi-GPU supported:** Compatible with multi-GPU setups (PyTorch FSDP, DeepSpeed, DDP, etc.).\n- **Trainer Framework Integration**: [Axolotl](https://github.com/axolotl-ai-cloud/axolotl), [LLaMa-Factory](https://github.com/hiyouga/LLaMA-Factory), [SFTTrainer](https://github.com/huggingface/trl/releases/tag/v0.10.1), [Hugging Face Trainer](https://github.com/huggingface/transformers/pull/32860), [SWIFT](https://github.com/modelscope/ms-swift), [oumi](https://github.com/oumi-ai/oumi/tree/main)\n\n## Installation\n\n### Dependencies\n\n#### CUDA\n\n- `torch >= 2.1.2`\n- `triton >= 2.3.0`\n\n#### ROCm\n\n- `torch >= 2.5.0` Install according to the instruction in Pytorch official webpage.\n- `triton >= 3.0.0` Install from pypi. (e.g. `pip install triton==3.0.0`)\n\n```bash\n# Need to pass the url when installing\npip install -e .[dev] --extra-index-url https://download.pytorch.org/whl/nightly/rocm6.2\n```\n\n### Optional Dependencies\n\n- `transformers >= 4.x`: Required if you plan to use the transformers models patching APIs. The specific model you are working will dictate the minimum version of transformers.\n\n> **Note:**\n> Our kernels inherit the full spectrum of hardware compatibility offered by [Triton](https://github.com/triton-lang/triton).\n\nTo install the stable version:\n\n```bash\n$ pip install liger-kernel\n```\n\nTo install the nightly version:\n\n```bash\n$ pip install liger-kernel-nightly\n```\n\nTo install from source:\n\n```bash\ngit clone https://github.com/linkedin/Liger-Kernel.git\ncd Liger-Kernel\n\n# Install Default Dependencies\n# Setup.py will detect whether you are using AMD or NVIDIA\npip install -e .\n\n# Setup Development Dependencies\npip install -e \".[dev]\"\n```\n\n\n## Getting Started\n\nThere are a couple of ways to apply Liger kernels, depending on the level of customization required.\n\n### 1. Use AutoLigerKernelForCausalLM\n\nUsing the `AutoLigerKernelForCausalLM` is the simplest approach, as you don't have to import a model-specific patching API. If the model type is supported, the modeling code will be automatically patched using the default settings.\n\n```python\nfrom liger_kernel.transformers import AutoLigerKernelForCausalLM\n\n# This AutoModel wrapper class automatically monkey-patches the\n# model with the optimized Liger kernels if the model is supported.\nmodel = AutoLigerKernelForCausalLM.from_pretrained(\"path/to/some/model\")\n```\n\n### 2. Apply Model-Specific Patching APIs\n\nUsing the [patching APIs](#patching), you can swap Hugging Face models with optimized Liger Kernels.\n\n```python\nimport transformers\nfrom liger_kernel.transformers import apply_liger_kernel_to_llama\n\n# 1a. Adding this line automatically monkey-patches the model with the optimized Liger kernels\napply_liger_kernel_to_llama()\n\n# 1b. You could alternatively specify exactly which kernels are applied\napply_liger_kernel_to_llama(\n  rope=True,\n  swiglu=True,\n  cross_entropy=True,\n  fused_linear_cross_entropy=False,\n  rms_norm=False\n)\n\n# 2. Instantiate patched model\nmodel = transformers.AutoModelForCausalLM(\"path/to/llama/model\")\n```\n\n### 3. Compose Your Own Model\n\nYou can take individual [kernels](https://github.com/linkedin/Liger-Kernel?tab=readme-ov-file#model-kernels) to compose your models.\n\n```python\nfrom liger_kernel.transformers import LigerFusedLinearCrossEntropyLoss\nimport torch.nn as nn\nimport torch\n\nmodel = nn.Linear(128, 256).cuda()\n\n# fuses linear + cross entropy layers together and performs chunk-by-chunk computation to reduce memory\nloss_fn = LigerFusedLinearCrossEntropyLoss()\n\ninput = torch.randn(4, 128, requires_grad=True, device=\"cuda\")\ntarget = torch.randint(256, (4, ), device=\"cuda\")\n\nloss = loss_fn(model.weight, input, target)\nloss.backward()\n```\n\n## High-level APIs\n\n### AutoModel\n\n| **AutoModel Variant** | **API** |\n|-----------|---------|\n| AutoModelForCausalLM | `liger_kernel.transformers.AutoLigerKernelForCausalLM` |\n\n\n### Patching\n\n| **Model**   | **API**                                                      | **Supported Operations**                                                |\n|-------------|--------------------------------------------------------------|-------------------------------------------------------------------------|\n| Llama4 (Text) & (Multimodal)      | `liger_kernel.transformers.apply_liger_kernel_to_llama4`   | RMSNorm, LayerNorm, GeGLU, CrossEntropyLoss, FusedLinearCrossEntropy         |\n| LLaMA 2 & 3 | `liger_kernel.transformers.apply_liger_kernel_to_llama`   | RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy        |\n| LLaMA 3.2-Vision | `liger_kernel.transformers.apply_liger_kernel_to_mllama`   | RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy        |\n| Mistral     | `liger_kernel.transformers.apply_liger_kernel_to_mistral`  | RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy        |\n| Mixtral     | `liger_kernel.transformers.apply_liger_kernel_to_mixtral`  | RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy        |\n| Gemma1      | `liger_kernel.transformers.apply_liger_kernel_to_gemma`    | RoPE, RMSNorm, GeGLU, CrossEntropyLoss, FusedLinearCrossEntropy         |\n| Gemma2      | `liger_kernel.transformers.apply_liger_kernel_to_gemma2`   | RoPE, RMSNorm, GeGLU, CrossEntropyLoss, FusedLinearCrossEntropy         |\n| Gemma3 (Text)      | `liger_kernel.transformers.apply_liger_kernel_to_gemma3_text`   | RoPE, RMSNorm, GeGLU, CrossEntropyLoss, FusedLinearCrossEntropy         |\n| Gemma3 (Multimodal)      | `liger_kernel.transformers.apply_liger_kernel_to_gemma3`   | LayerNorm, RoPE, RMSNorm, GeGLU, CrossEntropyLoss, FusedLinearCrossEntropy         |\n| Paligemma, Paligemma2, & Paligemma2 Mix      | `liger_kernel.transformers.apply_liger_kernel_to_paligemma`   | LayerNorm, RoPE, RMSNorm, GeGLU, CrossEntropyLoss, FusedLinearCrossEntropy         |\n| Qwen2, Qwen2.5, & QwQ      | `liger_kernel.transformers.apply_liger_kernel_to_qwen2`    | RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy        |\n| Qwen2-VL, & QVQ       | `liger_kernel.transformers.apply_liger_kernel_to_qwen2_vl`    | RMSNorm, LayerNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy        |\n| Qwen2.5-VL       | `liger_kernel.transformers.apply_liger_kernel_to_qwen2_5_vl`    | RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy        |\n| Qwen3   | `liger_kernel.transformers.apply_liger_kernel_to_qwen3`    |  RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy       |\n| Qwen3 MoE | `liger_kernel.transformers.apply_liger_kernel_to_qwen3_moe` | RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy       |\n| Phi3 & Phi3.5       | `liger_kernel.transformers.apply_liger_kernel_to_phi3`     | RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy         |\n| Granite 3.0 & 3.1   | `liger_kernel.transformers.apply_liger_kernel_to_granite`     | RoPE, RMSNorm, SwiGLU, CrossEntropyLoss |\n| OLMo2   | `liger_kernel.transformers.apply_liger_kernel_to_olmo2`     | RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy |\n| GLM-4   | `liger_kernel.transformers.apply_liger_kernel_to_glm4`     | RoPE, RMSNorm, SwiGLU, CrossEntropyLoss, FusedLinearCrossEntropy |\n\n\n## Low-level APIs\n\n- `Fused Linear` kernels combine linear layers with losses, reducing memory usage by up to 80% - ideal for HBM-constrained workloads.\n- Other kernels use fusion and in-place techniques for memory and performance optimization.\n\n### Model Kernels\n\n| **Kernel**                      | **API**                                                     |\n|---------------------------------|-------------------------------------------------------------|\n| RMSNorm                         | `liger_kernel.transformers.LigerRMSNorm`                    |\n| LayerNorm                       | `liger_kernel.transformers.LigerLayerNorm`                  |\n| RoPE                            | `liger_kernel.transformers.liger_rotary_pos_emb`            |\n| SwiGLU                          | `liger_kernel.transformers.LigerSwiGLUMLP`                  |\n| GeGLU                           | `liger_kernel.transformers.LigerGEGLUMLP`                   |\n| CrossEntropy                    | `liger_kernel.transformers.LigerCrossEntropyLoss`           |\n| Fused Linear CrossEntropy       | `liger_kernel.transformers.LigerFusedLinearCrossEntropyLoss`|\n| Multi Token Attention           | `liger_kernel.transformers.LigerMultiTokenAttention`        |\n| Softmax                         | `liger_kernel.transformers.LigerSoftmax`                    |\n| Sparsemax                       | `liger_kernel.transformers.LigerSparsemax`                  |\n\n\n### Alignment Kernels\n\n| **Kernel**                      | **API**                                                     |\n|---------------------------------|-------------------------------------------------------------|\n| Fused Linear CPO Loss           | `liger_kernel.chunked_loss.LigerFusedLinearCPOLoss`       |\n| Fused Linear DPO Loss           | `liger_kernel.chunked_loss.LigerFusedLinearDPOLoss`       |\n| Fused Linear ORPO Loss          | `liger_kernel.chunked_loss.LigerFusedLinearORPOLoss`      |\n| Fused Linear SimPO Loss         | `liger_kernel.chunked_loss.LigerFusedLinearSimPOLoss`     |\n| Fused Linear KTO Loss           | `liger_kernel.chunked_loss.LigerFusedLinearKTOLoss`     |\n\n### Distillation Kernels\n\n| **Kernel**                      | **API**                                                     |\n|---------------------------------|-------------------------------------------------------------|\n| KLDivergence                    | `liger_kernel.transformers.LigerKLDIVLoss`                  |\n| JSD                             | `liger_kernel.transformers.LigerJSD`                        |\n| Fused Linear JSD                  | `liger_kernel.transformers.LigerFusedLinearJSD`             |\n| TVD                             | `liger_kernel.transformers.LigerTVDLoss`                    |\n\n### Experimental Kernels\n\n| **Kernel**                      | **API**                                                     |\n|---------------------------------|-------------------------------------------------------------|\n| Embedding                       | `liger_kernel.transformers.experimental.LigerEmbedding`     |\n| Matmul int2xint8                | `liger_kernel.transformers.experimental.matmul` |\n\n\n## Contributing, Acknowledgements, and License\n\n- [Contributing Guidelines](https://github.com/linkedin/Liger-Kernel/blob/main/docs/contributing.md)\n- [Acknowledgements](https://github.com/linkedin/Liger-Kernel/blob/main/docs/acknowledgement.md)\n- [License Information](https://github.com/linkedin/Liger-Kernel/blob/main/docs/license.md)\n\n## Sponsorship and Collaboration\n\n- [Glows.ai](https://platform.glows.ai/): Sponsoring NVIDIA GPUs for our open source developers.\n- [AMD](https://www.amd.com/en.html): Providing AMD GPUs for our AMD CI.\n- [Intel](https://www.intel.com/): Providing Intel GPUs for our Intel CI.\n- [Modal](https://modal.com/): Free 3000 credits from GPU MODE IRL for our NVIDIA CI.\n- [EmbeddedLLM](https://embeddedllm.com/): Making Liger Kernel run fast and stable on AMD.\n- [HuggingFace](https://huggingface.co/): Integrating Liger Kernel into Hugging Face Transformers and TRL.\n- [Lightning AI](https://lightning.ai/): Integrating Liger Kernel into Lightning Thunder.\n- [Axolotl](https://axolotl.ai/): Integrating Liger Kernel into Axolotl.\n- [Llama-Factory](https://github.com/hiyouga/LLaMA-Factory): Integrating Liger Kernel into Llama-Factory.\n\n\n## CI status\n\n<table style=\"width: 100%; text-align: center; border-collapse: collapse;\">\n    <tr>\n        <th style=\"padding: 10px;\">Build</th>\n    </tr>\n    <tr>\n        <td style=\"padding: 10px;\">\n            <div style=\"display: block;\">\n                <a href=\"https://github.com/linkedin/Liger-Kernel/actions/workflows/nvi-ci.yml\">\n                    <img src=\"https://github.com/linkedin/Liger-Kernel/actions/workflows/nvi-ci.yml/badge.svg?event=schedule\" alt=\"Build\">\n                </a>\n            </div>\n            <div style=\"display: block;\">\n                <a href=\"https://github.com/linkedin/Liger-Kernel/actions/workflows/amd-ci.yml\">\n                    <img src=\"https://github.com/linkedin/Liger-Kernel/actions/workflows/amd-ci.yml/badge.svg?event=schedule\" alt=\"Build\">\n                </a>\n            </div>\n            <div style=\"display: block;\">\n                <a href=\"https://github.com/linkedin/Liger-Kernel/actions/workflows/amd-ci.yml\">\n                    <img src=\"https://github.com/linkedin/Liger-Kernel/actions/workflows/intel-ci.yml/badge.svg?event=schedule\" alt=\"Build\">\n                </a>\n            </div>\n        </td>\n    </tr>\n</table>\n\n\n\n## Contact\n\n- For issues, create a Github ticket in this repository\n- For open discussion, join [our discord channel on GPUMode](https://discord.com/channels/1189498204333543425/1275130785933951039)\n- For formal collaboration, send an email to Yanning Chen(yannchen@linkedin.com) and Zhipeng Wang(zhipwang@linkedin.com)\n\n## Cite this work\n\nBiblatex entry:\n```bib\n@inproceedings{\nhsu2025ligerkernel,\ntitle={Liger-Kernel: Efficient Triton Kernels for {LLM} Training},\nauthor={Pin-Lun Hsu and Yun Dai and Vignesh Kothapalli and Qingquan Song and Shao Tang and Siyu Zhu and Steven Shimizu and Shivam Sahni and Haowen Ning and Yanning Chen and Zhipeng Wang},\nbooktitle={Championing Open-source DEvelopment in ML Workshop @ ICML25},\nyear={2025},\nurl={https://openreview.net/forum?id=36SjAIT42G}\n}\n```\n\n## Star History\n[![Star History Chart](https://api.star-history.com/svg?repos=linkedin/Liger-Kernel&type=Date)](https://www.star-history.com/#linkedin/Liger-Kernel&Date)\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n    <a href=\"#readme-top\" style=\"text-decoration: none; color: #007bff; font-weight: bold;\">\n        \u2191 Back to Top \u2191\n    </a>\n</p>\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": "BSD 2-CLAUSE LICENSE\n        Copyright 2024 LinkedIn Corporation \n        All Rights Reserved.\n        Redistribution and use in source and binary forms, with or\n        without modification, are permitted provided that the following\n        conditions are met:\n        1. Redistributions of source code must retain the above copyright\n        notice, this list of conditions and the following disclaimer.\n        2. Redistributions in binary form must reproduce the above\n        copyright notice, this list of conditions and the following\n        disclaimer in the documentation and/or other materials provided\n        with the distribution.\n        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n        \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n        LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n        A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n        HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n        SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n        LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n        DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n        THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n        (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n        OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n        ", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "liger-kernel-nightly", "package_url": "https://pypi.org/project/liger-kernel-nightly/", "platform": null, "project_url": "https://pypi.org/project/liger-kernel-nightly/", "project_urls": {"Homepage": "https://github.com/linkedin/Liger-Kernel"}, "provides_extra": ["dev"], "release_url": "https://pypi.org/project/liger-kernel-nightly/0.6.1.dev20250728181841/", "requires_dist": ["torch>=2.1.2", "triton>=2.3.1", "transformers>=4.49.0; extra == \"dev\"", "matplotlib>=3.7.2; extra == \"dev\"", "flake8>=4.0.1.1; extra == \"dev\"", "black>=24.4.2; extra == \"dev\"", "isort>=5.13.2; extra == \"dev\"", "pytest>=7.1.2; extra == \"dev\"", "pytest-xdist; extra == \"dev\"", "pytest-rerunfailures; extra == \"dev\"", "datasets>=2.19.2; extra == \"dev\"", "seaborn; extra == \"dev\"", "mkdocs; extra == \"dev\"", "mkdocs-material; extra == \"dev\"", "torchvision>=0.20; extra == \"dev\""], "requires_python": null, "summary": "Efficient Triton kernels for LLM Training", "version": "0.6.1.dev20250728181841", "yanked": false, "yanked_reason": null}, "last_serial": 30388561, "urls": [{"comment_text": null, "digests": {"blake2b_256": "41fc62db9d3b0fd5c0f6864790947e2f750b1d7477cddc6ce3d01c6c79ec4aa3", "md5": "6e2ba2bd7d292e9eae313e41bb3ded9c", "sha256": "0b248b251edae058508172f03666595a77850e0551d5faca92ee026876784c68"}, "downloads": -1, "filename": "liger_kernel_nightly-0.6.1.dev20250728181841-py3-none-any.whl", "has_sig": false, "md5_digest": "6e2ba2bd7d292e9eae313e41bb3ded9c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 186439, "upload_time": "2025-07-28T18:18:49", "upload_time_iso_8601": "2025-07-28T18:18:49.406643Z", "url": "https://files.pythonhosted.org/packages/41/fc/62db9d3b0fd5c0f6864790947e2f750b1d7477cddc6ce3d01c6c79ec4aa3/liger_kernel_nightly-0.6.1.dev20250728181841-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "1b77e4d2264f16bb677fc16b06f631057a220f266f6592334c0869f0f3afee64", "md5": "a21b47a5b21aaf86d69627e1e28c2efd", "sha256": "0477dffadfb9ed0d84222ae1e312a5506445c93bf894cbed81e9fdb54127fcb1"}, "downloads": -1, "filename": "liger_kernel_nightly-0.6.1.dev20250728181841.tar.gz", "has_sig": false, "md5_digest": "a21b47a5b21aaf86d69627e1e28c2efd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3668737, "upload_time": "2025-07-28T18:18:51", "upload_time_iso_8601": "2025-07-28T18:18:51.387282Z", "url": "https://files.pythonhosted.org/packages/1b/77/e4d2264f16bb677fc16b06f631057a220f266f6592334c0869f0f3afee64/liger_kernel_nightly-0.6.1.dev20250728181841.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:18:47 GMT", "package": "chipfoundry-cli", "version": "1.0.3", "json": {"info": {"author": "ChipFoundry", "author_email": "marwan.abbas@chipfoundry.io", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9"], "description": "# ChipFoundry CLI (`cf-cli`)\n\n[![PyPI version](https://img.shields.io/pypi/v/chipfoundry-cli?color=blue)](https://badge.fury.io/py/chipfoundry-cli)\n[![PyPI downloads](https://img.shields.io/pypi/dm/chipfoundry-cli.svg)](https://pypi.org/project/chipfoundry-cli/)\n\nA command-line tool to automate the submission of ChipFoundry projects to the SFTP server.\n\n---\n\n## Overview\n\n`cf-cli` is a user-friendly command-line tool for securely submitting your ChipFoundry project files to the official SFTP server. It automatically collects the required files, generates or updates your project configuration, and uploads everything to the correct location on the server.\n\n---\n\n## Installation\n\nInstall from PyPI:\n\n```bash\npip install chipfoundry-cli\ncf --help\n```\n\n---\n\n## Quick Start\n\n1. **Generate SSH Key** (if you don't have one):\n   ```bash\n   cf keygen\n   ```\n\n2. **Register your key** at [https://chipfoundry.io/sftp-registration](https://chipfoundry.io/sftp-registration)\n\n3. **Configure your credentials**:\n   ```bash\n   cf config\n   ```\n\n4. **Initialize your project**:\n   ```bash\n   cf init\n   ```\n\n5. **Upload your project**:\n   ```bash\n   cf push\n   ```\n\n---\n\n## Project Structure Requirements\n\nYour project directory **must** contain:\n\n- `gds/` directory with **one** of the following:\n  - `user_project_wrapper.gds` (for digital projects)\n  - `user_analog_project_wrapper.gds` (for analog projects)\n  - `openframe_project_wrapper.gds` (for openframe projects)\n- `verilog/rtl/user_defines.v` (required for digital/analog)\n- `.cf/project.json` (optional; will be created/updated automatically)\n\n**Example:**\n```\nmy_project/\n\u251c\u2500\u2500 gds/\n\u2502   \u2514\u2500\u2500 user_project_wrapper.gds\n\u251c\u2500\u2500 verilog/\n\u2502   \u2514\u2500\u2500 rtl/\n\u2502       \u2514\u2500\u2500 user_defines.v\n\u2514\u2500\u2500 .cf/\n    \u2514\u2500\u2500 project.json\n```\n\n---\n\n## Authentication\n\nThe CLI uses SSH key authentication for secure SFTP access:\n\n- **Default key location**: `~/.ssh/chipfoundry-key` (generated by `cf keygen`)\n- **Alternative key**: Specify with `--sftp-key` option\n- **SFTP username**: Required and configured via `cf config`\n\n---\n\n## SFTP Server\n\n- **Default server**: `sftp.chipfoundry.io`\n- **Username format**: `firstname-lastname` (e.g., `john-doe`)\n\n---\n\n## Commands\n\n### Generate SSH Key\n\n```bash\ncf keygen [--overwrite]\n```\n\n- Generates a new RSA SSH key at `~/.ssh/chipfoundry-key`\n- Displays the public key for registration\n- Use `--overwrite` to regenerate an existing key\n- **Next step**: Submit the public key to [https://chipfoundry.io/sftp-registration](https://chipfoundry.io/sftp-registration)\n\n### View SSH Key\n\n```bash\ncf keyview\n```\n\n- Displays the current ChipFoundry SSH public key\n- Useful for viewing your key without generating a new one\n- Shows the same registration instructions as `cf keygen`\n\n### Configure User Credentials\n\n```bash\ncf config\n```\n\n- Prompts for your SFTP username and key path\n- Defaults to `~/.ssh/chipfoundry-key`\n- Only needs to be run once per user/machine\n\n### Initialize a New Project\n\n```bash\ncf init [--project-root DIRECTORY]\n```\n\n- **Smart defaults**: Auto-detects project name from directory and project type from GDS files\n- **Interactive prompts**: Shows detected values in prompts for easy acceptance\n- Creates `.cf/project.json` with project metadata\n- **Note**: GDS hash is generated during `push`, not `init`\n\n### Push a Project (Upload)\n\n```bash\ncf push [OPTIONS]\n```\n\n**Options:**\n- `--project-root`: Specify project directory\n- `--force-overwrite`: Overwrite existing files on SFTP\n- `--dry-run`: Preview what would be uploaded\n- `--sftp-username`: Override configured username\n- `--sftp-key`: Override configured key path\n\n**What happens:**\n1. Collects required project files\n2. Auto-detects project type from GDS file\n3. Updates project configuration and GDS hash\n4. Uploads files to SFTP with progress bars\n5. Shows clean, informative output\n\n### Pull Results\n\n```bash\ncf pull [--project-name NAME]\n```\n\n- Downloads project results from SFTP\n- Saves to `sftp-output/<project_name>/`\n- Shows download progress for each file\n\n### Check Status\n\n```bash\ncf status\n```\n\n- Lists all your projects on the SFTP server\n- Shows which projects have input files and/or results\n- Displays project status in a clean table format\n\n---\n\n## How the GDS Hash Works\n\n- The `user_project_wrapper_hash` in `.cf/project.json` is **automatically generated and updated during `push`**\n- The hash is calculated from the actual GDS file being uploaded\n- This ensures the hash always matches the file you are submitting\n- **You do not need to manage or update the hash manually**\n- The hash is NOT generated during `init` because the GDS file may not exist or may change before submission\n\n---\n\n## What Happens When You Run `cf push`?\n\n1. **File Collection:**\n   - Checks for required GDS and Verilog files\n   - Auto-detects project type (digital, analog, openframe) based on GDS file name\n\n2. **Configuration:**\n   - Creates or updates `.cf/project.json`\n   - Updates the GDS hash and any CLI-overridden fields\n\n3. **SFTP Upload:**\n   - Connects to the SFTP server securely\n   - Creates project directory structure\n   - Uploads files with progress indicators\n   - Shows clean, minimal output\n\n4. **Success:**\n   - Displays confirmation with project location\n\n---\n\n## Examples\n\n### Basic Workflow\n\n```bash\n# Generate SSH key and register it\ncf keygen\n# Copy the displayed key to https://chipfoundry.io/sftp-registration\n\n# Configure your account\ncf config\n# Enter: john-doe\n# Enter: (press Enter for default key)\n\n# Initialize project (in your project directory)\ncf init\n# Project name (detected: my_awesome_project): \n# Project type (digital/analog/openframe) (detected: digital): \n\n# Upload your project\ncf push\n# Connecting to sftp.chipfoundry.io...\n# Uploading project.json \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100%\n# Uploading user_project_wrapper.gds \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100%\n# \u2713 Uploaded to incoming/projects/my_awesome_project\n```\n\n### Advanced Usage\n\n```bash\n# Preview what would be uploaded\ncf push --dry-run\n\n# Force overwrite existing files\ncf push --force-overwrite\n\n# Use different project root\ncf push --project-root /path/to/project\n\n# Check project status\ncf status\n```\n\n---\n\n## Troubleshooting\n\n- **Missing files:**\n  - The tool will error out if required files are missing or if more than one GDS type is present\n\n- **Authentication errors:**\n  - Run `cf keygen` to generate a new key\n  - Ensure your key is registered at [https://chipfoundry.io/sftp-registration](https://chipfoundry.io/sftp-registration)\n  - Check your username with `cf config`\n\n- **SFTP errors:**\n  - Check your network connection\n  - Verify your credentials with `cf config`\n\n- **Project type detection:**\n  - Only one of the recognized GDS files should be present in your `gds/` directory\n\n- **ModuleNotFoundError:**\n  - Upgrade the CLI: `pip install --upgrade chipfoundry-cli`\n\n---\n\n## Support\n\n- For help, contact info@chipfoundry.io or visit [chipfoundry.io](https://chipfoundry.io)\n- For bug reports or feature requests, open an issue on [GitHub](https://github.com/chipfoundry/cf-cli)\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://chipfoundry.io", "keywords": null, "license": "Apache-2.0", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "chipfoundry-cli", "package_url": "https://pypi.org/project/chipfoundry-cli/", "platform": null, "project_url": "https://pypi.org/project/chipfoundry-cli/", "project_urls": {"Homepage": "https://chipfoundry.io", "Repository": "https://github.com/chipfoundry/cf-cli"}, "provides_extra": null, "release_url": "https://pypi.org/project/chipfoundry-cli/1.0.3/", "requires_dist": ["click<9,>=8.0.0", "paramiko<4,>=3.0.0", "rich<14,>=13", "toml<1.0,>=0.10"], "requires_python": ">=3.8.0", "summary": "CLI tool to automate ChipFoundry project submission to SFTP server", "version": "1.0.3", "yanked": false, "yanked_reason": null}, "last_serial": 30388555, "urls": [{"comment_text": null, "digests": {"blake2b_256": "94787462e2d56be8357664aa08ece1220534a581ce0af7ff4db78e6f71946a38", "md5": "2e7d6dd684f392171189474b2fbc7201", "sha256": "d1f8861a4a39b7daa699f6662b2677e81d1e04bafe59a90610fd74c0d575c06f"}, "downloads": -1, "filename": "chipfoundry_cli-1.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "2e7d6dd684f392171189474b2fbc7201", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8.0", "size": 16467, "upload_time": "2025-07-28T18:18:47", "upload_time_iso_8601": "2025-07-28T18:18:47.489742Z", "url": "https://files.pythonhosted.org/packages/94/78/7462e2d56be8357664aa08ece1220534a581ce0af7ff4db78e6f71946a38/chipfoundry_cli-1.0.3-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "a6eb3d1e2c402e38c85e04ccc2599ec2853740d5689101933d84436f30159c5c", "md5": "6c656d1015e4125d0b63b6c4b4df827c", "sha256": "1d0f7ef6d19db41ec1e3cae8cda8f365c838cfe5bb27a9bac39b0c3ff237e291"}, "downloads": -1, "filename": "chipfoundry_cli-1.0.3.tar.gz", "has_sig": false, "md5_digest": "6c656d1015e4125d0b63b6c4b4df827c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8.0", "size": 16596, "upload_time": "2025-07-28T18:18:48", "upload_time_iso_8601": "2025-07-28T18:18:48.608686Z", "url": "https://files.pythonhosted.org/packages/a6/eb/3d1e2c402e38c85e04ccc2599ec2853740d5689101933d84436f30159c5c/chipfoundry_cli-1.0.3.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:18:32 GMT", "package": "djecharts", "version": "0.1.3", "json": {"info": {"author": null, "author_email": "Andr\u00e9 Lopes <andrelopes.code@gmail.com>", "bugtrack_url": null, "classifiers": ["Environment :: Web Environment", "Framework :: Django", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3"], "description": "", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENCE"], "maintainer": null, "maintainer_email": null, "name": "djecharts", "package_url": "https://pypi.org/project/djecharts/", "platform": null, "project_url": "https://pypi.org/project/djecharts/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/djecharts/0.1.3/", "requires_dist": ["django<6.0,>=4.0"], "requires_python": ">=3.12", "summary": "Django Echarts", "version": "0.1.3", "yanked": false, "yanked_reason": null}, "last_serial": 30388552, "urls": [{"comment_text": null, "digests": {"blake2b_256": "326ad5170680654aeabf10c775aa7a8e633178a8518051b37c6cd73ec2771f78", "md5": "368a6d2f2257c04966bb10f25d0550d6", "sha256": "a67ddd5fdbeef2c7bc5b54f04fb678e59e49cfa52861a288e1b65d20d96afd1d"}, "downloads": -1, "filename": "djecharts-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "368a6d2f2257c04966bb10f25d0550d6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.12", "size": 354811, "upload_time": "2025-07-28T18:18:32", "upload_time_iso_8601": "2025-07-28T18:18:32.764933Z", "url": "https://files.pythonhosted.org/packages/32/6a/d5170680654aeabf10c775aa7a8e633178a8518051b37c6cd73ec2771f78/djecharts-0.1.3-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "e9e428d42fc034d9963ad5862addf53d403974b4883bc7bb28c754caa82a3f03", "md5": "e9b383d077352235dfadc2225102a019", "sha256": "da677cd4ba0f5abdbec9ee4ac7ba39f2fb337c09bb84871d0ba905e757b25092"}, "downloads": -1, "filename": "djecharts-0.1.3.tar.gz", "has_sig": false, "md5_digest": "e9b383d077352235dfadc2225102a019", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.12", "size": 352069, "upload_time": "2025-07-28T18:18:34", "upload_time_iso_8601": "2025-07-28T18:18:34.822442Z", "url": "https://files.pythonhosted.org/packages/e9/e4/28d42fc034d9963ad5862addf53d403974b4883bc7bb28c754caa82a3f03/djecharts-0.1.3.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:18:25 GMT", "package": "tensorlake", "version": "0.2.32", "json": {"info": {"author": "Tensorlake Inc.", "author_email": "support@tensorlake.ai", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13"], "description": "", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://github.com/tensorlakeai/tensorlake", "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "tensorlake", "package_url": "https://pypi.org/project/tensorlake/", "platform": null, "project_url": "https://pypi.org/project/tensorlake/", "project_urls": {"Homepage": "https://github.com/tensorlakeai/tensorlake", "Repository": "https://github.com/tensorlakeai/tensorlake"}, "provides_extra": null, "release_url": "https://pypi.org/project/tensorlake/0.2.32/", "requires_dist": ["aiofiles<25.0.0,>=24.1.0", "click==8.2.1", "cloudpickle<4.0.0,>=3.1.0", "grpcio==1.74.0", "grpcio-tools==1.74.0", "httpx-sse<0.5.0,>=0.4.1", "httpx[http2]<1.0,>=0.27.2", "pydantic<3.0,>=2.0", "python-magic<0.5.0,>=0.4.27", "pyyaml<7.0.0,>=6.0.2", "retry<0.10.0,>=0.9.2", "rich==14.0.0", "structlog==25.4.0", "tqdm<5.0.0,>=4.67.1"], "requires_python": "<4.0,>=3.10", "summary": "Tensorlake SDK for Document Ingestion API and Serverless Workflows", "version": "0.2.32", "yanked": false, "yanked_reason": null}, "last_serial": 30388549, "urls": [{"comment_text": null, "digests": {"blake2b_256": "a091f571153028dabad61269658611568f9c81d3cefb2053012c8df562e6a8b9", "md5": "4647c66978d78fef07289026f7c47319", "sha256": "d752b80d76c6eba2120339d455aaa63dd9c5adaf84dd3be1bb2f614013b08d4e"}, "downloads": -1, "filename": "tensorlake-0.2.32-py3-none-any.whl", "has_sig": false, "md5_digest": "4647c66978d78fef07289026f7c47319", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<4.0,>=3.10", "size": 110174, "upload_time": "2025-07-28T18:18:25", "upload_time_iso_8601": "2025-07-28T18:18:25.692243Z", "url": "https://files.pythonhosted.org/packages/a0/91/f571153028dabad61269658611568f9c81d3cefb2053012c8df562e6a8b9/tensorlake-0.2.32-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "c18a1a2448933e210665d13dc2500153439a7062ce110799db5c39885bd4aab8", "md5": "7fa0969f7016ac9763150a634f472fd2", "sha256": "a231eadd403296b27d32c55b9d7909d991ab0167e7c14f07f5a3dd65dcee35ca"}, "downloads": -1, "filename": "tensorlake-0.2.32.tar.gz", "has_sig": false, "md5_digest": "7fa0969f7016ac9763150a634f472fd2", "packagetype": "sdist", "python_version": "source", "requires_python": "<4.0,>=3.10", "size": 75641, "upload_time": "2025-07-28T18:18:27", "upload_time_iso_8601": "2025-07-28T18:18:27.417318Z", "url": "https://files.pythonhosted.org/packages/c1/8a/1a2448933e210665d13dc2500153439a7062ce110799db5c39885bd4aab8/tensorlake-0.2.32.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:18:19 GMT", "package": "leprikon", "version": "4.1.0", "json": {"info": {"author": "Jakub Dor\u0148\u00e1k", "author_email": "jakub.dornak@qbsoftware.cz", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Web Environment", "Framework :: Django", "Framework :: Django :: 3", "Intended Audience :: Education", "License :: OSI Approved :: BSD License", "Natural Language :: Czech", "Natural Language :: English", "Operating System :: POSIX", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Topic :: Education"], "description": "Leprik\u00f3n\n========\n\nLeprik\u00f3n is web information system for leisure centres and other educational organizations.\n\n`www.leprikon.cz <https://www.leprikon.cz/>`__\n\n`Docker image <https://hub.docker.com/r/leprikon/leprikon/>`__\n\n\nInstallation with pip\n---------------------\n\n.. code:: shell\n\n    # create and enter an empty directory of your choice\n    mkdir leprikon && cd leprikon\n\n    # create and activate virtual environment\n    virtualenv env\n    . env/bin/activate\n\n    # install leprikon with all the requirements\n    pip install leprikon\n\n    # create database\n    leprikon migrate\n\n    # create admin user\n    leprikon createsuperuser\n\n    # run development server\n    ./manage.py runserver\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://leprikon.cz/", "keywords": null, "license": "BSD-3-Clause", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "leprikon", "package_url": "https://pypi.org/project/leprikon/", "platform": null, "project_url": "https://pypi.org/project/leprikon/", "project_urls": {"Homepage": "https://leprikon.cz/", "Repository": "https://github.com/leprikon-cz/leprikon"}, "provides_extra": null, "release_url": "https://pypi.org/project/leprikon/4.1.0/", "requires_dist": ["Django<4", "certbot-nginx", "certbot", "django-bankreader", "djangocms-audio", "djangocms-file", "djangocms-googlemap", "djangocms-link", "djangocms-picture", "djangocms-snippet", "djangocms-style", "djangocms-text-ckeditor", "djangocms-video", "djangorestframework", "djangorestframework-camel-case", "django-admin-sortable2<2", "django-cms<4", "djangocms-admin-style<3.2.5", "django-cors-headers", "django-countries", "django-cron", "django-excel", "django-filer", "django-ganalytics", "django-haystack", "django-localflavor", "django-mathfilters", "django-multiselectfield", "django-pays", "django-qr-code", "django-redis", "django-staticfiles-downloader", "django-template-admin", "django-user-unique-email", "django-verified-email-field", "drf-spectacular", "drf-spectacular-sidecar", "gunicorn", "html2rml", "importlib-resources", "ipython", "lxml", "mysqlclient", "psycopg2-binary", "pyexcel-xlsxw", "PyICU", "pypdf", "python-dateutil", "python-memcached", "requests", "sentry-sdk", "setuptools<81", "schwifty", "social-auth-app-django", "sqlparse", "trml2pdf", "Whoosh", "icalendar<7.0.0,>=6.3.1"], "requires_python": "<4.0,>=3.12", "summary": "Django CMS based IS for education", "version": "4.1.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388546, "urls": [{"comment_text": "", "digests": {"blake2b_256": "ebe858d97e9efe72695218cc37ba216b6bc686e4ef0d9fd01f7cc676c438e36d", "md5": "4b20174bbc704601f36fc88af952e5f2", "sha256": "cbc8b4fb4f8d3bb949c76503cea0551132b7e491369ff0f1d82bc029f35a2774"}, "downloads": -1, "filename": "leprikon-4.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "4b20174bbc704601f36fc88af952e5f2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<4.0,>=3.12", "size": 2201853, "upload_time": "2025-07-28T18:18:19", "upload_time_iso_8601": "2025-07-28T18:18:19.900223Z", "url": "https://files.pythonhosted.org/packages/eb/e8/58d97e9efe72695218cc37ba216b6bc686e4ef0d9fd01f7cc676c438e36d/leprikon-4.1.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "9768f57730f5d79b8d7a3d0f2263bae117e604cf37fbab2aa880a50cd0d0da9a", "md5": "bd30b3b82f75ad172b3b68e356af9167", "sha256": "d078ea0647d681db47c87e4268062a3cedac61472cac437ec97790ee237ff2a3"}, "downloads": -1, "filename": "leprikon-4.1.0.tar.gz", "has_sig": false, "md5_digest": "bd30b3b82f75ad172b3b68e356af9167", "packagetype": "sdist", "python_version": "source", "requires_python": "<4.0,>=3.12", "size": 2014890, "upload_time": "2025-07-28T18:18:23", "upload_time_iso_8601": "2025-07-28T18:18:23.896986Z", "url": "https://files.pythonhosted.org/packages/97/68/f57730f5d79b8d7a3d0f2263bae117e604cf37fbab2aa880a50cd0d0da9a/leprikon-4.1.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:17:17 GMT", "package": "django-dbsync", "version": "1.1.1", "json": {"info": {"author": "love dazzell", "author_email": "lovepreetdazzell@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Web Environment", "Framework :: Django", "Framework :: Django :: 3.2", "Framework :: Django :: 4.0", "Framework :: Django :: 4.1", "Framework :: Django :: 4.2", "Framework :: Django :: 5.0", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Database", "Topic :: Internet :: WWW/HTTP :: Dynamic Content", "Topic :: Software Development :: Libraries :: Application Frameworks", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# Django DB Sync\r\n\r\n**Django DB Sync** is a powerful, intelligent database synchronization tool designed specifically for Django projects. It automatically detects and resolves schema differences between your Django models and database tables, eliminating the need for manual migrations in many scenarios.\r\n\r\n## Why Django DB Sync?\r\n\r\nUnlike Django's built-in migrations system, Django DB Sync works by analyzing the current state of your database and comparing it directly with your Django models. This approach is particularly valuable when:\r\n\r\n- Working with legacy databases that weren't created with Django\r\n- Dealing with databases that have been manually modified\r\n- Syncing schemas across different environments\r\n- Cleaning up orphaned tables and unused columns\r\n- Requiring granular control over database schema changes\r\n\r\n## Key Features\r\n\r\n- Multi-Database Support: Works seamlessly with MySQL, PostgreSQL, SQLite, and Oracle\r\n- Intelligent Schema Detection: Automatically compares Django models with actual database schema\r\n- Safety First: Built-in dry-run mode and backup creation before making changes\r\n- Comprehensive Reporting: Detailed HTML reports and colored terminal output\r\n- Orphaned Table Management: Identifies and manages tables without corresponding Django models\r\n- Smart Field Mapping: Intelligent mapping between Django field types and database column types\r\n- Constraint Handling: Proper management of foreign keys, indexes, and other constraints\r\n- Beautiful Interface: Colored terminal output with progress indicators and status updates\r\n\r\n## \ud83d\udee0\ufe0f Core Capabilities\r\n\r\n1. **Table Management**: Create, rename, and manage database tables\r\n2. **Column Operations**: Add, modify, and remove columns with proper type mapping\r\n3. **Constraint Handling**: Manage foreign keys, unique constraints, and indexes\r\n4. **Data Preservation**: Safely modify schemas while preserving existing data\r\n5. **Backup Integration**: Automatic backup creation before destructive operations\r\n6. **Detailed Reporting**: Comprehensive logs and HTML reports of all operations\r\n\r\n## \ud83d\udd27 Technical Highlights\r\n\r\n- **Database Agnostic**: Works with all major database backends supported by Django\r\n- **Type-Safe Operations**: Intelligent field type mapping and validation\r\n- **Transaction Safety**: All operations wrapped in database transactions\r\n- **Extensible Architecture**: Modular design for easy customization and extension\r\n- **Production Ready**: Thoroughly tested with comprehensive error handling\r\n\r\n## Installation\r\n\r\n```bash\r\npip install django-dbsync\r\n```\r\n\r\nAdd to your Django settings:\r\n\r\n```python\r\nINSTALLED_APPS = [\r\n    # ... other apps\r\n    'django_dbsync',\r\n]\r\n\r\n# Optional: Configure django-dbsync\r\nDJANGO_DBSYNC = {\r\n    'DEFAULT_DATABASE': 'default',\r\n    'AUTO_CREATE_TABLES': True,\r\n    'AUTO_ADD_COLUMNS': True,\r\n    'AUTO_DROP_COLUMNS': False,\r\n    'EXCLUDE_APPS': ['admin', 'contenttypes', 'sessions'],\r\n    'COLORED_OUTPUT': True,\r\n    'SHOW_ORPHANED_TABLES': True,\r\n}\r\n```\r\n\r\n## Usage\r\n\r\n### Basic Sync\r\n```bash\r\n# Basic sync commands\r\npython manage.py dbsync  # Sync default database\r\npython manage.py dbsync --database=secondary  # Sync specific database\r\npython manage.py dbsync --dry-run  # Show changes without applying\r\npython manage.py dbsync --auto-approve  # Auto-approve all changes (dangerous!)\r\npython manage.py dbsync --drop-orphaned  # Drop orphaned tables (dangerous!)\r\n```\r\n\r\n### Advanced Options\r\n```bash\r\n# App management\r\npython manage.py dbsync --exclude-apps admin auth contenttypes  # Exclude specific apps\r\npython manage.py dbsync --include-apps myapp otherapp  # Include only specific apps\r\n\r\n# Backup and reporting\r\npython manage.py dbsync --backup  # Create backup before sync\r\npython manage.py dbsync --report json  # Generate JSON report\r\npython manage.py dbsync --report html  # Generate HTML report\r\npython manage.py dbsync --report both  # Generate both JSON and HTML reports\r\n\r\n# Safety checks\r\npython manage.py dbsync --drop-orphaned --dry-run  # Check what would be dropped\r\npython manage.py dbsync --suggest-manual-commands  # Show manual SQL commands\r\npython manage.py dbsync --generate-orphaned-models  # Generate models for orphaned tables\r\n```\r\n\r\n### Database Check\r\n```bash\r\n# Database checking commands\r\npython manage.py dbcheck  # Check database schema\r\npython manage.py dbcheck --database=secondary  # Check specific database\r\npython manage.py dbcheck --table=my_table  # Show specific table details\r\npython manage.py dbcheck --compare-models  # Compare with Django models\r\npython manage.py dbcheck --check-case-mismatches  # Check for case mismatches\r\npython manage.py dbcheck --check-name-conflicts  # Check for name conflicts\r\npython manage.py dbcheck --verbose  # Show detailed information\r\npython manage.py dbcheck --fix  # Attempt to fix issues automatically\r\npython manage.py dbcheck --include-apps=app1,app2  # Check specific apps only\r\npython manage.py dbcheck --include-tables=table1,table2  # Check specific tables only\r\n```\r\n\r\n## Configuration\r\n\r\n### Database Settings\r\n\r\nSupport for multiple databases: \r\n\r\n```python\r\nDATABASES = {\r\n    'default': {\r\n        'ENGINE': 'django.db.backends.mysql',\r\n        'NAME': 'main_db',\r\n        'USER': 'user',\r\n        'PASSWORD': 'pass',\r\n        'HOST': 'localhost',\r\n    },\r\n    'analytics': {\r\n        'ENGINE': 'django.db.backends.postgresql',\r\n        'NAME': 'analytics_db',\r\n        'USER': 'user',\r\n        'PASSWORD': 'pass',\r\n        'HOST': 'localhost',\r\n    }\r\n}\r\n\r\n# Sync configuration per database\r\nDJANGO_DBSYNC = {\r\n    'CUSTOM_DATABASES': {\r\n        'analytics': {\r\n            'AUTO_DROP_COLUMNS': True,\r\n            'EXCLUDE_APPS': ['admin'],\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n### Complete Settings Reference\r\n\r\n```python\r\nDJANGO_DBSYNC = {\r\n    # Database configuration\r\n    'DEFAULT_DATABASE': 'default',\r\n    'CUSTOM_DATABASES': None,\r\n    \r\n    # Sync behavior\r\n    'AUTO_CREATE_TABLES': True,\r\n    'AUTO_ADD_COLUMNS': True,\r\n    'AUTO_DROP_COLUMNS': False,\r\n    'AUTO_RENAME_TABLES': False,\r\n    'AUTO_FIX_TABLE_CASE': True,  # Automatically fix table name case mismatches\r\n    'BACKUP_BEFORE_SYNC': True,\r\n    \r\n    # Output settings\r\n    'COLORED_OUTPUT': True,\r\n    'VERBOSE_LOGGING': True,\r\n    'SHOW_PROGRESS': True,\r\n    \r\n    # Safety settings\r\n    'EXCLUDE_APPS': ['sessions', 'admin', 'contenttypes'],\r\n    'EXCLUDE_TABLES': [],\r\n    'DRY_RUN_MODE': False,\r\n    \r\n    # Report settings\r\n    'GENERATE_HTML_REPORT': False,\r\n    'REPORT_OUTPUT_DIR': 'dbsync_reports/',\r\n    'SHOW_ORPHANED_TABLES': True,\r\n}\r\n```\r\n\r\n## Supported Field Types\r\n\r\nAll Django field types are supported across MySQL, PostgreSQL, and SQLite:\r\n\r\n- AutoField, BigAutoField\r\n- CharField, TextField, EmailField, URLField, SlugField\r\n- IntegerField, BigIntegerField, SmallIntegerField\r\n- PositiveIntegerField, PositiveSmallIntegerField\r\n- FloatField, DecimalField\r\n- BooleanField\r\n- DateField, DateTimeField, TimeField\r\n- UUIDField, JSONField\r\n- FileField, ImageField\r\n- ForeignKey, OneToOneField, ManyToManyField\r\n\r\n## Table Name Case Handling\r\n\r\nDjango-dbsync automatically detects and handles table name case mismatches between your Django models and the database. This is common when:\r\n\r\n- Your model has `db_table = 'abcd'` but the database table is `ABCD`\r\n- Database systems are case-insensitive but Django models use specific casing\r\n- Tables were created with different naming conventions\r\n\r\n### Automatic Case Fixing\r\n\r\nBy default, the tool will automatically fix case-only mismatches (when `AUTO_FIX_TABLE_CASE = True`):\r\n\r\n```bash\r\n# The tool will automatically rename 'ABCD' to 'abcd'\r\npython manage.py dbsync\r\n```\r\n\r\n### Manual Control\r\n\r\nTo disable automatic case fixing and get prompted for each rename:\r\n\r\n```python\r\nDJANGO_DBSYNC = {\r\n    'AUTO_FIX_TABLE_CASE': False,\r\n}\r\n```\r\n\r\n### Checking for Case Mismatches\r\n\r\nTo check for table name case mismatches without fixing them:\r\n\r\n```bash\r\npython manage.py dbcheck --check-case-mismatches\r\n```\r\n\r\nThis will show you all mismatches found and provide guidance on how to fix them.\r\n\r\n### Manual SQL Commands for Table Renames\r\n\r\nWhen table name conflicts are detected, you can get manual SQL commands to resolve them:\r\n\r\n```bash\r\npython manage.py dbsync --dry-run --suggest-manual-commands\r\n```\r\n\r\nThis will show you the exact SQL commands needed to rename tables manually:\r\n\r\n```\r\n============================================================\r\n\ud83d\udd27 MANUAL SQL COMMANDS FOR TABLE RENAMES\r\n============================================================\r\nThe following SQL commands can be run manually to rename tables:\r\n\r\n1. MySQL case-insensitive conflict resolution: 'publisher_detail2' \u2192 'Publisher_detail2'\r\n   SQL: RENAME TABLE `publisher_detail2` TO `Publisher_detail2`;\r\n\r\n\ud83d\udca1 Instructions:\r\n   1. Connect to your database using your preferred SQL client\r\n   2. Run the commands above one by one\r\n   3. Run 'python manage.py dbsync' again to complete the sync\r\n   4. Make sure to backup your database before running these commands!\r\n```\r\n\r\nThis gives you full control over table renaming operations while ensuring data safety.\r\n\r\n**Note:** Manual SQL commands are automatically displayed in dry-run mode, so you don't need the `--suggest-manual-commands` flag anymore.\r\n\r\n### Generating Models for Orphaned Tables\r\n\r\nWhen orphaned tables are found, you can generate Django models for them:\r\n\r\n```bash\r\npython manage.py dbsync --dry-run --generate-orphaned-models\r\n```\r\n\r\nThis creates a Python file with Django models for all orphaned tables:\r\n\r\n```python\r\n# Django Models for Orphaned Tables\r\n# Generated by django-dbsync on 2025-07-28 10:34:31\r\n# \r\n# Instructions:\r\n# 1. Copy the models you want to keep to your Django app's models.py\r\n# 2. Remove the 'managed = False' line if you want Django to manage the table\r\n# 3. Update the Meta class as needed\r\n# 4. Run 'python manage.py makemigrations' and 'python manage.py migrate'\r\n\r\nfrom django.db import models\r\n\r\n# Table: publisher\r\n# Rows: 0, Size: 0.02 MB\r\nclass Publisher(models.Model):\r\n    \"\"\"\r\n    Auto-generated model for table 'publisher'\r\n    Generated by django-dbsync\r\n    \"\"\"\r\n    name = models.CharField(max_length=100, null=False, blank=False)\r\n    website = models.CharField(max_length=200, null=False, blank=False)\r\n    created_at = models.DateTimeField(null=False, blank=False)\r\n\r\n    class Meta:\r\n        db_table = 'publisher'\r\n        managed = False  # Django won't manage this table\r\n\r\n    def __str__(self):\r\n        return f'Publisher(id={self.id})'\r\n```\r\n\r\n**Benefits:**\r\n- **Easy retention**: Copy models to keep orphaned tables\r\n- **Auto-generated**: No manual model writing needed\r\n- **Safe**: Uses `managed = False` by default\r\n- **Complete**: Includes all field types and constraints\r\n\r\n### Table Name Conflicts\r\n\r\nSometimes databases can have both lowercase and uppercase versions of the same table name (e.g., `publisher` and `Publisher`). This can cause issues with table renaming operations.\r\n\r\nTo check for table name conflicts:\r\n\r\n```bash\r\npython manage.py dbcheck --check-name-conflicts\r\n```\r\n\r\nThis will identify any tables that have case conflicts and provide guidance on how to resolve them.\r\n\r\n**Example conflict scenario:**\r\n- Database has both `publisher` and `Publisher` tables\r\n- Django model expects `Publisher` \r\n- The tool will detect this conflict and avoid the rename operation\r\n- You'll need to manually resolve the conflict before syncing\r\n\r\n## Example Output\r\n\r\n```\r\nDjango Database Sync v1.0.2\r\n==================================================\r\nStarting synchronization...\r\n\r\n\u2705 myapp.User\r\n   - Added column 'phone' to 'users'\r\n   - Modified column 'email' in 'users'\r\n\r\n\u26a0\ufe0f  myapp.Order\r\n   - Table 'orders_old' renamed to 'orders'\r\n   - Extra column 'temp_field' in 'orders' (kept)\r\n\r\n\u274c myapp.Product\r\n   - Failed to add column 'description'\r\n\r\n\u26a0\ufe0f  Orphaned Tables (2 found):\r\n\ud83d\uddc3\ufe0f  old_backup_table - 1,247 rows, 2.45 MB\r\n\ud83d\uddc3\ufe0f  temp_migration - 0 rows, 0.01 MB\r\n\r\nSynchronization completed!\r\n```\r\n\r\n## Contributing\r\n\r\n1. Fork the repository\r\n2. Create a feature branch\r\n3. Add tests for new functionality\r\n4. Ensure all tests pass\r\n5. Submit a pull request\r\n\r\n\r\n## Support\r\n\r\n- GitHub Issues: https://github.com/Lovedazzell/db_sync/issues\r\n- Documentation: https://django-dbsync.readthedocs.io/\r\n- Email: lovepreetdazzell@gmail.com\r\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "Keywords", "License", "Platform", "Project-Url", "Provides-Extra", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/Lovedazzell/django-dbsync", "keywords": "django database sync migration schema synchronization", "license": "MIT", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "django-dbsync", "package_url": "https://pypi.org/project/django-dbsync/", "platform": "any", "project_url": "https://pypi.org/project/django-dbsync/", "project_urls": {"Bug Tracker": "https://github.com/Lovedazzell/django-dbsync/issues", "Documentation": "https://django-dbsync.readthedocs.io/", "Homepage": "https://github.com/Lovedazzell/django-dbsync", "Source Code": "https://github.com/Lovedazzell/django-dbsync"}, "provides_extra": ["dev", "mysql", "postgresql", "oracle"], "release_url": "https://pypi.org/project/django-dbsync/1.1.1/", "requires_dist": ["Django<6.0,>=3.2", "colorama>=0.4.0", "tabulate>=0.8.0", "pytest>=6.0; extra == \"dev\"", "pytest-django>=4.0; extra == \"dev\"", "pytest-cov>=2.0; extra == \"dev\"", "black>=22.0; extra == \"dev\"", "flake8>=4.0; extra == \"dev\"", "isort>=5.0; extra == \"dev\"", "mypy>=0.900; extra == \"dev\"", "mysqlclient>=2.0; extra == \"mysql\"", "psycopg2-binary>=2.8; extra == \"postgresql\"", "cx_Oracle>=8.0; extra == \"oracle\""], "requires_python": ">=3.8", "summary": "Intelligent database synchronization tool for Django projects", "version": "1.1.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388543, "urls": [{"comment_text": null, "digests": {"blake2b_256": "eb131600dd9df7b0cf0205d8ae082f7d8fca8fcd51f5f1e2fa543b7d1cc1bd99", "md5": "18a9ee93ecdcfeee4b0ca3b79ab64e63", "sha256": "78b974ae14e7780854af47aa90040c52a686db7ee39b5ccdbb5c145833bd113a"}, "downloads": -1, "filename": "django_dbsync-1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "18a9ee93ecdcfeee4b0ca3b79ab64e63", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 44021, "upload_time": "2025-07-28T18:17:17", "upload_time_iso_8601": "2025-07-28T18:17:17.192276Z", "url": "https://files.pythonhosted.org/packages/eb/13/1600dd9df7b0cf0205d8ae082f7d8fca8fcd51f5f1e2fa543b7d1cc1bd99/django_dbsync-1.1.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "5c03b7701aeb7c8a71400e11c7842d92fc9b65a9f07fa08f37c2cc08b110f463", "md5": "77e910890dd05f4ee911d55a23d15b6d", "sha256": "f7dfff161f9c6e7bbd5be7af04bbc3e0ce5639a9b4525172ec889b24a6ee8742"}, "downloads": -1, "filename": "django_dbsync-1.1.1.tar.gz", "has_sig": false, "md5_digest": "77e910890dd05f4ee911d55a23d15b6d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 41488, "upload_time": "2025-07-28T18:17:18", "upload_time_iso_8601": "2025-07-28T18:17:18.587327Z", "url": "https://files.pythonhosted.org/packages/5c/03/b7701aeb7c8a71400e11c7842d92fc9b65a9f07fa08f37c2cc08b110f463/django_dbsync-1.1.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:17:00 GMT", "package": "clickhouse-mcp-agent", "version": "0.1.0a1", "json": {"info": {"author": null, "author_email": "Altu\u011f Ceylan <altug.ceylan.yes@gmail.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Topic :: Database", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# ClickHouse MCP Agent\n\nA PydanticAI agent that integrates with ClickHouse databases using the Model Context Protocol (MCP).\n\n## Features\n\n- MCP-based ClickHouse integration with secure database access\n- Structured output with natural language analysis and SQL transparency\n- Flexible connection management for different ClickHouse instances\n- Built-in configurations for common scenarios\n\n## Installation\n\n```bash\n./setup.sh               # Setup virtual environment and install\ncp .env.example .env      # Configure environment\n# Edit .env with your GOOGLE_API_KEY\n```\n\n## Run\n\n```bash\n./run.sh                 # Activate environment and run project\n```\n\n## Usage\n\n### Basic Query\n\n```python\nfrom agent import query_clickhouse\n\nresult = await query_clickhouse(\n    query=\"What are the top 5 GitHub repositories by stars?\",\n    connection=\"playground\"\n)\nprint(f\"Analysis: {result.analysis}\")\n```\n\n### Custom Connection\n\n```python\nfrom agent import ClickHouseConfig\n\nconfig = ClickHouseConfig(\n    name=\"production\",\n    host=\"clickhouse.company.com\",\n    port=\"8443\",\n    user=\"analyst\",\n    password=\"secret\"\n)\n\nresult = await query_clickhouse(query=\"SHOW TABLES\", connection=config)\n```\n\n### Current Usage (Library Import)\n\n```python\n# When installed as a library: pip install clickhouse-mcp-agent\nfrom agent import query_clickhouse, ClickHouseConfig\n\n# Basic usage\nresult = await query_clickhouse(\"SHOW DATABASES\", \"playground\")\n\n# RBAC with dynamic user credentials (already supported)\nuser_config = ClickHouseConfig(\n    name=\"user_session\",\n    host=\"clickhouse.company.com\",\n    user=\"analyst_jane\",\n    password=\"jane_specific_password\"\n)\nresult = await query_clickhouse(\"SELECT * FROM user_logs\", user_config)\n\n# Completely dynamic \nresult = await query_clickhouse(\n    query=\"SHOW TABLES\",\n    connection=ClickHouseConfig(\n        name=\"runtime\",\n        host=\"dynamic.clickhouse.com\",\n        user=\"runtime_user\",\n        password=\"runtime_pass\"\n    ),\n    model=\"gemini-1.5-flash\",\n    api_key=\"your-google-api-key-here\" \n)\n```\n\n### Environment Variables\n\nThe project automatically loads from `.env` file:\n\n```python\nresult = await query_clickhouse(query=\"SELECT 1\", connection=\"env\")\n```\n\n## Built-in Connections\n\n- `playground`: ClickHouse SQL playground (public demo data)\n- `local`: Local instance (localhost:9000)\n- `env`: From environment variables\n\n## CLI\n\n```bash\n./run.sh                         # Automated run script\n# OR manually:\nsource .venv/bin/activate\nclickhouse-mcp-demo             # CLI command from pyproject.toml\npython -m agent.main            # Direct module execution\n```\n\n## Output\n\nReturns `ClickHouseOutput` with:\n\n- `analysis`: Natural language results with SQL queries mentioned in the response\n\n## Requirements\n\n- Python 3.10+\n- AI API key (Google/Gemini) - can be set via environment variable, .env file, or passed directly to the function\n\nAll other dependencies (UV, MCP servers, etc.) are handled automatically by pyproject.toml.\n\n## Roadmap\n\n### \u2705 Completed Features\n\n- [x] **MCP Integration**: PydanticAI + ClickHouse MCP server integration\n- [x] **Query Execution**: SQL query execution via MCP tools\n- [x] **Schema Inspection**: Database, table, and column exploration\n- [x] **Connection Management**: Multiple connection configurations (playground, local, env)\n- [x] **RBAC Support**: Pass different user credentials dynamically via ClickHouseConfig\n- [x] **Dynamic Connections**: Runtime connection configuration without environment dependencies\n- [x] **Direct API Key Passing**: Pass AI API keys directly to functions without environment variables\n- [x] **Structured Output**: ClickHouseOutput with analysis, SQL, and confidence\n- [x] **CLI Interface**: Command-line tool via clickhouse-mcp-demo\n\n### \ud83d\udea7 Planned Features (Discussed)\n\n#### Enhanced Conversation Support\n\n- [ ] **Message History**: Add message_history parameter to query_clickhouse() for conversation continuity\n- [ ] **Conversational Agent**: ConversationalClickHouseAgent class for persistent memory across queries\n\n#### Model Support\n\n- [ ] **Model Agnostic Support**: Support for different AI models beyond Gemini\n\n---\n\n### Contributing\n\nHave ideas for new features? Found something missing?\n\n1. Check existing issues/discussions\n2. Open a feature request with use case details\n3. Consider contributing via pull request\n\n**Current Focus**: Message history integration and model agnostic support.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": "clickhouse, mcp, ai, database, analysis", "license": "MIT", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "clickhouse-mcp-agent", "package_url": "https://pypi.org/project/clickhouse-mcp-agent/", "platform": null, "project_url": "https://pypi.org/project/clickhouse-mcp-agent/", "project_urls": {"Homepage": "https://github.com/AranNomante/clickhousemcp", "Issues": "https://github.com/AranNomante/clickhousemcp/issues", "Repository": "https://github.com/AranNomante/clickhousemcp"}, "provides_extra": ["dev", "examples"], "release_url": "https://pypi.org/project/clickhouse-mcp-agent/0.1.0a1/", "requires_dist": ["pydantic-ai-slim[google,mcp]", "mcp-clickhouse", "python-dotenv", "uv", "pytest; extra == \"dev\"", "pytest-asyncio; extra == \"dev\"", "black; extra == \"dev\"", "isort; extra == \"dev\"", "mypy; extra == \"dev\"", "flake8; extra == \"dev\"", "jupyter; extra == \"examples\"", "ipython; extra == \"examples\""], "requires_python": ">=3.10", "summary": "AI agent for ClickHouse database analysis via MCP", "version": "0.1.0a1", "yanked": false, "yanked_reason": null}, "last_serial": 30388540, "urls": [{"comment_text": null, "digests": {"blake2b_256": "d45bafed229e7421171af25f97195993c388526d2cb1199496e243903cd7085e", "md5": "9e10bbe69020a44cbdd13ef9dc631edf", "sha256": "17266840bf48660160bc0325f0616176e517fbcfb6eba7a9b9543b55f1101a63"}, "downloads": -1, "filename": "clickhouse_mcp_agent-0.1.0a1-py3-none-any.whl", "has_sig": false, "md5_digest": "9e10bbe69020a44cbdd13ef9dc631edf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.10", "size": 10956, "upload_time": "2025-07-28T18:17:00", "upload_time_iso_8601": "2025-07-28T18:17:00.873774Z", "url": "https://files.pythonhosted.org/packages/d4/5b/afed229e7421171af25f97195993c388526d2cb1199496e243903cd7085e/clickhouse_mcp_agent-0.1.0a1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "5f5bc54fa75eba55473686832827f2d7f8ef650b0c739dddba12277216723d8c", "md5": "a3dde088485e3c839649633f08acc47f", "sha256": "2fd9de7c1adb0ffee4e22393aad920e56820e3b846a9eb3c6c3088f97d201d03"}, "downloads": -1, "filename": "clickhouse_mcp_agent-0.1.0a1.tar.gz", "has_sig": false, "md5_digest": "a3dde088485e3c839649633f08acc47f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.10", "size": 11805, "upload_time": "2025-07-28T18:17:02", "upload_time_iso_8601": "2025-07-28T18:17:02.304740Z", "url": "https://files.pythonhosted.org/packages/5f/5b/c54fa75eba55473686832827f2d7f8ef650b0c739dddba12277216723d8c/clickhouse_mcp_agent-0.1.0a1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:16:37 GMT", "package": "decksmith", "version": "0.1.7", "json": {"info": {"author": "Julio Cabria", "author_email": "juliocabria@tutanota.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: GNU General Public License v2 (GPLv2)", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.13"], "description": "# DeckSmith\n\n*A command-line application to dynamically generate decks of cards from a JSON specification and a CSV data file, inspired by nandeck.*\n\n<br>\n<p align=\"center\">\n  <img width=\"600\" src=\"https://raw.githubusercontent.com/Julynx/decksmith/refs/heads/main/docs/assets/decksmith.png\">\n</p>\n<br>\n\nDeckSmith is ideal for automating the creation of all kinds of decks, including TCG decks, tarot decks, business cards, and even slides.\n\n## Features\n\n- [Initialize a sample project and edit it instead of starting from scratch](https://github.com/Julynx/decksmith/blob/main/docs/DOCS.md#creating-a-project)\n\n- [Include images](https://github.com/Julynx/decksmith/blob/main/docs/DOCS.md#images), [text](https://github.com/Julynx/decksmith/blob/main/docs/DOCS.md#text), [and different kinds of shapes](https://github.com/Julynx/decksmith/blob/main/docs/DOCS.md#shapes)\n\n- [Link any field to a column in the CSV file](https://github.com/Julynx/decksmith/blob/main/docs/DOCS.md#basic-example-with-deckcsv)\n\n- [Position elements absolutely or relative to other elements, using anchors to simplify placement](https://github.com/Julynx/decksmith/blob/main/docs/DOCS.md#positioning)\n\n- [Transform images using filters like crop, resize, rotate, or flip](https://github.com/Julynx/decksmith/blob/main/docs/DOCS.md#images)\n\n- [Build card images and export to PDF for printing](https://github.com/Julynx/decksmith/blob/main/docs/DOCS.md#building-the-deck)\n\n## Getting started\n\nTo start creating decks, check out [Getting Started](https://github.com/Julynx/decksmith/blob/main/docs/DOCS.md/#getting-started).\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://github.com/Julynx/decksmith", "keywords": null, "license": "GPL-2.0-only", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "decksmith", "package_url": "https://pypi.org/project/decksmith/", "platform": null, "project_url": "https://pypi.org/project/decksmith/", "project_urls": {"Homepage": "https://github.com/Julynx/decksmith"}, "provides_extra": null, "release_url": "https://pypi.org/project/decksmith/0.1.7/", "requires_dist": ["click", "jval==1.0.6", "pandas", "pillow>=11.3.0", "reportlab>=4.4.3", "pytest; extra == \"dev\"", "poetry; extra == \"dev\""], "requires_python": ">=3.13", "summary": "A command-line application to dynamically generate decks of cards from a JSON specification and a CSV data file, inspired by nandeck.", "version": "0.1.7", "yanked": false, "yanked_reason": null}, "last_serial": 30388534, "urls": [{"comment_text": "", "digests": {"blake2b_256": "e1de232f6d0e7c9b6701ad7a7f49798f0ca909fcd692fc5a079968e002d573d2", "md5": "487dd1cccc80aca35bbc79e9f6fdb691", "sha256": "89a9ea4f26bae4703f21eec261ad29b57c14952cb8b3c3b65a3de7ae3990d307"}, "downloads": -1, "filename": "decksmith-0.1.7-py3-none-any.whl", "has_sig": false, "md5_digest": "487dd1cccc80aca35bbc79e9f6fdb691", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.13", "size": 13704, "upload_time": "2025-07-28T18:16:37", "upload_time_iso_8601": "2025-07-28T18:16:37.949045Z", "url": "https://files.pythonhosted.org/packages/e1/de/232f6d0e7c9b6701ad7a7f49798f0ca909fcd692fc5a079968e002d573d2/decksmith-0.1.7-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "2ca730ca35c057368aacc64478656e89d1e0cacb7dc6d2d6b9a8557aa7dbf90b", "md5": "ed5b4b6b501e45092a1a9e584237736a", "sha256": "63bbcf6a66507e3ae5fa40d6198600241070ac84d05cb599807b1c5b9e9f930b"}, "downloads": -1, "filename": "decksmith-0.1.7.tar.gz", "has_sig": false, "md5_digest": "ed5b4b6b501e45092a1a9e584237736a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.13", "size": 10785, "upload_time": "2025-07-28T18:16:39", "upload_time_iso_8601": "2025-07-28T18:16:39.071963Z", "url": "https://files.pythonhosted.org/packages/2c/a7/30ca35c057368aacc64478656e89d1e0cacb7dc6d2d6b9a8557aa7dbf90b/decksmith-0.1.7.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:16:25 GMT", "package": "dataprobe", "version": "2.1.1", "json": {"info": {"author": "SANTHOSH KRISHNAN R", "author_email": "santhoshkrishnan3006@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Software Development :: Debuggers", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "\ufeff## Project description\r\n\r\n# DataProbe\r\n\r\n**DataProbe** is a comprehensive Python toolkit for debugging, profiling, and optimizing data pipelines. It provides powerful tools to track data lineage, identify bottlenecks, monitor memory usage, and visualize pipeline execution flow with **enterprise-grade visualizations**.\r\n\r\n## \ud83c\udfa8 **NEW: Enterprise-Grade Visualizations**\r\n\r\nDataProbe v2.1.0 introduces comprehensive pipeline debugging capabilities with professional-quality visualizations, intelligent optimization recommendations, advanced memory profiling, data lineage tracking, and enterprise-grade reporting - providing complete visibility and control over your data pipeline performance.\r\n\r\n### **Dashboard Features**\r\n\r\n#### \ud83c\udfe2 **Enterprise Dashboard**\r\n\r\n- **KPI Panels**: Real-time success rates, duration, memory usage\r\n- **Pipeline Flowchart**: Interactive operation flow with status indicators\r\n- **Performance Analytics**: Memory usage timelines with peak detection\r\n- **Data Insights**: Comprehensive lineage and transformation tracking\r\n\r\n```python\r\n# Generate enterprise dashboard\r\ndebugger.visualize_pipeline()\r\n```\r\n\r\n#### \ud83c\udf10 **3D Pipeline Network**\r\n\r\n- **3D Visualization**: Interactive network showing operation relationships\r\n- **Performance Mapping**: Z-axis represents operation duration\r\n- **Status Color-coding**: Visual error and bottleneck identification\r\n\r\n```python\r\n# Create 3D network visualization\r\ndebugger.create_3d_pipeline_visualization()\r\n```\r\n\r\n#### \ud83d\udcca **Executive Reports**\r\n\r\n- **Multi-page Reports**: Professional stakeholder-ready documentation\r\n- **Performance Trends**: Dual-axis charts showing duration and memory patterns\r\n- **Optimization Recommendations**: AI-powered suggestions for improvements\r\n- **Data Quality Metrics**: Comprehensive pipeline health scoring\r\n\r\n```python\r\n# Generate executive report\r\ndebugger.generate_executive_report()\r\n```\r\n\r\n### **Color-Coded Status System**\r\n\r\n- \ud83d\udfe2 **Success**: Operations completed without issues\r\n- \ud83d\udfe1 **Warning**: Performance bottlenecks detected\r\n- \ud83d\udd34 **Error**: Failed operations requiring attention\r\n- \ud83d\udfe6 **Info**: Data flow and transformation indicators\r\n\r\n## \ud83d\ude80 Features\r\n\r\n### PipelineDebugger\r\n\r\n* **\ud83d\udd0d Operation Tracking** : Automatically track execution time, memory usage, and data shapes for each operation\r\n* **\ud83d\udcca Enterprise-Grade Visualizations** : Professional dashboards, 3D networks, and executive reports\r\n* **\ud83d\udcbe Memory Profiling** : Monitor memory usage and identify memory-intensive operations\r\n* **\ud83d\udd17 Data Lineage** : Track data transformations and column changes throughout the pipeline\r\n* **\u26a0\ufe0f Bottleneck Detection** : Automatically identify slow operations and memory peaks\r\n* **\ud83d\udcc8 Performance Reports** : Generate comprehensive debugging reports with optimization suggestions\r\n* **\ud83c\udfaf Error Tracking** : Capture and track errors with full traceback information\r\n* **\ud83c\udf33 Nested Operations** : Support for tracking nested function calls and their relationships\r\n\r\n## \ud83d\udce6 Installation\r\n\r\n```bash\r\npip install dataprobe\r\n```\r\n\r\nFor development installation:\r\n\r\n```bash\r\ngit clone https://github.com/santhoshkrishnan30/dataprobe.git\r\ncd dataprobe\r\npip install -e \".[dev]\"\r\n```\r\n\r\n## \ud83c\udfaf Quick Start\r\n\r\n### Basic Usage with Enhanced Visualizations\r\n\r\n```python\r\nfrom dataprobe import PipelineDebugger\r\nimport pandas as pd\r\n\r\n# Initialize the debugger with enhanced features\r\ndebugger = PipelineDebugger(\r\n    name=\"My_ETL_Pipeline\",\r\n    track_memory=True,\r\n    track_lineage=True\r\n)\r\n\r\n# Use decorators to track operations\r\n@debugger.track_operation(\"Load Data\")\r\ndef load_data(file_path):\r\n    return pd.read_csv(file_path)\r\n\r\n@debugger.track_operation(\"Transform Data\")\r\ndef transform_data(df):\r\n    df['new_column'] = df['value'] * 2\r\n    return df\r\n\r\n# Run your pipeline\r\ndf = load_data(\"data.csv\")\r\ndf = transform_data(df)\r\n\r\n# Generate enterprise-grade visualizations\r\ndebugger.visualize_pipeline()              # Enterprise dashboard\r\ndebugger.create_3d_pipeline_visualization() # 3D network view  \r\ndebugger.generate_executive_report()       # Executive report\r\n\r\n# Get AI-powered optimization suggestions\r\nsuggestions = debugger.suggest_optimizations()\r\nfor suggestion in suggestions:\r\n    print(f\"\ud83d\udca1 {suggestion['suggestion']}\")\r\n\r\n# Print summary and reports\r\ndebugger.print_summary()\r\nreport = debugger.generate_report()\r\n```\r\n\r\n### Memory Profiling\r\n\r\n```python\r\n@debugger.profile_memory\r\ndef memory_intensive_operation():\r\n    large_df = pd.DataFrame(np.random.randn(1000000, 50))\r\n    result = large_df.groupby(large_df.index % 1000).mean()\r\n    return result\r\n```\r\n\r\n### DataFrame Analysis\r\n\r\n```python\r\n# Analyze DataFrames for potential issues\r\ndebugger.analyze_dataframe(df, name=\"Sales Data\")\r\n```\r\n\r\n## \ud83d\udcca Example Output\r\n\r\n### Enterprise Dashboard\r\n\r\nProfessional KPI dashboard with real-time metrics, pipeline flowchart, memory analytics, and performance insights.\r\n\r\n### Pipeline Summary\r\n\r\n```\r\nPipeline Summary: My_ETL_Pipeline\r\n\u251c\u2500\u2500 Execution Statistics\r\n\u2502   \u251c\u2500\u2500 Total Operations: 5\r\n\u2502   \u251c\u2500\u2500 Total Duration: 2.34s\r\n\u2502   \u2514\u2500\u2500 Total Memory Used: 125.6MB\r\n\u251c\u2500\u2500 Bottlenecks (1)\r\n\u2502   \u2514\u2500\u2500 Transform Data: 1.52s\r\n\u2514\u2500\u2500 Memory Peaks (1)\r\n    \u2514\u2500\u2500 Load Large Dataset: +85.3MB\r\n```\r\n\r\n### Optimization Suggestions\r\n\r\n```\r\n\ud83d\udca1 OPTIMIZATION RECOMMENDATIONS:\r\n\r\n1. [PERFORMANCE] Transform Data\r\n   Issue: Operation took 1.52s\r\n   \ud83d\udca1 Consider optimizing this operation or parallelizing if possible\r\n\r\n2. [MEMORY] Load Large Dataset  \r\n   Issue: High memory usage: +85.3MB\r\n   \ud83d\udca1 Consider processing data in chunks or optimizing memory usage\r\n```\r\n\r\n## \ud83d\udd27 Advanced Features\r\n\r\n### Multiple Visualization Options\r\n\r\n```python\r\n# Enterprise dashboard - Professional KPI dashboard\r\ndebugger.visualize_pipeline()\r\n\r\n# 3D network visualization - Interactive operation relationships  \r\ndebugger.create_3d_pipeline_visualization()\r\n\r\n# Executive report - Multi-page stakeholder documentation\r\ndebugger.generate_executive_report()\r\n```\r\n\r\n### Data Lineage Tracking\r\n\r\n```python\r\n# Export data lineage information\r\nlineage_json = debugger.export_lineage(format=\"json\")\r\n\r\n# Track column changes automatically\r\n@debugger.track_operation(\"Add Features\")\r\ndef add_features(df):\r\n    df['feature_1'] = df['value'].rolling(7).mean()\r\n    df['feature_2'] = df['value'].shift(1)\r\n    return df\r\n```\r\n\r\n### Custom Metadata\r\n\r\n```python\r\n@debugger.track_operation(\"Process Batch\", batch_id=123, source=\"api\")\r\ndef process_batch(data):\r\n    # Operation metadata is stored and included in reports\r\n    return processed_data\r\n```\r\n\r\n### Checkpoint Saving\r\n\r\n```python\r\n# Auto-save is enabled by default\r\ndebugger = PipelineDebugger(name=\"Pipeline\", auto_save=True)\r\n\r\n# Manual checkpoint\r\ndebugger.save_checkpoint()\r\n```\r\n\r\n## \ud83d\udcc8 Performance Tips\r\n\r\n1. **Use with Context** : The debugger adds minimal overhead, but for production pipelines, you can disable tracking:\r\n\r\n```python\r\n   debugger = PipelineDebugger(name=\"Pipeline\", track_memory=False, track_lineage=False)\r\n```\r\n\r\n2. **Batch Operations** : Group small operations together to reduce tracking overhead\r\n3. **Memory Monitoring** : Set appropriate memory thresholds to catch issues early:\r\n\r\n```python\r\n   debugger = PipelineDebugger(name=\"Pipeline\", memory_threshold_mb=500)\r\n```\r\n\r\n## \ud83d\udcbc **Enterprise Features**\r\n\r\n\u2705 **Professional Styling**: Modern design matching enterprise standards\r\n\u2705 **Executive Ready**: Suitable for stakeholder presentations\r\n\u2705 **Performance Insights**: AI-powered optimization recommendations\r\n\u2705 **Export Options**: High-resolution PNG outputs\r\n\u2705 **Responsive Design**: Scales from detailed debugging to executive overview\r\n\u2705 **Real-time Metrics**: Live performance and memory tracking\r\n\r\n## \ud83e\udd1d Contributing\r\n\r\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\r\n\r\n1. Fork the repository\r\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\r\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\r\n4. Push to the branch (`git push origin feature/AmazingFeature`)\r\n5. Open a Pull Request\r\n\r\n## \ud83d\udcc4 License\r\n\r\nThis project is licensed under the MIT License - see the [LICENSE](https://github.com/santhoshkrishnan30/dataprobe/blob/main/LICENSE) file for details.\r\n\r\n## \ud83d\ude4f Acknowledgments\r\n\r\n* Built with [Rich](https://github.com/Textualize/rich) for beautiful terminal output\r\n* Uses [NetworkX](https://networkx.org/) for pipeline visualization\r\n* Enhanced with [Matplotlib](https://matplotlib.org/) and [Seaborn](https://seaborn.pydata.org/) for enterprise-grade visualizations\r\n* Inspired by the need for better data pipeline debugging tools\r\n\r\n## \ud83d\udcde Support\r\n\r\n* \ud83d\udce7 Email: [santhoshkrishnan3006@gmail.com](mailto:santhoshkrishnan3006@gmail.com)\r\n* \ud83d\udc1b Issues: [GitHub Issues](https://github.com/santhoshkrishnan30/dataprobe/issues)\r\n* \ud83d\udcd6 Documentation: [Read the Docs](https://dataprobe.readthedocs.io/)\r\n\r\n## \ud83d\uddfa\ufe0f Roadmap\r\n\r\n* [X] Enterprise-grade dashboard visualizations\r\n* [X] 3D pipeline network views\r\n* [X] Executive-level reporting capabilities\r\n* [ ] Support for distributed pipeline debugging\r\n* [ ] Integration with popular orchestration tools (Airflow, Prefect, Dagster)\r\n* [ ] Real-time pipeline monitoring dashboard\r\n* [ ] Advanced anomaly detection in data flow\r\n* [ ] Support for streaming data pipelines\r\n\r\n---\r\n\r\nMade with \u2764\ufe0f by Santhosh Krishnan R\r\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "Keywords", "License-File", "Project-Url", "Provides-Extra", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/santhoshkrishnan30/dataprobe", "keywords": "data-pipeline, debugging, profiling, data-engineering, etl, data-lineage, memory-profiling, pandas, polars", "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "dataprobe", "package_url": "https://pypi.org/project/dataprobe/", "platform": null, "project_url": "https://pypi.org/project/dataprobe/", "project_urls": {"Bug Tracker": "https://github.com/santhoshkrishnan30/dataprobe/issues", "Documentation": "https://dataprobe.readthedocs.io", "Homepage": "https://github.com/santhoshkrishnan30/dataprobe", "Source Code": "https://github.com/santhoshkrishnan30/dataprobe"}, "provides_extra": ["dev", "all"], "release_url": "https://pypi.org/project/dataprobe/2.1.1/", "requires_dist": ["numpy>=1.21.0", "pandas>=1.3.0", "polars>=0.19.0", "scikit-learn>=1.0.0", "matplotlib>=3.5.0", "seaborn>=0.12.0", "rich>=13.0.0", "click>=8.0.0", "psutil>=5.9.0", "memory-profiler>=0.60.0", "graphviz>=0.20.0", "networkx>=2.8.0", "pytest>=7.0.0; extra == \"dev\"", "pytest-cov>=4.0.0; extra == \"dev\"", "black>=22.0.0; extra == \"dev\"", "flake8>=4.0.0; extra == \"dev\"", "mypy>=0.990; extra == \"dev\"", "sphinx>=4.0.0; extra == \"dev\"", "sphinx-rtd-theme>=1.0.0; extra == \"dev\"", "sqlalchemy>=2.0.0; extra == \"all\"", "pyarrow>=10.0.0; extra == \"all\"", "jupyter>=1.0.0; extra == \"all\"", "ipython>=8.0.0; extra == \"all\""], "requires_python": ">=3.8", "summary": "Advanced data pipeline debugging and profiling tools for Python", "version": "2.1.1", "yanked": true, "yanked_reason": "Documentation corrections available in v2.1.1"}, "last_serial": 30388572, "urls": [{"comment_text": null, "digests": {"blake2b_256": "66cda1a9bf069f29806697b84841b3146a2b639b633498a5032470ece6d7e64d", "md5": "dd2908c031496f962b1fc2f64b5e1b87", "sha256": "9893de27e7fd11dde0986bbb68cd7875dbf630613361a906643bd6e259319d0c"}, "downloads": -1, "filename": "dataprobe-2.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "dd2908c031496f962b1fc2f64b5e1b87", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 24029, "upload_time": "2025-07-28T18:16:25", "upload_time_iso_8601": "2025-07-28T18:16:25.459502Z", "url": "https://files.pythonhosted.org/packages/66/cd/a1a9bf069f29806697b84841b3146a2b639b633498a5032470ece6d7e64d/dataprobe-2.1.1-py3-none-any.whl", "yanked": true, "yanked_reason": "Documentation corrections available in v2.1.1"}, {"comment_text": null, "digests": {"blake2b_256": "95e69c8dd5ab2164344becf0fdc653ca58d9f21d6bfc22c0965c047e2a2d5f5d", "md5": "4d2c5097deb229b417c4ef0938457ce9", "sha256": "7cb19eb91724ebba7547d74921159c82ead55b05e10637f37ec5f8c432bc1b9c"}, "downloads": -1, "filename": "dataprobe-2.1.1.tar.gz", "has_sig": false, "md5_digest": "4d2c5097deb229b417c4ef0938457ce9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 29298, "upload_time": "2025-07-28T18:16:26", "upload_time_iso_8601": "2025-07-28T18:16:26.866187Z", "url": "https://files.pythonhosted.org/packages/95/e6/9c8dd5ab2164344becf0fdc653ca58d9f21d6bfc22c0965c047e2a2d5f5d/dataprobe-2.1.1.tar.gz", "yanked": true, "yanked_reason": "Documentation corrections available in v2.1.1"}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:16:12 GMT", "package": "qbosdk", "version": "1.4.0", "json": {"info": {"author": "Shwetabh Kumar", "author_email": "shwetabh.kumar@fyle.in", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Internet :: WWW/HTTP"], "description": "# QuickbooksOnlineSDK\n\nPython SDK for accessing QBO APIs.\n\n## Installation\n\nThis project requires [Python 3+](https://www.python.org/downloads/) and [Requests](https://pypi.org/project/requests/) library (pip install requests).\n\n1. Download this project and use it (copy it in your project, etc).\n2. Install it from [pip](https://pypi.org).\n        \n        $ pip install qbosdk\n\n## Usage\n\nTo use this SDK you'll need these QBO credentials used for OAuth2 authentication: **client ID**, **client secret** and **refresh token**.\n\nThis SDK is very easy to use.\n1. First you'll need to create a connection using the main class QuickbooksOnlineSDK.\n```python\nfrom qbosdk import QuickbooksOnlineSDK\n\nconnection = QuickbooksOnlineSDK(\n    client_id='<YOUR CLIENT ID>',\n    client_secret='<YOUR CLIENT SECRET>',\n    refresh_token='<YOUR REFRESH TOKEN>',\n    realm_id='<REALM / COMPANY ID>',\n    environment='<sandbox / production>'\n)\n```\n2. After that you'll be able to access any of the API classes\n```python\n\"\"\"\nUSAGE: <QuickbooksOnlineSDK INSTANCE>.<API_NAME>.<API_METHOD>(<PARAMETERS>)\n\"\"\"\n\n# Get a list of all Employees (with all available details for Employee)\nresponse = connection.employees.get()\n\n# Get a list of all Accounts\nresponse = connection.accounts.get()\n```\n\nSee more details about the usage into the wiki pages of this project.\n\n## Integration Tests\n\nTo run integration tests, you will need a mechanism to connect to a real qbo account. Save this info in a test_credentials.json file in your root directory:\n\n```json\n{\n  \"client_id\": \"<client_id>\",\n  \"client_secret\": \"<client_secret>\",\n  \"realm_id\": \"<realm_id>\",\n  \"refresh_token\": \"<refresh_token>\",\n  \"environment\": \"<environment sandbox / production>\"\n}\n```\n\n```bash\n$ pip install pytest\n\n$ python -m pytest test/integration\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "Keywords", "License", "License-File", "Requires-Dist", "Summary"], "home_page": "https://github.com/fylein/qbo-sdk-py", "keywords": "quickbooks-online, quickbooks, fyle, api, python, sdk", "license": "MIT", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "qbosdk", "package_url": "https://pypi.org/project/qbosdk/", "platform": null, "project_url": "https://pypi.org/project/qbosdk/", "project_urls": {"Homepage": "https://github.com/fylein/qbo-sdk-py"}, "provides_extra": null, "release_url": "https://pypi.org/project/qbosdk/1.4.0/", "requires_dist": ["requests>=2.25.0", "future==0.18.2"], "requires_python": null, "summary": "Python SDK for accessing Quickbooks Online APIs", "version": "1.4.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388528, "urls": [{"comment_text": null, "digests": {"blake2b_256": "ad6c9b56e5dc6049f474f3d2980ac00d0e7f04f7ff798690c2963952b7a80883", "md5": "3cd1db4a92f7e7c6028b2b92d904d3b7", "sha256": "3c50ea05dce5a57b5576583c54cf33329a168e26543c5f0cc65f46527ab9268f"}, "downloads": -1, "filename": "qbosdk-1.4.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3cd1db4a92f7e7c6028b2b92d904d3b7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23866, "upload_time": "2025-07-28T18:16:12", "upload_time_iso_8601": "2025-07-28T18:16:12.072002Z", "url": "https://files.pythonhosted.org/packages/ad/6c/9b56e5dc6049f474f3d2980ac00d0e7f04f7ff798690c2963952b7a80883/qbosdk-1.4.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "412f59339d7231ef58298c83a48d0aa025ea6ce38fe31c00a16fa73295138826", "md5": "cf12dfd0d96a6d0451b0745d3d340926", "sha256": "2431946aa81b535e0f2b7fcd3be7735b1f3b109160ec1d79e9d5c4e686328a6d"}, "downloads": -1, "filename": "qbosdk-1.4.0.tar.gz", "has_sig": false, "md5_digest": "cf12dfd0d96a6d0451b0745d3d340926", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14939, "upload_time": "2025-07-28T18:16:13", "upload_time_iso_8601": "2025-07-28T18:16:13.099241Z", "url": "https://files.pythonhosted.org/packages/41/2f/59339d7231ef58298c83a48d0aa025ea6ce38fe31c00a16fa73295138826/qbosdk-1.4.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:15:53 GMT", "package": "supersafecalc", "version": "3.0.1", "json": {"info": {"author": null, "author_email": null, "bugtrack_url": null, "classifiers": [], "description": "", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Summary"], "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "supersafecalc", "package_url": "https://pypi.org/project/supersafecalc/", "platform": null, "project_url": "https://pypi.org/project/supersafecalc/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/supersafecalc/3.0.1/", "requires_dist": null, "requires_python": null, "summary": "Malicious supersafecalc package", "version": "3.0.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388525, "urls": [{"comment_text": null, "digests": {"blake2b_256": "5ebb92bc60793423be31f646a0f4501bce4a22d21dea0cb3c7bcf564a604f99c", "md5": "f748bb3804af072c918126d657db89b0", "sha256": "7c1a6f7892830447db591af8350489a6b5fa78ebc37804846edd985e59b56c51"}, "downloads": -1, "filename": "supersafecalc-3.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "f748bb3804af072c918126d657db89b0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1223, "upload_time": "2025-07-28T18:15:53", "upload_time_iso_8601": "2025-07-28T18:15:53.924228Z", "url": "https://files.pythonhosted.org/packages/5e/bb/92bc60793423be31f646a0f4501bce4a22d21dea0cb3c7bcf564a604f99c/supersafecalc-3.0.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "66eb3ce0f2ab6fbec1649d290fa16b7d9926214b36c814d464518a70fc943dc4", "md5": "6624a6efb43a63b14ef835297c23e1db", "sha256": "f88d365004db338cffe851ba111eedc2ceb35e76991542887e8a3207a6ddf429"}, "downloads": -1, "filename": "supersafecalc-3.0.1.tar.gz", "has_sig": false, "md5_digest": "6624a6efb43a63b14ef835297c23e1db", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 977, "upload_time": "2025-07-28T18:15:55", "upload_time_iso_8601": "2025-07-28T18:15:55.267895Z", "url": "https://files.pythonhosted.org/packages/66/eb/3ce0f2ab6fbec1649d290fa16b7d9926214b36c814d464518a70fc943dc4/supersafecalc-3.0.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:15:52 GMT", "package": "connector-py", "version": "4.101.0", "json": {"info": {"author": null, "author_email": "teamlumos <security@lumos.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Programming Language :: Python", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy"], "description": "# Lumos Connector SDK\n\nPlug apps back into Lumos using an integration connector built with this SDK.\n\n[![PyPI - Version](https://img.shields.io/pypi/v/connector-py.svg)](https://pypi.org/project/connector-py)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/connector-py.svg)](https://pypi.org/project/connector-py)\n\n-----\n\n## Table of Contents\n\n- [Lumos Connector SDK](#lumos-connector-sdk)\n  - [Table of Contents](#table-of-contents)\n  - [Installation](#installation)\n  - [Usage](#usage)\n    - [Print the spec](#print-the-spec)\n  - [Create a new connector](#create-a-new-connector)\n    - [Learning the connector's capabilities](#learning-the-connectors-capabilities)\n    - [Connector implementation](#connector-implementation)\n    - [Running unit tests](#running-unit-tests)\n    - [Typechecking with MyPy](#typechecking-with-mypy)\n    - [Error Handling](#error-handling)\n      - [Raising an exception](#raising-an-exception)\n      - [Response](#response)\n    - [OAuth Module](#oauth-module)\n      - [OAuth Flow Types](#oauth-flow-types)\n  - [Connector Configuration](#connector-configuration)\n    - [Where should I set my connector's configuration?](#where-should-i-set-my-connectors-configuration)\n    - [The connection sequence for Lumos](#the-connection-sequence-for-lumos)\n  - [Deploying a connector](#deploying-a-connector)\n    - [Deployment models](#deployment-models)\n  - [Tips](#tips)\n    - [The library I want to use is synchronous only](#the-library-i-want-to-use-is-synchronous-only)\n  - [License](#license)\n\n## Installation\n\n```console\npip install \"connector-py[dev]\"\n```\n\n## Usage\n\nThis package has...\n\n1. A CLI to create a custom connector with its own CLI to call commands\n2. A library to assist building custom connectors in Python\n\nTo get started with the CLI, run `connector --help`\n\n### Print the spec\n\nThis SDK has an OpenAPI spec that you can render and view with the [Swagger editor](https://editor.swagger.io/).\n\n```console\nconnector spec\n```\n\n## Create a new connector\n\nFrom your shell, run\n\n```shell\n# Create a connector\n# CLI     cmd      name           folder\nconnector scaffold demo-connector demo_connector\n\n# Install its dependencies in a virtual env\ncd demo_connector\npython -m venv .venv\n. .venv/bin/activate\npip install \".[all]\"\n\n# Lint and run tests\nmypy .\npytest\n\n# Run the info capability (note the hyphens, instead of underscores)\ndemo-connector info\n```\n\n### Learning the connector's capabilities\n\nCustom and on-premise Lumos connectors are called via the CLI; they're passed JSON and should print the response JSON to stdout.\n\nRun the `info` capability to learn what other capabilities the connector supports, what resource and entitlement types, its name, etc.\n\nLook at the info, using `jq` to pretty-print the response:\n\n```shell\ndemo-connector info | jq .response\n# or just the capabilities\ndemo-connector info | jq .response.capabilities\n```\n\nTo call most capabilities, you run a command where you pass the request (JSON) as a string.\n\n```console\n<CONNECTOR COMMAND> <CAPABILITY NAME> --json '<A STRINGIFIED JSON OBJECT>'\n```\n\nThe most important capability to implement is `validate_credentials`. Lumos uses this capability to ensure a user-established connection works, and has resulted in authentication credentials your connector can use to perform other actions.\n\n```py\ntest-connector validate_credentials --json '{\n    \"auth\": {\n        \"oauth\": {\n            \"access_token\":\"this will not work\"\n        }\n    },\n    \"request\": {},\n    \"settings\": {\n        \"account_id\":\"foo\"\n    }\n}'\n```\n\n**This is expected to \ud83d\udca5 fail with a brand-new connector**. You'll need to figure out how to [configure the authentication](#connector-configuration) to the underlying app server, and how to surface that as user (auth) configuration.\n\nTo learn more about all the capabilities, check out the OpenAPI spec in a Swagger editor.\n\nTo see a working capability, you can use the Lumos mock connector's `validate_credentials` call, using `jq` to pretty print the JSON:\n\n```console\nmock-connector validate_credentials --json '{\"auth\":{\"basic\":{\"username\":\"foo\",\"password\":\"bar\"}},\"request\":{},\"settings\":{\"host\":\"google.com\"}}' | jq .\n```\n\n```json\n{\n  \"response\": {\n    \"valid\": true,\n    \"unique_tenant_id\": \"mock-connector-tenant-id\"\n  },\n  \"page\": null\n}\n```\n\n### Connector implementation\n\nConnectors can implement whichever Lumos capabilities make sense for the underlying app.\n\nTo see what a minimal implementation looks like, you can inspect a newly scaffolded connector, and look at the integration declaration, and the _uncommented out_ capability registrations.\n\nThe integration declaration looks something like this:\n\n```python\nintegration = Integration(\n    app_id=\"my_app\",\n    version=__version__,\n    auth=BasicCredential,\n    settings_model=MyAppSettings,\n    exception_handlers=[\n        (httpx.HTTPStatusError, HTTPHandler, None),\n    ],\n    description_data=DescriptionData(\n        logo_url=\"https://logo.clearbit.com/foobar.com\",\n        user_friendly_name=\"Foo Bar\",\n        description=\"Foobar is a cloud-based platform that lets you manage foos and bars\",\n        categories=[AppCategory.DEVELOPERS, AppCategory.COLLABORATION],\n    ),\n    resource_types=resource_types,\n    entitlement_types=entitlement_types,\n)\n```\n\nAnd capability registration looks something like this:\n\n```py\n@integration.register_capability(StandardCapabilityName.LIST_ACCOUNTS)\nasync def list_accounts(request: ListAccountsRequest) -> ListAccountsResponse:\n    # do whatever is needed to get accounts, probably make an http call\n    return ListAccountsResponse(\n        response=[],\n        ...\n    )\n```\n\n### Running unit tests\n\nScaffolded connectors come with a bunch of unit test examples - they're all skipped by default, but you can remove the skip marker to use the existing test.\n\nTo run unit tests:\n\n```console\npytest .\n```\n\nTo understand the test structure:\n\n```text\ndemo_connector/\n    demo_connector/\n    tests/\n        test_basic_capabilities/\n            test_list_accounts_cases.py\n            test_list_accounts.py\n            ...\n```\n\n- `test_list_accounts.py` is the actual Pytest test code. It uses all the existing test cases from\n- `test_list_accounts_cases.py`\n\nYou can see the reference here\n\n```py\n@pytest_cases.parametrize_with_cases(\n    [\"args\", \"response_body_map\", \"expected_response\"],\n    cases=[\n        # The name of the Python module\n        \"tests.test_basic_capabilities.test_list_accounts_cases\",\n    ],\n)\n```\n\n### Typechecking with MyPy\n\nThe generated Python code is typed, and can be typechecked with MyPy (installed as a dev dependency).\n\n```console\nmypy .\n```\n\n### Error Handling\n\nError handling is facilitated through an exception handler decorator.\n\nAn exception handler can be attached to the connector library as follows:\n\n```python\nfrom httpx import HTTPStatusError\nfrom connector.oai.errors import HTTPHandler\n\nintegration = Integration(\n    ...,\n    exception_handlers=[\n        (HTTPStatusError, HTTPHandler, None),\n    ],\n    handle_errors=True,\n)\n```\n\nThe decorator accepts a list of tuples of three.\n\n1. the exception type you would like to catch\n2. the handler (default or implemented on your own)\n3. a specific error code that you would like to associate with this handler.\n\nBy default it is recommended to make use of the default HTTPHandler which will handle `raise_for_status()` for you and properly error code it. For more complex errors it is recommended to subclass the ExceptionHandler (in `connector/oai/errors.py`) and craft your own handler.\n\n#### Raising an exception\n\nAmong this, there is a custom exception class available as well as a default list of error codes:\n\n```python\nfrom connector.oai.errors import ConnectorError\nfrom connector_sdk_types.generated import ErrorCode\n\ndef some_method(self, args):\n    raise ConnectorError(\n        message=\"Received wrong data, x: y\",\n        app_error_code=\"foobar.some_unique_string\",\n        error_code=ErrorCode.BAD_REQUEST,\n    )\n```\n\nIt is preferred to raise any manually raisable exception with this class. A connector can implement its own error codes list, which should be properly documented.\n\n#### Response\n\nAn example response when handled this way:\n\n```json\n// BAD_REQUEST error from github connector\n{\"error\":{\"message\":\"Some message\",\"status_code\":400,\"error_code\":\"bad_request\",\"raised_by\":\"HTTPStatusError\",\"raised_in\":\"github.integration:validate_credentials\"}, \"response\": null, \"raw_data\": null}\n```\n\n### OAuth Module\n\nThe OAuth module is responsible for handling the OAuth2.0 flow for a connector.\nIt is configured with `oauth_settings` in the `Integration` class.\nNot configuring this object will disable the OAuth module completely.\n\n```python\nfrom connector.oai.modules.oauth_module_types import (\n    OAuthSettings,\n    OAuthCapabilities,\n    OAuthRequest,\n    RequestDataType,\n)\n\nintegration = Integration(\n    ...,\n    oauth_settings=OAuthSettings(\n        # Authorization & Token URLs for the particular connector\n        authorization_url=\"https://app.connector.com/oauth/authorize\",\n        token_url=\"https://api.connector.com/oauth/v1/token\",\n\n        # Scopes per capability (space delimited string)\n        scopes={\n            StandardCapabilityName.VALIDATE_CREDENTIALS: \"test:scope another:scope\",\n            ... # further capabilities as implemented in the connector\n        },\n\n        # You can modify the request type if the default is not appropriate\n        # common options for method are \"POST\" and \"GET\"\n        # available options for data are \"FORMDATA\", \"QUERY\", and \"JSON\" (form-data / url query params / json body)\n        # *default is POST and FORMDATA*\n        request_type=OAuthRequest(data=RequestDataType.FORMDATA),\n\n        # You can modify the authentication method if the default is not appropriate\n        # available options for auth_method are \"CLIENT_SECRET_POST\" and \"CLIENT_SECRET_BASIC\"\n        # *default is CLIENT_SECRET_POST*\n        client_auth=ClientAuthenticationMethod.CLIENT_SECRET_POST,\n\n        # You can turn off specific or all capabilities for the OAuth module\n        # This means that these will either be skipped or you have to implement them manually\n        capabilities=OAuthCapabilities(\n            refresh_access_token=False,\n        ),\n\n        #\u00a0You can specify the type of OAuth flow to use\n        #\u00a0Available options are \"CODE_FLOW\" and \"CLIENT_CREDENTIALS\"\n        #\u00a0*default is CODE_FLOW*\n        flow_type=OAuthFlowType.CODE_FLOW,\n\n        # You can enable PKCE (Proof Key for Code Exchange)\n        #\u00a0*default is False*\n        # S256 is the default hashing algorithm, and the only supported at the moment\n        pkce=True,\n    ),\n)\n```\n\nIt might happen that your integration requires a dynamic authorization/token URL.\nFor example when the service provider has specific URLs and uses the customers custom subdomain. (eg. `https://{subdomain}.service.com/oauth/authorize`)\nIn that case you can pass a callable that takes the request args (`AuthRequest`, without the auth parameter) as an argument (only available during request).\n\n```python\n# method definitions\ndef get_authorization_url(args: AuthRequest) -> str:\n    settings = get_settings(args, ConnectorSettings)\n    return f\"https://{settings.subdomain}.service.com/oauth/authorize\"\n\ndef get_token_url(args: AuthRequest) -> str:\n    settings = get_settings(args, ConnectorSettings)\n    return f\"https://{settings.subdomain}.service.com/oauth/token\"\n\n# oauth settings\nintegration = Integration(\n    ...,\n    oauth_settings=OAuthSettings(\n        authorization_url=get_authorization_url,\n        token_url=get_token_url,\n    ),\n)\n```\n\n#### OAuth Flow Types\n\nThe OAuth module supports two flow types:\n\n- `CODE_FLOW`: The authorization code flow (default)\n- `CLIENT_CREDENTIALS`: The client credentials flow (sometimes called \"2-legged OAuth\" or \"Machine-to-Machine OAuth\")\n\nThe flow type can be specified in the `OAuthSettings` object.\n\nUsing the authorization code flow you have three available capabilities:\n\n- `GET_AUTHORIZATION_URL`: To get the authorization URL\n- `HANDLE_AUTHORIZATION_CALLBACK`: To handle the authorization callback\n- `REFRESH_ACCESS_TOKEN`: To refresh the access token\n\nUsing the client credentials flow you have two available capabilities:\n\n- `HANDLE_CLIENT_CREDENTIALS_REQUEST`: To handle the client credentials request, uses the token URL\n- `REFRESH_ACCESS_TOKEN`: To refresh the access token\n\nThese are registered by default via the module and can be overriden by the connector.\n\nIf you run:\n\n```sh\nconnector info\n```\n\nYou will see that the OAuth capabilities are included in the available connector capabilities.\n\n## Connector Configuration\n\nA connector is used to connect to multiple tenants of the same app. Each tenant has a connection in Lumos, and the unique tenant ID is used to distinguish the different connections.\n\nEach connection has its own...\n\n- ...auth object that fits the connector's auth model.\n- ...settings object that fits the connector's settings model.\n- ...set of data (accounts, resources, entitlements) that Lumos reads and stores.\n\n![How a connector is used](https://lumos-static.s3.us-west-2.amazonaws.com/prod/public/sdk-documentation/connector-tenant-data-model.png)\n\nA connector can be used for multiple underlying instances of the same app. For instance, you might use a `github` connector to establish connections with different Github Organizations. The nature of \"what is a tenant\" is dependent on the underlying app.\n\nA scaffolded connector has OAuth authentication, and a Settings type with `account_id`. You don't have to keep these - you can change the authentication model and the Settings type to whatever is appropriate for the underlying app (settings may be empty).\n\n### Where should I set my connector's configuration?\n\nA quick rule is sensitive data that would allow an attacker the ability to access the underlying app, goes into the `auth` payload. Anything else that's not sensitive, not absolutely required to connect to a tenant, or can have a sane default, is `settings`.\n\n### The connection sequence for Lumos\n\n1. Lumos sees a new connector, and queries its settings and auth models via the `info` command.\n2. Lumos uses these parts of the `info` response to render a connection form for the user.\n    - the settings (JSON schema + included documentation)\n    - auth models (string matching to the auth model)\n    - app logo, description, tags, etc.\n3. The user enters all the relevant data/auth materials to connect to an app, and/or does an OAuth consent flow to the underlying app.\n4. Lumos validates the credentials and settings via the `validate_credentials` capability.\n\n![The process of connecting a new connector](https://lumos-static.s3.us-west-2.amazonaws.com/prod/public/sdk-documentation/connector-setup.png)\n\nAt this point, the connection is considered established, and Lumos will attempt to read all data from the connector, allow user provisioning and deprovisioning, etc.\n\n## Deploying a connector\n\nQuick steps:\n\n1. Package up the connector you've built into an archive with a native executable. We use [`pyinstaller`](https://pyinstaller.org/en/stable/) for our Python connectors.\n\n    ```shell\n    # SDK     command         ...required args\n    connector compile-on-prem --connector-root-module-dir ./demo_connector/demo_connector --app-id demo\n    ```\n\n2. Run the [Lumos on-premise agent](https://developers.lumos.com/reference/on-premise-agent).\n3. On the same machine as (3), deploy the packaged-up connector from (1) in the same folder.\n4. The integration should show up in the Lumos AdminView > Integrations screen.\n\n### Deployment models\n\nLumos calls a connector's APIs with auth and settings data to read all the accounts, entitlements, resources, and associations in the connected app.\n\nThere are two ways this happens, depending on who's hosting the connector.\n\nIf Lumos is hosting it, we call it directly in our backend.\n\n![Lumos hosts and calls our own connectors](https://lumos-static.s3.us-west-2.amazonaws.com/prod/public/sdk-documentation/connector-hosting-lumos.png)\n\nIf it's a custom connector, it runs as an on-premise connector on a customer's computer, and is called by the [Lumos on-prem agent](https://developers.lumos.com/reference/on-premise-agent).\n\n![Lumos hosts and calls our own connectors](https://lumos-static.s3.us-west-2.amazonaws.com/prod/public/sdk-documentation/connector-hosting-onprem.png)\n\n## Tips\n\n### The library I want to use is synchronous only\n\nYou can use a package called `asgiref`. This package converts I/O bound synchronous\ncalls into asyncio non-blocking calls. First, add asgiref to your dependencies list\nin `pyproject.toml`. Then, in your async code, use `asgiref.sync_to_async` to convert\nsynchronous calls to asynchronous calls.\n\n```python\nfrom asgiref.sync import sync_to_async\nimport requests\n\nasync def async_get_data():\n    response = await sync_to_async(requests.get)(\"url\")\n```\n\n## License\n\n`connector` is distributed under the terms of the [Apache 2.0](./LICENSE.txt) license.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": "integrations", "license": "\n                                         Apache License\n                                   Version 2.0, January 2004\n                                http://www.apache.org/licenses/\n        \n           TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n        \n           1. Definitions.\n        \n              \"License\" shall mean the terms and conditions for use, reproduction,\n              and distribution as defined by Sections 1 through 9 of this document.\n        \n              \"Licensor\" shall mean the copyright owner or entity authorized by\n              the copyright owner that is granting the License.\n        \n              \"Legal Entity\" shall mean the union of the acting entity and all\n              other entities that control, are controlled by, or are under common\n              control with that entity. For the purposes of this definition,\n              \"control\" means (i) the power, direct or indirect, to cause the\n              direction or management of such entity, whether by contract or\n              otherwise, or (ii) ownership of fifty percent (50%) or more of the\n              outstanding shares, or (iii) beneficial ownership of such entity.\n        \n              \"You\" (or \"Your\") shall mean an individual or Legal Entity\n              exercising permissions granted by this License.\n        \n              \"Source\" form shall mean the preferred form for making modifications,\n              including but not limited to software source code, documentation\n              source, and configuration files.\n        \n              \"Object\" form shall mean any form resulting from mechanical\n              transformation or translation of a Source form, including but\n              not limited to compiled object code, generated documentation,\n              and conversions to other media types.\n        \n              \"Work\" shall mean the work of authorship, whether in Source or\n              Object form, made available under the License, as indicated by a\n              copyright notice that is included in or attached to the work\n              (an example is provided in the Appendix below).\n        \n              \"Derivative Works\" shall mean any work, whether in Source or Object\n              form, that is based on (or derived from) the Work and for which the\n              editorial revisions, annotations, elaborations, or other modifications\n              represent, as a whole, an original work of authorship. For the purposes\n              of this License, Derivative Works shall not include works that remain\n              separable from, or merely link (or bind by name) to the interfaces of,\n              the Work and Derivative Works thereof.\n        \n              \"Contribution\" shall mean any work of authorship, including\n              the original version of the Work and any modifications or additions\n              to that Work or Derivative Works thereof, that is intentionally\n              submitted to Licensor for inclusion in the Work by the copyright owner\n              or by an individual or Legal Entity authorized to submit on behalf of\n              the copyright owner. For the purposes of this definition, \"submitted\"\n              means any form of electronic, verbal, or written communication sent\n              to the Licensor or its representatives, including but not limited to\n              communication on electronic mailing lists, source code control systems,\n              and issue tracking systems that are managed by, or on behalf of, the\n              Licensor for the purpose of discussing and improving the Work, but\n              excluding communication that is conspicuously marked or otherwise\n              designated in writing by the copyright owner as \"Not a Contribution.\"\n        \n              \"Contributor\" shall mean Licensor and any individual or Legal Entity\n              on behalf of whom a Contribution has been received by Licensor and\n              subsequently incorporated within the Work.\n        \n           2. Grant of Copyright License. Subject to the terms and conditions of\n              this License, each Contributor hereby grants to You a perpetual,\n              worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n              copyright license to reproduce, prepare Derivative Works of,\n              publicly display, publicly perform, sublicense, and distribute the\n              Work and such Derivative Works in Source or Object form.\n        \n           3. Grant of Patent License. Subject to the terms and conditions of\n              this License, each Contributor hereby grants to You a perpetual,\n              worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n              (except as stated in this section) patent license to make, have made,\n              use, offer to sell, sell, import, and otherwise transfer the Work,\n              where such license applies only to those patent claims licensable\n              by such Contributor that are necessarily infringed by their\n              Contribution(s) alone or by combination of their Contribution(s)\n              with the Work to which such Contribution(s) was submitted. If You\n              institute patent litigation against any entity (including a\n              cross-claim or counterclaim in a lawsuit) alleging that the Work\n              or a Contribution incorporated within the Work constitutes direct\n              or contributory patent infringement, then any patent licenses\n              granted to You under this License for that Work shall terminate\n              as of the date such litigation is filed.\n        \n           4. Redistribution. You may reproduce and distribute copies of the\n              Work or Derivative Works thereof in any medium, with or without\n              modifications, and in Source or Object form, provided that You\n              meet the following conditions:\n        \n              (a) You must give any other recipients of the Work or\n                  Derivative Works a copy of this License; and\n        \n              (b) You must cause any modified files to carry prominent notices\n                  stating that You changed the files; and\n        \n              (c) You must retain, in the Source form of any Derivative Works\n                  that You distribute, all copyright, patent, trademark, and\n                  attribution notices from the Source form of the Work,\n                  excluding those notices that do not pertain to any part of\n                  the Derivative Works; and\n        \n              (d) If the Work includes a \"NOTICE\" text file as part of its\n                  distribution, then any Derivative Works that You distribute must\n                  include a readable copy of the attribution notices contained\n                  within such NOTICE file, excluding those notices that do not\n                  pertain to any part of the Derivative Works, in at least one\n                  of the following places: within a NOTICE text file distributed\n                  as part of the Derivative Works; within the Source form or\n                  documentation, if provided along with the Derivative Works; or,\n                  within a display generated by the Derivative Works, if and\n                  wherever such third-party notices normally appear. The contents\n                  of the NOTICE file are for informational purposes only and\n                  do not modify the License. You may add Your own attribution\n                  notices within Derivative Works that You distribute, alongside\n                  or as an addendum to the NOTICE text from the Work, provided\n                  that such additional attribution notices cannot be construed\n                  as modifying the License.\n        \n              You may add Your own copyright statement to Your modifications and\n              may provide additional or different license terms and conditions\n              for use, reproduction, or distribution of Your modifications, or\n              for any such Derivative Works as a whole, provided Your use,\n              reproduction, and distribution of the Work otherwise complies with\n              the conditions stated in this License.\n        \n           5. Submission of Contributions. Unless You explicitly state otherwise,\n              any Contribution intentionally submitted for inclusion in the Work\n              by You to the Licensor shall be under the terms and conditions of\n              this License, without any additional terms or conditions.\n              Notwithstanding the above, nothing herein shall supersede or modify\n              the terms of any separate license agreement you may have executed\n              with Licensor regarding such Contributions.\n        \n           6. Trademarks. This License does not grant permission to use the trade\n              names, trademarks, service marks, or product names of the Licensor,\n              except as required for reasonable and customary use in describing the\n              origin of the Work and reproducing the content of the NOTICE file.\n        \n           7. Disclaimer of Warranty. Unless required by applicable law or\n              agreed to in writing, Licensor provides the Work (and each\n              Contributor provides its Contributions) on an \"AS IS\" BASIS,\n              WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n              implied, including, without limitation, any warranties or conditions\n              of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n              PARTICULAR PURPOSE. You are solely responsible for determining the\n              appropriateness of using or redistributing the Work and assume any\n              risks associated with Your exercise of permissions under this License.\n        \n           8. Limitation of Liability. In no event and under no legal theory,\n              whether in tort (including negligence), contract, or otherwise,\n              unless required by applicable law (such as deliberate and grossly\n              negligent acts) or agreed to in writing, shall any Contributor be\n              liable to You for damages, including any direct, indirect, special,\n              incidental, or consequential damages of any character arising as a\n              result of this License or out of the use or inability to use the\n              Work (including but not limited to damages for loss of goodwill,\n              work stoppage, computer failure or malfunction, or any and all\n              other commercial damages or losses), even if such Contributor\n              has been advised of the possibility of such damages.\n        \n           9. Accepting Warranty or Additional Liability. While redistributing\n              the Work or Derivative Works thereof, You may choose to offer,\n              and charge a fee for, acceptance of support, warranty, indemnity,\n              or other liability obligations and/or rights consistent with this\n              License. However, in accepting such obligations, You may act only\n              on Your own behalf and on Your sole responsibility, not on behalf\n              of any other Contributor, and only if You agree to indemnify,\n              defend, and hold each Contributor harmless for any liability\n              incurred by, or claims asserted against, such Contributor by reason\n              of your accepting any such warranty or additional liability.\n        \n           END OF TERMS AND CONDITIONS\n        \n           APPENDIX: How to apply the Apache License to your work.\n        \n              To apply the Apache License to your work, attach the following\n              boilerplate notice, with the fields enclosed by brackets \"[]\"\n              replaced with your own identifying information. (Don't include\n              the brackets!)  The text should be enclosed in the appropriate\n              comment syntax for the file format. We also recommend that a\n              file or class name and description of purpose be included on the\n              same \"printed page\" as the copyright notice for easier\n              identification within third-party archives.\n        \n           Copyright 2024 Lumos App, Inc.\n        \n           Licensed under the Apache License, Version 2.0 (the \"License\");\n           you may not use this file except in compliance with the License.\n           You may obtain a copy of the License at\n        \n               http://www.apache.org/licenses/LICENSE-2.0\n        \n           Unless required by applicable law or agreed to in writing, software\n           distributed under the License is distributed on an \"AS IS\" BASIS,\n           WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n           See the License for the specific language governing permissions and\n           limitations under the License.\n        ", "license_expression": null, "license_files": ["LICENSE.txt"], "maintainer": null, "maintainer_email": null, "name": "connector-py", "package_url": "https://pypi.org/project/connector-py/", "platform": null, "project_url": "https://pypi.org/project/connector-py/", "project_urls": {"Documentation": "https://developers.lumos.com/reference/the-lumos-connector-api"}, "provides_extra": ["fastapi", "dev", "all"], "release_url": "https://pypi.org/project/connector-py/4.101.0/", "requires_dist": ["connector-sdk-types", "gql[httpx]", "httpx>=0.27.0", "msgpack>=1", "pydantic<2.10,>=2", "urllib3>=1.25.2", "botocore>=1.34.0", "PyJWT==2.10.1", "pip-system-certs==5.2; platform_system == \"Windows\"", "fastapi-slim; extra == \"fastapi\"", "uvicorn; extra == \"fastapi\"", "pytest>=8; extra == \"dev\"", "pytest-asyncio>=0.23; extra == \"dev\"", "pytest-httpx>=0.33; extra == \"dev\"", "Flask; extra == \"dev\"", "pyinstaller>=6.8.0; extra == \"dev\"", "connector-py[dev,fastapi]; extra == \"all\""], "requires_python": ">=3.10", "summary": "An Abstract Tool to Perform Actions on Integrations", "version": "4.101.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388524, "urls": [{"comment_text": null, "digests": {"blake2b_256": "8b2ac77e38df5e0b708eee4bdd80f2f4696d7e694fe63729d2dff5c9d9b6f5a0", "md5": "c2639fb5339f23226ced0573dfab803d", "sha256": "c797a6c518566bfa85e375261462f21c0cb6d7a32113ad2fedd80300ded5b4cd"}, "downloads": -1, "filename": "connector_py-4.101.0-py3-none-any.whl", "has_sig": false, "md5_digest": "c2639fb5339f23226ced0573dfab803d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.10", "size": 261866, "upload_time": "2025-07-28T18:15:52", "upload_time_iso_8601": "2025-07-28T18:15:52.110207Z", "url": "https://files.pythonhosted.org/packages/8b/2a/c77e38df5e0b708eee4bdd80f2f4696d7e694fe63729d2dff5c9d9b6f5a0/connector_py-4.101.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "9ad1d2f096d1e53406db283351c50d23d2261d28f4ce4ac01a34eea2a2a71274", "md5": "eab9a092b2e24d0b3a06c2cfac8e4fd7", "sha256": "a066d1c09d745bf59b12c69a553a7b5418df38dddb45c138336d20a236a74dd7"}, "downloads": -1, "filename": "connector_py-4.101.0.tar.gz", "has_sig": false, "md5_digest": "eab9a092b2e24d0b3a06c2cfac8e4fd7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.10", "size": 153779, "upload_time": "2025-07-28T18:15:53", "upload_time_iso_8601": "2025-07-28T18:15:53.493688Z", "url": "https://files.pythonhosted.org/packages/9a/d1/d2f096d1e53406db283351c50d23d2261d28f4ce4ac01a34eea2a2a71274/connector_py-4.101.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:15:28 GMT", "package": "cell-eval", "version": "0.5.42", "json": {"info": {"author": null, "author_email": "Noam Teyssier <noam.teyssier@arcinstitute.org>, Abhinav Adduri <abhinav.adduri@arcinstitute.org>, Yusuf Roohani <yusuf.roohani@arcinstitute.org>", "bugtrack_url": null, "classifiers": [], "description": "# cell-eval\n\n## Description\n\nThis package provides a comprehensive suite of metrics for evaluating the performance of models that predict cellular responses to perturbations at the single-cell level. It can be used either as a command-line tool or as a Python module.\n\n## Installation\n\nDistribution with [`uv`](https://docs.astral.sh/uv/)\n\n```bash\n# install from pypi\nuv pip install -U cell-eval\n\n# install from github directly\nuv pip install -U git+ssh://github.com/arcinstitute/cell-eval\n\n# install cli with uv tool\nuv tool install -U git+ssh://github.com/arcinstitute/cell-eval\n\n# Check installation\ncell-eval --help\n```\n\n## Usage\n\nTo get started you'll need to have two anndata files.\n\n1. a predicted anndata (`adata_pred`).\n2. a real anndata to compare against (`adata_real`).\n\n### Prep (VCC)\n\nTo prepare an anndata for [VCC evaluation](https://virtualcellchallenge.org/) you can use the `cell-eval prep` command.\nThis will strip the anndata to bare essentials, compress it, adjust naming conventions, and ensure compatibility with the evaluation framework.\n\nThis step is optional for downstream usage, but recommended for optimal performance and compatibility.\n\nRun this on your predicted anndata:\n\n```bash\ncell-eval prep \\\n    -i <your/path/to>.h5ad \\\n    -g <expected_genelist>\n```\n\n### Run\n\nTo run an evaluation between two anndatas you can use the `cell-eval run` command.\n\nThis will run [differential expression](https://github.com/arcinstitute/pdex) for each anndata and then run a suite of\nevaluation metrics to compare the two (select your suite of metrics with the `--profile` flag).\n\nTo save time you can submit precomputed differential expression results, see the `cell-eval run --help` menu for more information.\n\n```bash\ncell-eval run \\\n    -ap <your/path/to/pred>.h5ad \\\n    -ar <your/path/to/real>.h5ad \\\n    --num-threads 64 \\\n    --profile full\n```\n\nTo run this as a python module you will need to use the `MetricsEvaluator` class.\n\n```python\nfrom cell_eval import MetricsEvaluator\nfrom cell_eval.data import build_random_anndata, downsample_cells\n\nadata_real = build_random_anndata()\nadata_pred = downsample_cells(adata_real, fraction=0.5)\nevaluator = MetricsEvaluator(\n    adata_pred=adata_pred,\n    adata_real=adata_real,\n    control_pert=\"control\",\n    pert_col=\"perturbation\",\n    num_threads=64,\n)\n(results, agg_results) = evaluator.compute()\n```\n\nThis will give you metric evaluations for each perturbation individually (`results`) and aggregated results over all perturbations (`agg_results`).\n\n### Score\n\nTo normalize your scores against a baseline you can run the `cell-eval score` command.\n\nThis accepts two `agg_results.csv` (or `agg_results` objects in python) as input.\n\n```bash\ncell-eval score \\\n    --user-input <your/path/to/user>/agg_results.csv \\\n    --base-input <your/path/to/base>/agg_results.csv\n```\n\nOr from python:\n\n```python\nfrom cell_eval import score_agg_metrics\n\nuser_input = \"./cell-eval-user/agg_results.csv\"\nbase_input = \"./cell-eval-base/agg_results.csv\"\noutput_path = \"./score.csv\"\n\nscore_agg_metrics(\n    results_user=user_input,\n    results_base=base_input,\n    output=output_path,\n)\n```\n\n## Library Design\n\nThe metrics are built using the python registry pattern. This allows for easy extension for new metrics with a well-typed interface.\n\nTake a look at existing metrics in `cell_eval.metrics` to get started.\n\n## Development\n\nThis work is open-source and welcomes contributions. Feel free to submit a pull request or open an issue.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "cell-eval", "package_url": "https://pypi.org/project/cell-eval/", "platform": null, "project_url": "https://pypi.org/project/cell-eval/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/cell-eval/0.5.42/", "requires_dist": ["igraph>=0.11.8", "pdex>=0.1.20", "polars>=1.30.0", "pyarrow>=18.0.0", "pyyaml>=6.0.2", "scanpy>=1.10.3", "tqdm>=4.67.1"], "requires_python": "<3.13,>=3.10", "summary": "Evaluation metrics for single-cell perturbation predictions", "version": "0.5.42", "yanked": false, "yanked_reason": null}, "last_serial": 30388519, "urls": [{"comment_text": null, "digests": {"blake2b_256": "928bf04fcc6a89f6024a3035a7b337209b796405f033b712cf2974b2af8dd0a3", "md5": "6aac251b7ab39686a9cb4222d9a87d9f", "sha256": "c83973d843d55a7e107c59023f74b09ab4e22ba8dfc4c3773a79741bbee2a5e5"}, "downloads": -1, "filename": "cell_eval-0.5.42-py3-none-any.whl", "has_sig": false, "md5_digest": "6aac251b7ab39686a9cb4222d9a87d9f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<3.13,>=3.10", "size": 37031, "upload_time": "2025-07-28T18:15:28", "upload_time_iso_8601": "2025-07-28T18:15:28.983826Z", "url": "https://files.pythonhosted.org/packages/92/8b/f04fcc6a89f6024a3035a7b337209b796405f033b712cf2974b2af8dd0a3/cell_eval-0.5.42-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "f3eef86d8eeab534714dc0facec207b469e7039e8ba4c3f8e65ede86c3a181ee", "md5": "8a760b7ba2350bf5d2b288f2ccca92f7", "sha256": "42976ac2006b769f1b223d63788ee09a5958c1b1305f8e5679370fbb9120a799"}, "downloads": -1, "filename": "cell_eval-0.5.42.tar.gz", "has_sig": false, "md5_digest": "8a760b7ba2350bf5d2b288f2ccca92f7", "packagetype": "sdist", "python_version": "source", "requires_python": "<3.13,>=3.10", "size": 30906, "upload_time": "2025-07-28T18:15:30", "upload_time_iso_8601": "2025-07-28T18:15:30.185227Z", "url": "https://files.pythonhosted.org/packages/f3/ee/f86d8eeab534714dc0facec207b469e7039e8ba4c3f8e65ede86c3a181ee/cell_eval-0.5.42.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:15:13 GMT", "package": "abjad", "version": "3.27", "json": {"info": {"author": null, "author_email": "Trevor Ba\u010da <trevor.baca@gmail.com>, Jos\u00e9phine Wolf Oberholtzer <josephine.wolf.oberholtzer@gmail.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "License :: OSI Approved :: GNU General Public License (GPL)", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Topic :: Artistic Software"], "description": "Abjad 3.27\n==========\n\nAbjad helps composers build up complex pieces of music notation in iterative\nand incremental ways. Use Abjad to create a symbolic representation of all the\nnotes, rests, chords, tuplets, beams and slurs in any score. Because Abjad\nextends the Python programming language, you can use Abjad to make systematic\nchanges to music as you work. Because Abjad wraps the LilyPond music notation\npackage, you can use Abjad to control the typographic detail of symbols on the\npage.\n\n..  image:: https://img.shields.io/badge/python-3.12-blue.svg\n    :target: https://www.python.org/downloads/release/python-312/\n    :alt: Python 3.12\n\n..  image:: https://img.shields.io/badge/python-3.13-blue.svg\n    :target: https://www.python.org/downloads/release/python-313/\n    :alt: Python 3.13\n\n..  image:: https://img.shields.io/pypi/v/abjad.svg?style=flat-square\n    :target: https://pypi.python.org/pypi/abjad\n\n..  image:: https://img.shields.io/pypi/dm/abjad.svg?style=flat-square\n    :target: https://pypi.python.org/pypi/abjad\n\n..  image:: https://img.shields.io/badge/code%20style-black-000000.svg\n    :target: https://github.com/ambv/black\n\n----\n\nAbjad's documentation is available here: https://abjad.github.io\n\nAbjad's install instructions are tested on macOS and Linux.\n\nAbjad requires Python 3.12 (or later):\n\n..  code-block::\n\n    ~$ python --version\n    Python 3.13.3\n\nAbjad requires LilyPond 2.25.26 (or later).\n\nMake sure LilyPond is installed:\nhttp://lilypond.org/development.html\n\nMake sure LilyPond is callable from the commandline:\n\n..  code-block::\n\n    $ lilypond --version\n    GNU LilyPond 2.25.26 (running Guile 3.0)\n\n    Copyright (c) 1996--2023 by\n      Han-Wen Nienhuys <hanwen@xs4all.nl>\n      Jan Nieuwenhuizen <janneke@gnu.org>\n      and others.\n\n    This program is free software.  It is covered by the GNU General Public\n    License and you are welcome to change it and/or distribute copies of it\n    under certain conditions.  Invoke as `lilypond --warranty' for more\n    information.\n\nCreate a Python 3 virtual environment for Abjad:\nhttps://docs.python.org/3/tutorial/venv.html\n\nActivate the virtual environment and then use pip to install Abjad:\n\n..  code-block::\n\n    ~$ python -m pip install abjad\n\nStart Python, import Abjad, start making music notation:\n\n..  code-block::\n\n    ~$ python\n    >>> import abjad\n    >>> note = abjad.Note(\"c'4\")\n    >>> abjad.show(note)\n\n..\n    ..  image:: hello.png\n\n----\n\nJoin the Abjad community: https://abjad.github.io/appendices/community.html\n", "description_content_type": "text/x-rst", "docs_url": "https://pythonhosted.org/abjad/", "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": "lilypond, music composition, music notation", "license": "MIT", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "abjad", "package_url": "https://pypi.org/project/abjad/", "platform": null, "project_url": "https://pypi.org/project/abjad/", "project_urls": {"Homepage": "https://abjad.github.io"}, "provides_extra": ["dev"], "release_url": "https://pypi.org/project/abjad/3.27/", "requires_dist": ["ply>=3.11", "roman>=5.0", "uqbar>=0.7.4", "black==25.1.0; extra == \"dev\"", "build==1.2.2.post1; extra == \"dev\"", "flake8==7.2.0; extra == \"dev\"", "isort==6.0.1; extra == \"dev\"", "mypy==1.15.0; extra == \"dev\"", "pytest==8.3.5; extra == \"dev\"", "pytest-cov==6.1.1; extra == \"dev\"", "pytest-helpers-namespace==2021.12.29; extra == \"dev\"", "setuptools==80.0.0; extra == \"dev\"", "sphinx==8.1.3; extra == \"dev\"", "sphinx-rtd-theme==3.0.2; extra == \"dev\"", "sphinx-toggleprompt==0.6.0; extra == \"dev\"", "twine==6.1.0; extra == \"dev\""], "requires_python": ">=3.12", "summary": "Abjad is a Python API for building LilyPond files.", "version": "3.27", "yanked": false, "yanked_reason": null}, "last_serial": 30388516, "urls": [{"comment_text": null, "digests": {"blake2b_256": "50458d9ee72cf0df9eb28ce829c317fb5e5e24077c5291d8a11fdd7f03a6d15d", "md5": "f632fdb73814bd8d4c17c3be6bb7261e", "sha256": "77a3230eda7a9c4dec877c635c8a06a257716b40dc3a7ce2fbfff50596ba2711"}, "downloads": -1, "filename": "abjad-3.27-py3-none-any.whl", "has_sig": false, "md5_digest": "f632fdb73814bd8d4c17c3be6bb7261e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.12", "size": 438116, "upload_time": "2025-07-28T18:15:13", "upload_time_iso_8601": "2025-07-28T18:15:13.623335Z", "url": "https://files.pythonhosted.org/packages/50/45/8d9ee72cf0df9eb28ce829c317fb5e5e24077c5291d8a11fdd7f03a6d15d/abjad-3.27-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "61bd56cf8879b0222aa20bbd3b4a3288d391b3b4a735fb8afb55b37f94140944", "md5": "2168137fad3c5567617e4642d9b26e84", "sha256": "bface6c836cfce707748329e1a801d8515ce1759a46cc3a97b1119ecafa4cc3f"}, "downloads": -1, "filename": "abjad-3.27.tar.gz", "has_sig": false, "md5_digest": "2168137fad3c5567617e4642d9b26e84", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.12", "size": 471259, "upload_time": "2025-07-28T18:15:15", "upload_time_iso_8601": "2025-07-28T18:15:15.327167Z", "url": "https://files.pythonhosted.org/packages/61/bd/56cf8879b0222aa20bbd3b4a3288d391b3b4a735fb8afb55b37f94140944/abjad-3.27.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:15:12 GMT", "package": "google-sheets-local", "version": "0.0.1", "json": {"info": {"author": "Circles", "author_email": "info@circlez.ai", "bugtrack_url": null, "classifiers": ["Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "PyPI Package for Circles google-sheets-local Python\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "Requires-Dist", "Summary"], "home_page": "https://github.com/circles-zone/google-sheets-local-python-package", "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "google-sheets-local", "package_url": "https://pypi.org/project/google-sheets-local/", "platform": null, "project_url": "https://pypi.org/project/google-sheets-local/", "project_urls": {"Homepage": "https://github.com/circles-zone/google-sheets-local-python-package"}, "provides_extra": null, "release_url": "https://pypi.org/project/google-sheets-local/0.0.1/", "requires_dist": ["python-sdk-remote", "logger-local", "user-external-local>=0.0.114", "api-management-local>=0.0.80", "google-auth-oauthlib>=1.2.2", "google-auth-httplib2>=0.2.0", "google-api-python-client>=2.128.0", "user-external-local>=0.0.149", "api-management-local>=0.0.80", "fields-local>=0.0.9", "database_mysql_local>=0.1.23", "storage_local>=0.1.59", "google_account_local>=0.0.27", "email_address_local>=0.0.65"], "requires_python": null, "summary": "PyPI Package for Circles google-sheets-local Python", "version": "0.0.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388515, "urls": [{"comment_text": null, "digests": {"blake2b_256": "77fa515551198616258cf10f9fda4ae383352a20dce2061b497bc8e966b72842", "md5": "db70b9fb8f248d4ea5ffc0a65a77613f", "sha256": "3fbb113f8b25928e74dd74665928b67d1e17ace134a08dc30c00430f718c00da"}, "downloads": -1, "filename": "google_sheets_local-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "db70b9fb8f248d4ea5ffc0a65a77613f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7606, "upload_time": "2025-07-28T18:15:12", "upload_time_iso_8601": "2025-07-28T18:15:12.124409Z", "url": "https://files.pythonhosted.org/packages/77/fa/515551198616258cf10f9fda4ae383352a20dce2061b497bc8e966b72842/google_sheets_local-0.0.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "59508968369bfb054e2b6933a2cfe84457f7fd23e2ad7ade35a3afe0d142cee1", "md5": "97934e3c6fee7511762a654b59d873ca", "sha256": "d31b7fd3491ede4a5f0f5040de45a8f9377c4291d46220676e98feeee2ab5da4"}, "downloads": -1, "filename": "google_sheets_local-0.0.1.tar.gz", "has_sig": false, "md5_digest": "97934e3c6fee7511762a654b59d873ca", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8182, "upload_time": "2025-07-28T18:15:13", "upload_time_iso_8601": "2025-07-28T18:15:13.972571Z", "url": "https://files.pythonhosted.org/packages/59/50/8968369bfb054e2b6933a2cfe84457f7fd23e2ad7ade35a3afe0d142cee1/google_sheets_local-0.0.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:15:11 GMT", "package": "layeutils", "version": "0.2.43", "json": {"info": {"author": "LayeWang", "author_email": "laye0619@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "Laye private utils\n", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "layeutils", "package_url": "https://pypi.org/project/layeutils/", "platform": null, "project_url": "https://pypi.org/project/layeutils/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/layeutils/0.2.43/", "requires_dist": null, "requires_python": null, "summary": "Private Utils", "version": "0.2.43", "yanked": false, "yanked_reason": null}, "last_serial": 30388512, "urls": [{"comment_text": null, "digests": {"blake2b_256": "000eae38b8034902aea319c71330685da56692da7ac44131206530ae62872273", "md5": "f48d91e49b1e34060ba6802be0bfc36d", "sha256": "98d7218306ef257587eac3677e7f83dee1466450b7217f8a890dc25d428cb823"}, "downloads": -1, "filename": "layeutils-0.2.43-py3-none-any.whl", "has_sig": false, "md5_digest": "f48d91e49b1e34060ba6802be0bfc36d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 17397, "upload_time": "2025-07-28T18:15:11", "upload_time_iso_8601": "2025-07-28T18:15:11.585938Z", "url": "https://files.pythonhosted.org/packages/00/0e/ae38b8034902aea319c71330685da56692da7ac44131206530ae62872273/layeutils-0.2.43-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "effe7f5adc4a78b95e69ea1ba9308a097e888067448547afcb2634f9fb129390", "md5": "e8617c80256d451568c7935021fd3ee8", "sha256": "a4da129d83cf2cf409d28bf9ce124641298e32560f9c42fe266a19ac3930aeee"}, "downloads": -1, "filename": "layeutils-0.2.43.tar.gz", "has_sig": false, "md5_digest": "e8617c80256d451568c7935021fd3ee8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13999, "upload_time": "2025-07-28T18:15:12", "upload_time_iso_8601": "2025-07-28T18:15:12.975773Z", "url": "https://files.pythonhosted.org/packages/ef/fe/7f5adc4a78b95e69ea1ba9308a097e888067448547afcb2634f9fb129390/layeutils-0.2.43.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:14:37 GMT", "package": "vacancycalculator", "version": "0.3.1.5", "json": {"info": {"author": "E.Bringa-S.Bergamin-SiMaF", "author_email": "santiagobergamin@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Home-Page", "License", "Requires-Dist", "Summary"], "home_page": "https://github.com/TiagoBe0/VFScript-SiMaF", "keywords": null, "license": "MIT", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "vacancycalculator", "package_url": "https://pypi.org/project/vacancycalculator/", "platform": null, "project_url": "https://pypi.org/project/vacancycalculator/", "project_urls": {"Homepage": "https://github.com/TiagoBe0/VFScript-SiMaF"}, "provides_extra": null, "release_url": "https://pypi.org/project/vacancycalculator/0.3.1.5/", "requires_dist": ["scikit-learn", "pandas", "xgboost", "ovito", "numpy"], "requires_python": null, "summary": "Defect analysis and vacancy calculation for materials science", "version": "0.3.1.5", "yanked": false, "yanked_reason": null}, "last_serial": 30388504, "urls": [{"comment_text": null, "digests": {"blake2b_256": "5567baad985e55adf8bbfb8893d2b28e9a516e5d8f2c5b296469916b8d2f3895", "md5": "cb69d766c623377155b2fecd3b4d7fce", "sha256": "f62c02a757535e3e6b578137fd3e24ed4c699bad2bf05be097c83303f11d2508"}, "downloads": -1, "filename": "vacancycalculator-0.3.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "cb69d766c623377155b2fecd3b4d7fce", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 59203, "upload_time": "2025-07-28T18:14:37", "upload_time_iso_8601": "2025-07-28T18:14:37.069686Z", "url": "https://files.pythonhosted.org/packages/55/67/baad985e55adf8bbfb8893d2b28e9a516e5d8f2c5b296469916b8d2f3895/vacancycalculator-0.3.1.5-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "0a216e43e58a4d5274939eca0788b6b335800abb042d339c040c5e486494124b", "md5": "fa4c69929456d4a6b20cd9262efa9a49", "sha256": "78ebbb1b0e83bdb2a24232f03d8f9e8c486618564621c32b525e2467e6050c5a"}, "downloads": -1, "filename": "vacancycalculator-0.3.1.5.tar.gz", "has_sig": false, "md5_digest": "fa4c69929456d4a6b20cd9262efa9a49", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 36353, "upload_time": "2025-07-28T18:14:38", "upload_time_iso_8601": "2025-07-28T18:14:38.200757Z", "url": "https://files.pythonhosted.org/packages/0a/21/6e43e58a4d5274939eca0788b6b335800abb042d339c040c5e486494124b/vacancycalculator-0.3.1.5.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:14:29 GMT", "package": "pulumi-artifactory", "version": "8.9.0a1753725884", "json": {"info": {"author": null, "author_email": null, "bugtrack_url": null, "classifiers": [], "description": "[![Actions Status](https://github.com/pulumi/pulumi-artifactory/workflows/main/badge.svg)](https://github.com/pulumi/pulumi-artifactory/actions)\n[![Slack](http://www.pulumi.com/images/docs/badges/slack.svg)](https://slack.pulumi.com)\n[![NPM version](https://badge.fury.io/js/%40pulumi%2Fartifactory.svg)](https://www.npmjs.com/package/@pulumi/artifactory)\n[![Python version](https://badge.fury.io/py/pulumi-artifactory.svg)](https://pypi.org/project/pulumi-artifactory)\n[![NuGet version](https://badge.fury.io/nu/pulumi.artifactory.svg)](https://badge.fury.io/nu/pulumi.artifactory)\n[![PkgGoDev](https://pkg.go.dev/badge/github.com/pulumi/pulumi-artifactory/sdk/go)](https://pkg.go.dev/github.com/pulumi/pulumi-artifactory/sdk/go)\n[![License](https://img.shields.io/npm/l/%40pulumi%2Fpulumi.svg)](https://github.com/pulumi/pulumi-artifactory/blob/main/LICENSE)\n\n# Artifactory Resource Provider\n\nThe Artifactory Resource Provider lets you manage Artifactory resources.\n\n## Installing\n\nThis package is available in many languages in the standard packaging formats.\n\n### Node.js (Java/TypeScript)\n\nTo use from JavaScript or TypeScript in Node.js, install using either `npm`:\n\n    $ npm install @pulumi/artifactory\n\nor `yarn`:\n\n    $ yarn add @pulumi/artifactory\n\n### Python\n\nTo use from Python, install using `pip`:\n\n    $ pip install pulumi_artifactory\n\n### Go\n\nTo use from Go, use `go get` to grab the latest version of the library\n\n    $ go get github.com/pulumi/pulumi-artifactory/sdk/v8\n\n### .NET\n\nTo use from .NET, install using `dotnet add package`:\n\n    $ dotnet add package Pulumi.Artifactory\n\n## Configuration\n\nThe following configuration points are available:\n\n- `artifactory:url` - (Required) URL of Artifactory. This can also be sourced from the `ARTIFACTORY_URL` environment variable.\n- `artifactory:username` - (Optional) Username for basic auth. Requires password to be set. Conflicts with `apiKey`, \n  and `accessToken`. This can also be sourced from the `ARTIFACTORY_USERNAME` environment variable.\n- `artifactory:password` - (Optional) Password for basic auth. Requires username to be set. Conflicts with `apiKey`, \n  and `accessToken`. This can also be sourced from the `ARTIFACTORY_PASSWORD` environment variable.\n- `artifactory:apiKey` - (Optional) API key for api auth. Uses `X-JFrog-Art-Api` header. Conflicts with `username`, \n  `password`, and `accessToken`. This can also be sourced from the `ARTIFACTORY_API_KEY` environment variable.\n- `artifactory:accessToken` - (Optional) API key for token auth. Uses `Authorization: Bearer` header. For xray \n  functionality, this is the only auth method accepted. Conflicts with `username` and `password`, and `apiKey`. This can\n  also be sourced from the `ARTIFACTORY_ACCESS_TOKEN` environment variable.\n\n## Reference\n\nFor further information, please visit [the Artifactory provider docs](https://www.pulumi.com/docs/intro/cloud-providers/artifactory)\nor for detailed reference documentation, please visit [the API docs](https://www.pulumi.com/docs/reference/pkg/artifactory).\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "pulumi, artifactory", "license": "Apache-2.0", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "pulumi-artifactory", "package_url": "https://pypi.org/project/pulumi-artifactory/", "platform": null, "project_url": "https://pypi.org/project/pulumi-artifactory/", "project_urls": {"Homepage": "https://pulumi.io", "Repository": "https://github.com/pulumi/pulumi-artifactory"}, "provides_extra": null, "release_url": "https://pypi.org/project/pulumi-artifactory/8.9.0a1753725884/", "requires_dist": ["parver>=0.2.1", "pulumi<4.0.0,>=3.165.0", "semver>=2.8.1", "typing-extensions<5,>=4.11; python_version < \"3.11\""], "requires_python": ">=3.9", "summary": "A Pulumi package for creating and managing artifactory cloud resources.", "version": "8.9.0a1753725884", "yanked": false, "yanked_reason": null}, "last_serial": 30388501, "urls": [{"comment_text": null, "digests": {"blake2b_256": "0d113433acce22be4f1b154c1eba03601943281b586ff22d9e5b930f1a3affa8", "md5": "b5f82a0a25785ef6779153b8e933f34c", "sha256": "59a3c85cd623904c80f7f6df022544665654eea151ea24f6f58f326a1a06f5e6"}, "downloads": -1, "filename": "pulumi_artifactory-8.9.0a1753725884-py3-none-any.whl", "has_sig": false, "md5_digest": "b5f82a0a25785ef6779153b8e933f34c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 1816997, "upload_time": "2025-07-28T18:14:29", "upload_time_iso_8601": "2025-07-28T18:14:29.481479Z", "url": "https://files.pythonhosted.org/packages/0d/11/3433acce22be4f1b154c1eba03601943281b586ff22d9e5b930f1a3affa8/pulumi_artifactory-8.9.0a1753725884-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "fd41b2a1d53414bc7fc7b2f42957bdaedb86cd5a61be80fccc4d70b640437616", "md5": "f118f67e3f656ea82dc77d57b01c39a5", "sha256": "b9132d65e873cf0c83a9726fb411a397c3092ec8a9f047108890e6dce74320bf"}, "downloads": -1, "filename": "pulumi_artifactory-8.9.0a1753725884.tar.gz", "has_sig": false, "md5_digest": "f118f67e3f656ea82dc77d57b01c39a5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 1160897, "upload_time": "2025-07-28T18:14:31", "upload_time_iso_8601": "2025-07-28T18:14:31.868194Z", "url": "https://files.pythonhosted.org/packages/fd/41/b2a1d53414bc7fc7b2f42957bdaedb86cd5a61be80fccc4d70b640437616/pulumi_artifactory-8.9.0a1753725884.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:13:35 GMT", "package": "llm-foundry", "version": "0.22.0.dev0", "json": {"info": {"author": "MosaicML", "author_email": "team@mosaicml.com", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12"], "description": "\n\n<p align=\"center\">\n    <a href=\"https://pypi.org/project/llm-foundry/\">\n        <img alt=\"PyPi Version\" src=\"https://img.shields.io/pypi/pyversions/llm-foundry\">\n    </a>\n    <a href=\"https://pypi.org/project/llm-foundry/\">\n        <img alt=\"PyPi Package Version\" src=\"https://img.shields.io/pypi/v/llm-foundry\">\n    </a>\n    <a href=\"https://mosaicml.me/slack\">\n        <img alt=\"Chat @ Slack\" src=\"https://img.shields.io/badge/slack-chat-2eb67d.svg?logo=slack\">\n    </a>\n    <a href=\"https://github.com/mosaicml/llm-foundry/blob/main/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache%202.0-green.svg\">\n    </a>\n</p>\n<br />\n\n# LLM Foundry\n\nThis repository contains code for training, finetuning, evaluating, and deploying LLMs for inference with [Composer](https://github.com/mosaicml/composer) and the [MosaicML platform](https://forms.mosaicml.com/demo?utm_source=github.com&utm_medium=referral&utm_campaign=llm-foundry). Designed to be easy-to-use, efficient _and_ flexible, this codebase enables rapid experimentation with the latest techniques.\n\nYou'll find in this repo:\n* `llmfoundry/` - source code for models, datasets, callbacks, utilities, etc.\n* `scripts/` - scripts to run LLM workloads\n  * `data_prep/` - convert text data from original sources to StreamingDataset format\n  * `train/` - train or finetune HuggingFace and MPT models from 125M - 70B parameters\n    * `train/benchmarking` - profile training throughput and MFU\n  * `inference/` - convert models to HuggingFace or ONNX format, and generate responses\n    * `inference/benchmarking` - profile inference latency and throughput\n  * `eval/` - evaluate LLMs on academic (or custom) in-context-learning tasks\n* `mcli/` - launch any of these workloads using [MCLI](https://docs.mosaicml.com/projects/mcli/en/latest/) and the [MosaicML platform](https://www.mosaicml.com/platform)\n* `TUTORIAL.md` - a deeper dive into the repo, example workflows, and FAQs\n\n# DBRX\n\nDBRX is a state-of-the-art open source LLM trained by Databricks Mosaic team. It uses the Mixture-of-Experts (MoE) architecture and was trained with optimized versions of [Composer](https://github.com/mosaicml/composer), LLM Foundry, and [MegaBlocks](https://github.com/databricks/megablocks). The model has 132B total parameters and 36B active parameters. We have released two DBRX models:\n\n\n| Model              | Context Length | Download                                           |\n| ------------------ | -------------- | -------------------------------------------------- |\n| DBRX Base          | 32768          | https://huggingface.co/databricks/dbrx-base        |\n| DBRX Instruct      | 32768          | https://huggingface.co/databricks/dbrx-instruct    |\n\nOur model weights and code are licensed for both researchers and commercial entities. The Databricks Open Source License can be found at [LICENSE](https://github.com/databricks/dbrx/blob/main/LICENSE), and our Acceptable Use Policy can be found [here](https://www.databricks.com/legal/acceptable-use-policy-open-model).\n\nFor more information about the DBRX models, see https://github.com/databricks/dbrx.\n\n# MPT\n\nMosaic Pretrained Transformers (MPT) are GPT-style models with some special features -- Flash Attention for efficiency, ALiBi for context length extrapolation, and stability improvements to mitigate loss spikes. As part of MosaicML's Foundation series, we have open-sourced several MPT models:\n\n\n| Model              | Context Length | Download                                           | Commercial use? |\n| ------------------ | -------------- | -------------------------------------------------- | --------------- |\n| MPT-30B            | 8192           | https://huggingface.co/mosaicml/mpt-30b            | Yes             |\n| MPT-30B-Instruct   | 8192           | https://huggingface.co/mosaicml/mpt-30b-instruct   | Yes             |\n| MPT-30B-Chat       | 8192           | https://huggingface.co/mosaicml/mpt-30b-chat       | No              |\n| MPT-7b-8k          | 8192           | https://huggingface.co/mosaicml/mpt-7b-8k          | Yes             |\n| MPT-7b-8k-Chat | 8192           | https://huggingface.co/mosaicml/mpt-7b-8k-chat         | No              |\n| MPT-7B             | 2048           | https://huggingface.co/mosaicml/mpt-7b             | Yes             |\n| MPT-7B-Instruct    | 2048           | https://huggingface.co/mosaicml/mpt-7b-instruct    | Yes             |\n| MPT-7B-Chat        | 2048           | https://huggingface.co/mosaicml/mpt-7b-chat        | No              |\n| MPT-7B-StoryWriter | 65536          | https://huggingface.co/mosaicml/mpt-7b-storywriter | Yes             |\n\nTo try out these models locally, [follow the instructions](https://github.com/mosaicml/llm-foundry/tree/main/scripts/inference#interactive-generation-with-modelgenerate) in `scripts/inference/README.md` to prompt HF models using our [hf_generate.py](https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/hf_generate.py) or [hf_chat.py](https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/hf_chat.py) scripts.\n\n# MPT Community\n\nWe've been overwhelmed by all the amazing work the community has put into MPT! Here we provide a few links to some of them:\n* [ReplitLM](https://github.com/replit/replitLM): `replit-code-v1-3b` is a 2.7B Causal Language Model focused on Code Completion. The model has been trained on a subset of the Stack Dedup v1.2 dataset covering 20 languages such as Java, Python, and C++\n* [LLaVa-MPT](https://github.com/haotian-liu/LLaVA#LLaVA-MPT-7b): Visual instruction tuning to get MPT multimodal capabilities\n* [ggml](https://github.com/ggerganov/ggml/tree/master): Optimized MPT version for efficient inference on consumer hardware\n* [GPT4All](https://gpt4all.io/index.html): locally running chat system, now with MPT support!\n* [Q8MPT-Chat](https://huggingface.co/spaces/Intel/Q8-Chat): 8-bit optimized MPT for CPU by our friends at Intel\n\nTutorial videos from the community:\n* [Using MPT-7B with Langchain](https://www.youtube.com/watch?v=DXpk9K7DgMo&t=3s) by [@jamesbriggs](https://www.youtube.com/@jamesbriggs)\n* [MPT-7B StoryWriter Intro](https://www.youtube.com/watch?v=O9Y_ZdsuKWQ) by [AItrepreneur](https://www.youtube.com/@Aitrepreneur)\n* [Fine-tuning MPT-7B on a single GPU](https://www.youtube.com/watch?v=KSlWkrByc0o&t=9s) by [@AIology2022](https://www.youtube.com/@AIology2022)\n* [How to Fine-tune MPT-7B-Instruct on Google Colab](https://youtu.be/3de0Utr9XnI) by [@VRSEN](https://www.youtube.com/@vrsen)\n\nSomething missing? Contribute with a PR!\n\n# Latest News\n* [Blog: Introducing DBRX: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)\n* [Blog: LLM Training and Inference with Intel Gaudi2 AI Accelerators](https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators)\n* [Blog: Training LLMs at Scale with AMD MI250 GPUs](https://www.databricks.com/blog/training-llms-scale-amd-mi250-gpus)\n* [Blog: Training LLMs with AMD MI250 GPUs and MosaicML](https://www.mosaicml.com/blog/amd-mi250)\n* [Blog: Announcing MPT-7B-8K: 8K Context Length for Document Understanding](https://www.mosaicml.com/blog/long-context-mpt-7b-8k)\n* [Blog: Training LLMs with AMD MI250 GPUs and MosaicML](https://www.mosaicml.com/blog/amd-mi250)\n* [Blog: MPT-30B: Raising the bar for open-source foundation models](https://www.mosaicml.com/blog/mpt-30b)\n* [Blog: Introducing MPT-7B](https://www.mosaicml.com/blog/mpt-7b)\n* [Blog: Benchmarking LLMs on H100](https://www.mosaicml.com/blog/coreweave-nvidia-h100-part-1)\n* [Blog: Blazingly Fast LLM Evaluation](https://www.mosaicml.com/blog/llm-evaluation-for-icl)\n* [Blog: GPT3 Quality for $500k](https://www.mosaicml.com/blog/gpt-3-quality-for-500k)\n* [Blog: Billion parameter GPT training made easy](https://www.mosaicml.com/blog/billion-parameter-gpt-training-made-easy)\n\n\n\n# Hardware and Software Requirements\nThis codebase has been tested with PyTorch 2.4 with NVIDIA A100s and H100s.\nThis codebase may also work on systems with other devices, such as consumer NVIDIA cards and AMD cards, but we are not actively testing these systems.\nIf you have success/failure using LLM Foundry on other systems, please let us know in a Github issue and we will update the support matrix!\n\n| Device         | Torch Version | Cuda Version | Status                       |\n| -------------- | ------------- | ------------ | ---------------------------- |\n| A100-40GB/80GB | 2.7.0         | 12.8         | :white_check_mark: Supported |\n| H100-80GB      | 2.7.0         | 12.8         | :white_check_mark: Supported |\n\n## MosaicML Docker Images\nWe highly recommend using our prebuilt Docker images. You can find them here: https://hub.docker.com/orgs/mosaicml/repositories.\n\nThe `mosaicml/pytorch` images are pinned to specific PyTorch and CUDA versions, and are stable and rarely updated.\n\nThe `mosaicml/llm-foundry` images are built with new tags upon every commit to the `main` branch.\nYou can select a specific commit hash such as `mosaicml/llm-foundry:2.7.0_cu128-9867a7b` or take the latest one using `mosaicml/llm-foundry:2.7.0_cu128-latest`.\n\n**Please Note:** The `mosaicml/llm-foundry` images do not come with the `llm-foundry` package preinstalled, just the dependencies. You will still need to `pip install llm-foundry` either from PyPi or from source.\n\n| Docker Image                                           | Torch Version | Cuda Version      | LLM Foundry dependencies installed? |\n| ------------------------------------------------------ | ------------- | ----------------- | ----------------------------------- |\n| `mosaicml/pytorch:2.7.0_cu128-python3.12-ubuntu22.04`  | 2.7.0         | 12.8 (Infiniband) | No                                  |\n| `mosaicml/llm-foundry:2.7.0_cu128-latest`              | 2.7.0         | 12.8 (Infiniband) | Yes                                 |\n| `mosaicml/llm-foundry:2.7.0_cu128_aws-latest`          | 2.7.0         | 12.8 (EFA)        | Yes                                 |\n\n\n# Installation\n\nThis assumes you already have PyTorch, CMake, and packaging installed. If not, you can install them with `pip install cmake packaging torch`.\n\nTo get started, clone the repo and set up your environment. Instructions to do so differ slightly depending on whether you're using Docker.\n\n### With Docker (recommended)\n\nWe *strongly* recommend working with LLM Foundry inside a Docker container (see our recommended Docker image above). If you are doing so, follow these steps to clone the repo and install the requirements.\n\n<!--pytest.mark.skip-->\n```bash\ngit clone https://github.com/mosaicml/llm-foundry.git\ncd llm-foundry\npip install -e \".[gpu]\"  # or `pip install -e .` if no NVIDIA GPU.\n```\n\n### Without Docker (not recommended)\n\nIf you choose not to use Docker, you should create and use a virtual environment.\n\n<!--pytest.mark.skip-->\n```bash\ngit clone https://github.com/mosaicml/llm-foundry.git\ncd llm-foundry\n\n# Creating and activate a virtual environment\npython3 -m venv llmfoundry-venv\nsource llmfoundry-venv/bin/activate\n\npip install cmake packaging torch  # setup.py requires these be installed\n\npip install -e \".[gpu]\"  # or `pip install -e .` if no NVIDIA GPU.\n```\n\n### TransformerEngine and amp_fp8 support\nNVIDIA H100 GPUs have FP8 support; we have installed Flash Attention and Transformer in our Docker images already (see above). If you are not using our Docker images, you can install these packages with:\n<!--pytest.mark.skip-->\n```bash\npip install flash-attn --no-build-isolation\npip install git+https://github.com/NVIDIA/TransformerEngine.git@stable\n```\n\nSee [here](https://github.com/mosaicml/llm-foundry/blob/main/TUTORIAL.md#TransformerEngine-and-amp_fp8-support) for more details on enabling TransformerEngine layers and amp_fp8.\n\n### AMD (BETA support)\n\nIn [our testing of AMD GPUs](https://www.mosaicml.com/blog/amd-mi250), the env setup includes:\n\n<!--pytest.mark.skip-->\n```bash\ngit clone https://github.com/mosaicml/llm-foundry.git\ncd llm-foundry\n\n# Creating and activate a virtual environment\npython3 -m venv llmfoundry-venv-amd\nsource llmfoundry-venv-amd/bin/activate\n\n# installs\npip install cmake packaging torch\npip install -e .  # This installs some things that are not needed but they don't hurt\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4.2\n```\n**Lastly**, install the ROCm enabled flash attention (instructions [here](https://github.com/ROCmSoftwarePlatform/flash-attention/tree/flash_attention_for_rocm2#amd-gpurocm-support)).\n\nNotes:\n1. We don't yet have a Docker image where everything works perfectly. You might need to up/downgrade some packages (in our case, we needed to downgrade to `numpy==1.23.5`) before everything works without issue.\n\n### Intel Gaudi\nSupport for LLM Foundry on Intel Gaudi devices is experimental, please use the branch `habana_alpha` and see the [README on that branch](https://github.com/mosaicml/llm-foundry/blob/habana_alpha) which has [install instructions and known issues.](https://github.com/mosaicml/llm-foundry/tree/habana_alpha?tab=readme-ov-file#intel-gaudi)\n\nFor training and inference performance results on Intel Gaudi2 accelerators, see our blog: https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators\n\n\n# Quickstart\n\n> **Note**\n> Make sure to go through the installation steps above before trying the quickstart!\n\nHere is an end-to-end workflow for preparing a subset of the C4 dataset, training an MPT-125M model for 10 batches,\nconverting the model to HuggingFace format, evaluating the model on the Winograd challenge, and generating responses to prompts.\n\n**(Remember this is a quickstart just to demonstrate the tools -- To get good quality, the LLM must be trained for longer than 10 batches \ud83d\ude04)**\n\n<!--pytest.mark.skip-->\n```bash\ncd scripts\n\n# Convert C4 dataset to StreamingDataset format\npython data_prep/convert_dataset_hf.py \\\n  --dataset allenai/c4 --data_subset en \\\n  --out_root my-copy-c4 --splits train_small val_small \\\n  --concat_tokens 2048 --tokenizer EleutherAI/gpt-neox-20b --eos_text '<|endoftext|>'\n\n# Train an MPT-125m model for 10 batches\ncomposer train/train.py \\\n  train/yamls/pretrain/mpt-125m.yaml \\\n  variables.data_local=my-copy-c4 \\\n  train_loader.dataset.split=train_small \\\n  eval_loader.dataset.split=val_small \\\n  max_duration=10ba \\\n  eval_interval=0 \\\n  save_folder=mpt-125m\n\n# Convert the model to HuggingFace format\npython inference/convert_composer_to_hf.py \\\n  --composer_path mpt-125m/ep0-ba10-rank0.pt \\\n  --hf_output_path mpt-125m-hf \\\n  --output_precision bf16 \\\n  # --hf_repo_for_upload user-org/repo-name\n\n# Evaluate the model on a subset of tasks\ncomposer eval/eval.py \\\n  eval/yamls/hf_eval.yaml \\\n  icl_tasks=eval/yamls/copa.yaml \\\n  model_name_or_path=mpt-125m-hf\n\n# Generate responses to prompts\npython inference/hf_generate.py \\\n  --name_or_path mpt-125m-hf \\\n  --max_new_tokens 256 \\\n  --prompts \\\n    \"The answer to life, the universe, and happiness is\" \\\n    \"Here's a quick recipe for baking chocolate chip cookies: Start by\"\n```\n\nNote: the `composer` command used above to train the model refers to the [Composer](https://github.com/mosaicml/composer) library's distributed launcher.\n\nIf you have a write-enabled [HuggingFace auth token](https://huggingface.co/docs/hub/security-tokens), you can optionally upload your model to the Hub! Just export your token like this:\n\n```bash\nexport HF_TOKEN=your-auth-token\n```\n\nand uncomment the line containing `--hf_repo_for_upload ...` in the above call to `inference/convert_composer_to_hf.py`.\n\n# Registry\n\nYou can use the registry to customize your workflows without forking the library. Some components of LLM Foundry are registrable, such as models, loggers, and callbacks. This means that you can register new options for these components, and then use them in your yaml config.\n\n## Discovering registrable components\nTo help find and understand registrable components, you can use the `llmfoundry registry` cli command.\n\nWe provide two commands currently:\n- `llmfoundry registry get [--group]`: List all registries, and their components, optionally specifying a specific registry. Example usage: `llmfoundry registry get --group loggers` or `llmfoundry registry get`\n- `llmfoundry registry find <group> <name>`: Get information about a specific registered component. Example usage: `llmfoundry registry find loggers wandb`\n\nUse `--help` on any of these commands for more information.\n\nThese commands can also help you understand what each registry is composed of, as each registry contains a docstring that will be printed out. The general concept is that each registry defines an interface, and components registered to that registry must implement that interface. If there is a part of the library that is not currently extendable, but you think it should be, please open an issue!\n\n## How to register\n\nThere are a few ways to register a new component:\n\n### Python entrypoints\n\nYou can specify registered components via a Python entrypoint if you are building your own package with registered components.\nThis would be the expected usage if you are building a large extension to LLM Foundry, and going to be overriding many components. Note that things registered via entrypoints will override components registered directly in code.\n\nFor example, the following would register the `MyLogger` class, under the key `my_logger`, in the `llm_foundry.loggers` registry:\n\n<!--pytest.mark.skip-->\n```yaml\n[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"foundry_registry\"\nversion = \"0.1.0\"\ndependencies = [\n    \"mosaicml\",\n    \"llm-foundry\",\n]\n\n# Note: Even though in python code, this would be llmfoundry.registry.loggers,\n# when specified in the entry_points, it has to be \"llmfoundry_loggers\". That is,\n# the segments of the name should be joined by an _ in the entry_points section.\n[project.entry-points.\"llmfoundry_loggers\"]\nmy_logger = \"foundry_registry.loggers:MyLogger\"\n```\n\nIf developing new components via entrypoints, it is important to note that Python entrypoints are global to the Python environment. This means that if you have multiple packages that register components with the same key, the last one installed will be the one used. This can be useful for overriding components in LLM Foundry, but can also lead to unexpected behavior if not careful. Additionally, if you change the pyproject.toml, you will need to reinstall the package for the changes to take effect. You can do this quickly by installing with `pip install -e . --no-deps` to avoid reinstalling dependencies.\n\n### Direct call to register\n\nYou can also register a component directly in your code:\n\n<!--pytest.mark.skip-->\n```python\nfrom composer.loggers import LoggerDestination\nfrom llmfoundry.registry import loggers\n\nclass MyLogger(LoggerDestination):\n    pass\n\nloggers.register(\"my_logger\", func=MyLogger)\n```\n\n### Decorators\n\nYou can also use decorators to register components directly from your code:\n\n<!--pytest.mark.skip-->\n```python\nfrom composer.loggers import LoggerDestination\nfrom llmfoundry.registry import loggers\n\n@loggers.register(\"my_logger\")\nclass MyLogger(LoggerDestination):\n    pass\n```\n\nFor both the direct call and decorator approaches, if using the LLM Foundry train/eval scripts, you will need to provide the `code_paths` argument, which is a list of files need to execute in order to register your components. For example, you may have a file called `foundry_imports.py` that contains the following:\n\n<!--pytest.mark.skip-->\n```python\nfrom foundry_registry.loggers import MyLogger\nfrom llmfoundry.registry import loggers\n\nloggers.register(\"my_logger\", func=MyLogger)\n```\n\nYou would then provide `code_paths` to the train/eval scripts in your yaml config:\n\n<!--pytest.mark.skip-->\n```yaml\n...\ncode_paths:\n  - foundry_imports.py\n...\n```\n\nOne of these would be the expected usage if you are building a small extension to LLM Foundry, only overriding a few components, and thus don't want to create an entire package.\n\n# Learn more about LLM Foundry!\n\nCheck out [TUTORIAL.md](https://github.com/mosaicml/llm-foundry/blob/main/TUTORIAL.md) to keep learning about working with LLM Foundry. The tutorial highlights example workflows, points you to other resources throughout the repo, and answers frequently asked questions!\n\n# Contact Us\n\nIf you run into any problems with the code, please file Github issues directly to this repo.\n\nIf you want to train LLMs on the MosaicML platform, reach out to us at [demo@mosaicml.com](mailto:demo@mosaicml.com)!\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "License-File", "Provides-Extra", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/mosaicml/llm-foundry/", "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "llm-foundry", "package_url": "https://pypi.org/project/llm-foundry/", "platform": null, "project_url": "https://pypi.org/project/llm-foundry/", "project_urls": {"Homepage": "https://github.com/mosaicml/llm-foundry/"}, "provides_extra": ["dev", "databricks", "tensorboard", "gpu-flash2", "gpu", "peft", "openai", "megablocks", "te", "databricks-serverless", "all-cpu", "all", "all-flash2"], "release_url": "https://pypi.org/project/llm-foundry/0.22.0.dev0/", "requires_dist": ["mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.33,>=0.32.1", "mlflow<3.0,>=2.14.1", "accelerate<1.9,>=0.25", "transformers<4.52,>=v4.51.0", "mosaicml-streaming<0.13,>=0.12.0", "torch<2.7.1,>=2.7.0", "datasets<3.7,>=3.3.2", "fsspec==2023.6.0", "sentencepiece==0.2.0", "einops==0.8.1", "omegaconf<3,>=2.2.3", "slack-sdk<4", "mosaicml-cli<1,>=0.6.10", "onnx==1.18.0", "onnxruntime==1.22.0", "boto3<2,>=1.21.45", "huggingface-hub[hf_xet]<0.34,>=0.30.0", "beautifulsoup4<5,>=4.12.2", "tenacity<10,>=8.2.3", "catalogue<3,>=2", "typer<1", "GitPython==3.1.44", "coverage[toml]==7.9.2; extra == \"dev\"", "pre-commit<4,>=3.4.0; extra == \"dev\"", "pytest<9,>=7.2.1; extra == \"dev\"", "pytest_codeblocks<0.18,>=0.16.1; extra == \"dev\"", "pytest-cov<7,>=4; extra == \"dev\"", "pyright==1.1.256; extra == \"dev\"", "toml<0.11,>=0.10.2; extra == \"dev\"", "packaging<26,>=21; extra == \"dev\"", "hf_transfer==0.1.9; extra == \"dev\"", "tenacity<10,>=9; extra == \"dev\"", "mosaicml[databricks]<0.33,>=0.32.1; extra == \"databricks\"", "numpy<2; extra == \"databricks\"", "databricks-sql-connector<4,>=3; extra == \"databricks\"", "databricks-connect==14.1.0; extra == \"databricks\"", "lz4<5,>=4; extra == \"databricks\"", "mosaicml[tensorboard]<0.33,>=0.32.1; extra == \"tensorboard\"", "flash-attn==2.7.4.post1; extra == \"gpu-flash2\"", "flash-attn==2.7.4.post1; extra == \"gpu\"", "mosaicml[peft]<0.33,>=0.32.1; extra == \"peft\"", "openai<2.0,>=1.56.0; extra == \"openai\"", "tiktoken<0.9.1,>=0.4; extra == \"openai\"", "megablocks<1.0; extra == \"megablocks\"", "grouped-gemm==0.3.0; extra == \"megablocks\"", "transformer-engine[pytorch]<2.0,>=1.13.0; extra == \"te\"", "numpy<2; extra == \"databricks-serverless\"", "coverage[toml]==7.9.2; extra == \"databricks-serverless\"", "tenacity<10,>=9; extra == \"databricks-serverless\"", "lz4<5,>=4; extra == \"databricks-serverless\"", "pytest-cov<7,>=4; extra == \"databricks-serverless\"", "mosaicml[peft]<0.33,>=0.32.1; extra == \"databricks-serverless\"", "tiktoken<0.9.1,>=0.4; extra == \"databricks-serverless\"", "pytest<9,>=7.2.1; extra == \"databricks-serverless\"", "pyright==1.1.256; extra == \"databricks-serverless\"", "databricks-sql-connector<4,>=3; extra == \"databricks-serverless\"", "openai<2.0,>=1.56.0; extra == \"databricks-serverless\"", "toml<0.11,>=0.10.2; extra == \"databricks-serverless\"", "hf_transfer==0.1.9; extra == \"databricks-serverless\"", "pytest_codeblocks<0.18,>=0.16.1; extra == \"databricks-serverless\"", "pre-commit<4,>=3.4.0; extra == \"databricks-serverless\"", "mosaicml[databricks]<0.33,>=0.32.1; extra == \"databricks-serverless\"", "packaging<26,>=21; extra == \"databricks-serverless\"", "numpy<2; extra == \"all-cpu\"", "coverage[toml]==7.9.2; extra == \"all-cpu\"", "tenacity<10,>=9; extra == \"all-cpu\"", "lz4<5,>=4; extra == \"all-cpu\"", "pytest-cov<7,>=4; extra == \"all-cpu\"", "mosaicml[peft]<0.33,>=0.32.1; extra == \"all-cpu\"", "tiktoken<0.9.1,>=0.4; extra == \"all-cpu\"", "pytest<9,>=7.2.1; extra == \"all-cpu\"", "pyright==1.1.256; extra == \"all-cpu\"", "databricks-sql-connector<4,>=3; extra == \"all-cpu\"", "openai<2.0,>=1.56.0; extra == \"all-cpu\"", "toml<0.11,>=0.10.2; extra == \"all-cpu\"", "hf_transfer==0.1.9; extra == \"all-cpu\"", "pytest_codeblocks<0.18,>=0.16.1; extra == \"all-cpu\"", "databricks-connect==14.1.0; extra == \"all-cpu\"", "pre-commit<4,>=3.4.0; extra == \"all-cpu\"", "mosaicml[databricks]<0.33,>=0.32.1; extra == \"all-cpu\"", "packaging<26,>=21; extra == \"all-cpu\"", "numpy<2; extra == \"all\"", "lz4<5,>=4; extra == \"all\"", "megablocks<1.0; extra == \"all\"", "openai<2.0,>=1.56.0; extra == \"all\"", "grouped-gemm==0.3.0; extra == \"all\"", "toml<0.11,>=0.10.2; extra == \"all\"", "pre-commit<4,>=3.4.0; extra == \"all\"", "packaging<26,>=21; extra == \"all\"", "tenacity<10,>=9; extra == \"all\"", "pytest-cov<7,>=4; extra == \"all\"", "databricks-sql-connector<4,>=3; extra == \"all\"", "mosaicml[databricks]<0.33,>=0.32.1; extra == \"all\"", "coverage[toml]==7.9.2; extra == \"all\"", "mosaicml[tensorboard]<0.33,>=0.32.1; extra == \"all\"", "tiktoken<0.9.1,>=0.4; extra == \"all\"", "pytest<9,>=7.2.1; extra == \"all\"", "pyright==1.1.256; extra == \"all\"", "flash-attn==2.7.4.post1; extra == \"all\"", "databricks-connect==14.1.0; extra == \"all\"", "mosaicml[peft]<0.33,>=0.32.1; extra == \"all\"", "hf_transfer==0.1.9; extra == \"all\"", "pytest_codeblocks<0.18,>=0.16.1; extra == \"all\"", "numpy<2; extra == \"all-flash2\"", "lz4<5,>=4; extra == \"all-flash2\"", "megablocks<1.0; extra == \"all-flash2\"", "openai<2.0,>=1.56.0; extra == \"all-flash2\"", "grouped-gemm==0.3.0; extra == \"all-flash2\"", "toml<0.11,>=0.10.2; extra == \"all-flash2\"", "pre-commit<4,>=3.4.0; extra == \"all-flash2\"", "packaging<26,>=21; extra == \"all-flash2\"", "tenacity<10,>=9; extra == \"all-flash2\"", "pytest-cov<7,>=4; extra == \"all-flash2\"", "databricks-sql-connector<4,>=3; extra == \"all-flash2\"", "mosaicml[databricks]<0.33,>=0.32.1; extra == \"all-flash2\"", "coverage[toml]==7.9.2; extra == \"all-flash2\"", "mosaicml[tensorboard]<0.33,>=0.32.1; extra == \"all-flash2\"", "tiktoken<0.9.1,>=0.4; extra == \"all-flash2\"", "pytest<9,>=7.2.1; extra == \"all-flash2\"", "pyright==1.1.256; extra == \"all-flash2\"", "flash-attn==2.7.4.post1; extra == \"all-flash2\"", "databricks-connect==14.1.0; extra == \"all-flash2\"", "mosaicml[peft]<0.33,>=0.32.1; extra == \"all-flash2\"", "hf_transfer==0.1.9; extra == \"all-flash2\"", "pytest_codeblocks<0.18,>=0.16.1; extra == \"all-flash2\""], "requires_python": ">=3.10", "summary": "LLM Foundry", "version": "0.22.0.dev0", "yanked": false, "yanked_reason": null}, "last_serial": 30388498, "urls": [{"comment_text": null, "digests": {"blake2b_256": "82984145b45e439c3b39a8802db9ec4eca27c1f31c32d848089db5687cc78064", "md5": "4ace3e35f871b2894574b0366e717e0a", "sha256": "6ee05db10936c2ca6e38e7b3de3a50825e7ef82b9e8497f9bcfbc16cda18b3ad"}, "downloads": -1, "filename": "llm_foundry-0.22.0.dev0-py3-none-any.whl", "has_sig": false, "md5_digest": "4ace3e35f871b2894574b0366e717e0a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.10", "size": 331037, "upload_time": "2025-07-28T18:13:35", "upload_time_iso_8601": "2025-07-28T18:13:35.012216Z", "url": "https://files.pythonhosted.org/packages/82/98/4145b45e439c3b39a8802db9ec4eca27c1f31c32d848089db5687cc78064/llm_foundry-0.22.0.dev0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "2ef6f0bfe2ecec65e1577ad69b29c141adcc75a5d081c16792e6dc487e903e13", "md5": "78c41ef95714152e0fd08588e46ad8fc", "sha256": "a88b2ea74a6d8f6216b9fd45af5688f7de97e27820106f99655e2ed33f838fc3"}, "downloads": -1, "filename": "llm_foundry-0.22.0.dev0.tar.gz", "has_sig": false, "md5_digest": "78c41ef95714152e0fd08588e46ad8fc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.10", "size": 281281, "upload_time": "2025-07-28T18:13:36", "upload_time_iso_8601": "2025-07-28T18:13:36.682715Z", "url": "https://files.pythonhosted.org/packages/2e/f6/f0bfe2ecec65e1577ad69b29c141adcc75a5d081c16792e6dc487e903e13/llm_foundry-0.22.0.dev0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:12:32 GMT", "package": "ingame", "version": "0.2.0", "json": {"info": {"author": null, "author_email": "Natuworkguy <info@nathannetwork.com>", "bugtrack_url": null, "classifiers": [], "description": "# InGame \ud83c\udfae\n\n**InGame** is a lightweight Python library designed to simplify making amazing UIs within a basic GUI window using `tkinter`. It enables developers to easily register and trigger events based on key presses with clean, decorator-based syntax.\n\n---\n\n## \u2728 Features\n\n- \u2705 Decorator-based event binding\n- \u2705 Enum-based key recognition (A\u2013Z, arrows, Enter, Escape, etc.)\n- \u2705 Clean and extensible architecture\n- \u2705 Simple GUI rendering using `tkinter`\n\n---\n\n## \ud83d\ude80 Getting Started\n\n### \ud83d\udd27 Installation\n\nUse `pip install ingame` to install the project.\n\n---\n\n## \ud83e\udde0 Usage Example\n\n```python\nfrom ingame.core import InGame, Screen, EventType\n\napp = InGame()\n\n@app.event(type=EventType.Key.A)\ndef handle_a():\n    print(\"Key A pressed!\")\n\n@app.event(type=EventType.Key.ESCAPE)\ndef handle_escape():\n    print(\"Escape pressed!\")\n    screen.quit()\n\nscreen = Screen(app, title=\"My InGame App\", width=600, height=400)\nscreen.show()\n````\n\n---\n\n## \ud83c\udfae Supported Keys\n\nSupported via `EventType.Key`, including:\n\n* A\u2013Z\n* Arrow keys: `UP`, `DOWN`, `LEFT`, `RIGHT`\n* `ENTER`, `ESCAPE`, `BACKSPACE`\n\n---\n\n## \ud83d\udce6 Components\n\n### `InGame`\n\nHandles registering and triggering events:\n\n* `@event(type: EventType.Key)`: Registers a function for a specific key event.\n* `trigger_event(type)`: Manually triggers an event.\n\n### `Screen`\n\nSimple `tkinter` window with key event binding:\n\n* `show()`: Opens the window and starts listening for key presses.\n* `quit()`: Closes the window.\n\n---\n\n## \u26a0\ufe0f Exceptions\n\n* `InGameException`: Raised for invalid usage such as missing event type or unregistered keys.\n\n---\n\n## \ud83d\udee0\ufe0f Development Notes\n\nWritten in Python 3.10+\nUses `tkinter`, `Enum`, and `inspect`.\n\n---\n\n## \ud83d\udcc4 License\n\n[MIT License](LICENSE)\n\n---\n\n## \u2764\ufe0f Contributing\n\nPull requests are welcome. For major changes, please open an issue first to discuss what you\u2019d like to change.\n\n---\n\n## \ud83d\udc64 Author\n\nMade with \u2764\ufe0f by [Natuworkguy](https://github.com/Natuworkguy/)\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": null, "license": "MIT", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "ingame", "package_url": "https://pypi.org/project/ingame/", "platform": null, "project_url": "https://pypi.org/project/ingame/", "project_urls": {"Bug Tracker": "https://github.com/Natuworkguy/ingame/issues", "Homepage": "https://github.com/Natuworkguy/ingame"}, "provides_extra": null, "release_url": "https://pypi.org/project/ingame/0.2.0/", "requires_dist": null, "requires_python": ">=3.9.6", "summary": "A window and UI library based on tkinter", "version": "0.2.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388494, "urls": [{"comment_text": null, "digests": {"blake2b_256": "96783ed7196b0ddce1ce0ff1d5cb8554f7410a43928eec14823b3ad564cb39dc", "md5": "f6f19c548fcf5261faf6d8bec923b83e", "sha256": "f35f49a33360587bfd818d44c7ee441bc2e23c39613c564c8c8bfc6e11817c51"}, "downloads": -1, "filename": "ingame-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f6f19c548fcf5261faf6d8bec923b83e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9.6", "size": 4811, "upload_time": "2025-07-28T18:12:32", "upload_time_iso_8601": "2025-07-28T18:12:32.216999Z", "url": "https://files.pythonhosted.org/packages/96/78/3ed7196b0ddce1ce0ff1d5cb8554f7410a43928eec14823b3ad564cb39dc/ingame-0.2.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "86303ca4ee40f9ded01cc55baed3dcb79028926a86a0eebaea0a11e08f64facb", "md5": "b80e1a9cc8b26047dc2e5181e255491c", "sha256": "aa8eb6807bf69eacfea014bca773a382e34c1d197fe5c1c4611fd376fe52c71a"}, "downloads": -1, "filename": "ingame-0.2.0.tar.gz", "has_sig": false, "md5_digest": "b80e1a9cc8b26047dc2e5181e255491c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9.6", "size": 4299, "upload_time": "2025-07-28T18:12:33", "upload_time_iso_8601": "2025-07-28T18:12:33.037292Z", "url": "https://files.pythonhosted.org/packages/86/30/3ca4ee40f9ded01cc55baed3dcb79028926a86a0eebaea0a11e08f64facb/ingame-0.2.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:12:09 GMT", "package": "cua-core", "version": "0.1.6", "json": {"info": {"author": null, "author_email": "TryCua <gh@trycua.com>", "bugtrack_url": null, "classifiers": [], "description": "<div align=\"center\">\n<h1>\n  <div class=\"image-wrapper\" style=\"display: inline-block;\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" alt=\"logo\" height=\"150\" srcset=\"../../img/logo_white.png\" style=\"display: block; margin: auto;\">\n      <source media=\"(prefers-color-scheme: light)\" alt=\"logo\" height=\"150\" srcset=\"../../img/logo_black.png\" style=\"display: block; margin: auto;\">\n      <img alt=\"Shows my svg\">\n    </picture>\n  </div>\n\n  [![Python](https://img.shields.io/badge/Python-333333?logo=python&logoColor=white&labelColor=333333)](#)\n  [![macOS](https://img.shields.io/badge/macOS-000000?logo=apple&logoColor=F0F0F0)](#)\n  [![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&logo=discord&logoColor=white)](https://discord.com/invite/mVnXXpdE85)\n  [![PyPI](https://img.shields.io/pypi/v/cua-core?color=333333)](https://pypi.org/project/cua-core/)\n</h1>\n</div>\n\n**Cua Core** provides essential shared functionality and utilities used across the Cua ecosystem:\n\n- Privacy-focused telemetry system for transparent usage analytics\n- Common helper functions and utilities used by other Cua packages\n- Core infrastructure components shared between modules\n\n## Installation\n\n```bash\npip install cua-core\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "cua-core", "package_url": "https://pypi.org/project/cua-core/", "platform": null, "project_url": "https://pypi.org/project/cua-core/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/cua-core/0.1.6/", "requires_dist": ["pydantic>=2.0.0", "httpx>=0.24.0", "posthog>=3.20.0"], "requires_python": ">=3.11", "summary": "Core functionality for Cua including telemetry and shared utilities", "version": "0.1.6", "yanked": false, "yanked_reason": null}, "last_serial": 30388491, "urls": [{"comment_text": null, "digests": {"blake2b_256": "d71727fd5b1f3969cedba7282c1cef3fdfe7c465475ba0d8d4ef82f5248fb462", "md5": "03b509f01a50e3ab7da7bce6769069d9", "sha256": "090d47dfdda6beee182c99e296faa2b281d9572694f344cdb31daf69bc108a7e"}, "downloads": -1, "filename": "cua_core-0.1.6-py3-none-any.whl", "has_sig": false, "md5_digest": "03b509f01a50e3ab7da7bce6769069d9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.11", "size": 11942, "upload_time": "2025-07-28T18:12:09", "upload_time_iso_8601": "2025-07-28T18:12:09.114482Z", "url": "https://files.pythonhosted.org/packages/d7/17/27fd5b1f3969cedba7282c1cef3fdfe7c465475ba0d8d4ef82f5248fb462/cua_core-0.1.6-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "091e24fdc4b6843ebb65af5826211c8494cbb6091cc8fe30f9807a0345659eb1", "md5": "aa3f1e991cd548309c1fe6b60169e290", "sha256": "8992feb8e2b5d7ba2d0a42aa46c099862a198f89a3584ef47e52e27e3db7cc94"}, "downloads": -1, "filename": "cua_core-0.1.6.tar.gz", "has_sig": false, "md5_digest": "aa3f1e991cd548309c1fe6b60169e290", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.11", "size": 9006, "upload_time": "2025-07-28T18:12:09", "upload_time_iso_8601": "2025-07-28T18:12:09.872443Z", "url": "https://files.pythonhosted.org/packages/09/1e/24fdc4b6843ebb65af5826211c8494cbb6091cc8fe30f9807a0345659eb1/cua_core-0.1.6.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:12:05 GMT", "package": "cmip-branded-variable-mapper", "version": "0.9.0", "json": {"info": {"author": null, "author_email": "Zebedee Nicholls <zebedee.nicholls@climate-energy-college.org>", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Science/Research", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3 :: Only", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Programming Language :: Python :: 3.9", "Typing :: Typed"], "description": "<!--- --8<-- [start:description] -->\n# CMIP Branded Variable Mapper\n\nMapping from CMIP variable and other information to branded variable names.\n\n**Key info :**\n[![Docs](https://readthedocs.org/projects/cmip-branded-variable-mapper/badge/?version=latest)](https://cmip-branded-variable-mapper.readthedocs.io)\n[![Main branch: supported Python versions](https://img.shields.io/python/required-version-toml?tomlFilePath=https%3A%2F%2Fraw.githubusercontent.com%2Fznicholls%2FCMIP-branded-variables-mapper%2Fmain%2Fpyproject.toml)](https://github.com/znicholls/CMIP-branded-variables-mapper/blob/main/pyproject.toml)\n[![Licence](https://img.shields.io/pypi/l/cmip-branded-variable-mapper?label=licence)](https://github.com/znicholls/CMIP-branded-variables-mapper/blob/main/LICENCE)\n\n**PyPI :**\n[![PyPI](https://img.shields.io/pypi/v/cmip-branded-variable-mapper.svg)](https://pypi.org/project/cmip-branded-variable-mapper/)\n[![PyPI install](https://github.com/znicholls/CMIP-branded-variables-mapper/actions/workflows/install-pypi.yaml/badge.svg?branch=main)](https://github.com/znicholls/CMIP-branded-variables-mapper/actions/workflows/install-pypi.yaml)\n\n**Tests :**\n[![CI](https://github.com/znicholls/CMIP-branded-variables-mapper/actions/workflows/ci.yaml/badge.svg?branch=main)](https://github.com/znicholls/CMIP-branded-variables-mapper/actions/workflows/ci.yaml)\n[![Coverage](https://codecov.io/gh/znicholls/CMIP-branded-variables-mapper/branch/main/graph/badge.svg)](https://codecov.io/gh/znicholls/CMIP-branded-variables-mapper)\n\n**Other info :**\n[![Last Commit](https://img.shields.io/github/last-commit/znicholls/CMIP-branded-variables-mapper.svg)](https://github.com/znicholls/CMIP-branded-variables-mapper/commits/main)\n[![Contributors](https://img.shields.io/github/contributors/znicholls/CMIP-branded-variables-mapper.svg)](https://github.com/znicholls/CMIP-branded-variables-mapper/graphs/contributors)\n## Status\n\n<!---\n\nWe recommend having a status line in your repo\nto tell anyone who stumbles on your repository where you're up to.\nSome suggested options:\n\n- prototype: the project is just starting up and the code is all prototype\n- development: the project is actively being worked on\n- finished: the project has achieved what it wanted\n  and is no longer being worked on, we won't reply to any issues\n- dormant: the project is no longer worked on\n  but we might come back to it,\n  if you have questions, feel free to raise an issue\n- abandoned: this project is no longer worked on\n  and we won't reply to any issues\n-->\n\n- prototype: the project is just starting up and the code is all prototype\n\n<!--- --8<-- [end:description] -->\n\nFull documentation can be found at:\n[cmip-branded-variable-mapper.readthedocs.io](https://cmip-branded-variable-mapper.readthedocs.io/en/latest/).\nWe recommend reading the docs there because the internal documentation links\ndon't render correctly on GitHub's viewer.\n\n## Installation\n\n<!--- --8<-- [start:installation] -->\n### As an application\n\nIf you want to use CMIP Branded Variable Mapper as an application,\nthen we recommend using the 'locked' version of the package.\nThis version pins the version of all dependencies too,\nwhich reduces the chance of installation issues\nbecause of breaking updates to dependencies.\n\nThe locked version of CMIP Branded Variable Mapper can be installed with\n\n=== \"pip\"\n    ```sh\n    pip install 'cmip-branded-variable-mapper[locked]'\n    ```\n\n### As a library\n\nIf you want to use CMIP Branded Variable Mapper as a library,\nfor example you want to use it\nas a dependency in another package/application that you're building,\nthen we recommend installing the package with the commands below.\nThis method provides the loosest pins possible of all dependencies.\nThis gives you, the package/application developer,\nas much freedom as possible to set the versions of different packages.\nHowever, the tradeoff with this freedom is that you may install\nincompatible versions of CMIP Branded Variable Mapper's dependencies\n(we cannot test all combinations of dependencies,\nparticularly ones which haven't been released yet!).\nHence, you may run into installation issues.\nIf you believe these are because of a problem in CMIP Branded Variable Mapper,\nplease [raise an issue](https://github.com/znicholls/CMIP-branded-variables-mapper/issues).\n\nThe (non-locked) version of CMIP Branded Variable Mapper can be installed with\n\n=== \"pip\"\n    ```sh\n    pip install cmip-branded-variable-mapper\n    ```\n\n### For developers\n\nFor development, we rely on [uv](https://docs.astral.sh/uv/)\nfor all our dependency management.\nTo get started, you will need to make sure that uv is installed\n([instructions here](https://docs.astral.sh/uv/getting-started/installation/)\n(we found that the self-managed install was best,\nparticularly for upgrading uv later).\n\nFor all of our work, we use our `Makefile`.\nYou can read the instructions out and run the commands by hand if you wish,\nbut we generally discourage this because it can be error prone.\nIn order to create your environment, run `make virtual-environment`.\n\nIf there are any issues, the messages from the `Makefile` should guide you through.\nIf not, please raise an issue in the\n[issue tracker](https://github.com/znicholls/CMIP-branded-variables-mapper/issues).\n\nFor the rest of our developer docs, please see [development][development].\n\n<!--- --8<-- [end:installation] -->\n\n## Original template\n\nThis project was generated from this template:\n[copier core python repository](https://gitlab.com/openscm/copier-core-python-repository).\n[copier](https://copier.readthedocs.io/en/stable/) is used to manage and\ndistribute this template.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": "placeholder", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "cmip-branded-variable-mapper", "package_url": "https://pypi.org/project/cmip-branded-variable-mapper/", "platform": null, "project_url": "https://pypi.org/project/cmip-branded-variable-mapper/", "project_urls": {"Changelog": "https://cmip-branded-variable-mapper.readthedocs.io/en/stable/changelog", "Documentation": "https://cmip-branded-variable-mapper.readthedocs.io", "Homepage": "https://cmip-branded-variable-mapper.readthedocs.io", "Issues": "https://github.com/znicholls/CMIP-branded-variables-mapper/issues", "Repository": "https://github.com/znicholls/CMIP-branded-variables-mapper"}, "provides_extra": ["locked"], "release_url": "https://pypi.org/project/cmip-branded-variable-mapper/0.9.0/", "requires_dist": ["attrs>=24.3.0", "openpyxl>=3.1.5", "pandas>=2.2.3", "numpy>=1.26.0; python_version < \"3.13\"", "numpy>=2.1.0; python_version >= \"3.13\"", "tabulate>=0.9.0", "attrs==24.3.0; extra == \"locked\"", "et-xmlfile==2.0.0; extra == \"locked\"", "numpy==2.0.2; python_version < \"3.13\" and python_version >= \"3.9\" and extra == \"locked\"", "numpy==2.3.2; python_version >= \"3.13\" and extra == \"locked\"", "openpyxl==3.1.5; extra == \"locked\"", "pandas==2.3.1; extra == \"locked\"", "python-dateutil==2.9.0.post0; extra == \"locked\"", "pytz==2025.2; extra == \"locked\"", "six==1.17.0; extra == \"locked\"", "tabulate==0.9.0; extra == \"locked\"", "tzdata==2025.2; extra == \"locked\""], "requires_python": ">=3.9", "summary": "Mapping from CMIP variable and other information to branded variable names.", "version": "0.9.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388488, "urls": [{"comment_text": null, "digests": {"blake2b_256": "fdf3327cdbcd7cdb747841879a8f9a779bd992f9e8d34251e47962cfeb55128c", "md5": "14738db2320a1ead68024c2663543857", "sha256": "92ab518fe7e1b21e9a426b2cf93c46d621cb58d459ce498740e53c40a3b4c759"}, "downloads": -1, "filename": "cmip_branded_variable_mapper-0.9.0-py3-none-any.whl", "has_sig": false, "md5_digest": "14738db2320a1ead68024c2663543857", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 11626, "upload_time": "2025-07-28T18:12:05", "upload_time_iso_8601": "2025-07-28T18:12:05.002135Z", "url": "https://files.pythonhosted.org/packages/fd/f3/327cdbcd7cdb747841879a8f9a779bd992f9e8d34251e47962cfeb55128c/cmip_branded_variable_mapper-0.9.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "7b6287993ae586195594ba93e6e217fed0b67c248475940dac1db54210b12b73", "md5": "14316f013659eb9fbaffa217998ec272", "sha256": "28ad0dbb1bfc100a9c1c36f2e0756e6118a8af5d0800fb2b51f083a0711ddb83"}, "downloads": -1, "filename": "cmip_branded_variable_mapper-0.9.0.tar.gz", "has_sig": false, "md5_digest": "14316f013659eb9fbaffa217998ec272", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 48427, "upload_time": "2025-07-28T18:12:06", "upload_time_iso_8601": "2025-07-28T18:12:06.029933Z", "url": "https://files.pythonhosted.org/packages/7b/62/87993ae586195594ba93e6e217fed0b67c248475940dac1db54210b12b73/cmip_branded_variable_mapper-0.9.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:11:58 GMT", "package": "translatable-xblocks", "version": "1.5.0", "json": {"info": {"author": "Nathan Sprenkle", "author_email": "nsprenkle@2u.org", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Framework :: Django", "Framework :: Django :: 4.2", "Intended Audience :: Developers", "Natural Language :: English", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.11"], "description": "translatable-xblocks\n####################\n\n|pypi-badge| |ci-badge| |codecov-badge| |doc-badge| |pyversions-badge|\n|license-badge| |status-badge|\n\nPurpose\n*******\n\nPlugin for adding translation behavior to XBlocks. This plugin assumes:\n\n1. Translations IDA is setup (https://github.com/edx/ai-translations).\n\n2. Frontend plugin setup to add translation selector.\n\nWhen present and enabled, requesting a ``src_lang`` and ``dest_lang`` as query params in a unit render\nshould request and return translated versions of those XBlocks' content.\n\n**WARNING**: This replaces some native XBlocks from ``edx-platform`` and replaces them with custom versions.\nWhile we only override small portions of functionality, there is a chance this will cause things to \"spookily\"\nbreak. See `translatable_xblocks/apps.py <translatable_xblocks/apps.py>`_ for where we do this block replacement.\n\nGuides\n******\n\n- `Quickstart`_.\n- `How to test this repository`_.\n- `How to deploy this repository`_.\n\n.. _Quickstart: docs/quickstarts/index.rst\n.. _How to test this repository: docs/how-tos/test_this_repo.rst\n.. _How to deploy this repository: docs/how-tos/deploy_this_repo.rst\n\nGetting Started with Development\n********************************\n\nSee `quickstart guide`.\n\n.. _quickstart guide: docs/quickstarts/index.rst\n\n\nConventions / Rules / Best Practices\n====================================\n\n1. Use conventional commits for making commits.\n\n2. Format files with ``black``. Do this and other fixes automatically with ``make format``.\n\n3. Squash commits when merging a PR.\n\nPlease see also the Open edX documentation for `guidance on Python development <https://docs.openedx.org/en/latest/developers/how-tos/get-ready-for-python-dev.html>`_ in this repo.\n\nTesting\n=======\n\nSee `how to test this repository`_.\n\n.. _how to test this repository: docs/how-tos/test_this_repo.rst\n\nDeploying\n*********\n\nSee `how to deploy this repository`_.\n\n.. _how to deploy this repository: docs/how-tos/deploy_this_repo.rst\n\nGetting Help\n************\n\nDocumentation\n=============\n\nPLACEHOLDER: Start by going through `the documentation`_.  If you need more help see below.\n\n.. _the documentation: https://docs.openedx.org/projects/translatable-xblocks\n\n(TODO: `Set up documentation <https://openedx.atlassian.net/wiki/spaces/DOC/pages/21627535/Publish+Documentation+on+Read+the+Docs>`_)\n\nMore Help\n=========\n\nIf you're having trouble, we have discussion forums at\nhttps://discuss.openedx.org where you can connect with others in the\ncommunity.\n\nOur real-time conversations are on Slack. You can request a `Slack\ninvitation`_, then join our `community Slack workspace`_.\n\nFor anything non-trivial, the best path is to open an issue in this\nrepository with as many details about the issue you are facing as you\ncan provide.\n\nhttps://github.com/edx/translatable-xblocks/issues\n\nFor more information about these options, see the `Getting Help <https://openedx.org/getting-help>`__ page.\n\n.. _Slack invitation: https://openedx.org/slack\n.. _community Slack workspace: https://openedx.slack.com/\n\nLicense\n*******\n\nThe code in this repository is licensed under the AGPL 3.0 unless\notherwise noted.\n\nPlease see `LICENSE.txt <LICENSE.txt>`_ for details.\n\nContributing\n************\n\nContributions are very welcome.\nPlease read `How To Contribute <https://openedx.org/r/how-to-contribute>`_ for details.\n\nThis project is currently accepting all types of contributions, bug fixes,\nsecurity fixes, maintenance work, or new features.  However, please make sure\nto have a discussion about your new feature idea with the maintainers prior to\nbeginning development to maximize the chances of your change being accepted.\nYou can start a conversation by creating a new issue on this repo summarizing\nyour idea.\n\nThe Open edX Code of Conduct\n****************************\n\nAll community members are expected to follow the `Open edX Code of Conduct`_.\n\n.. _Open edX Code of Conduct: https://openedx.org/code-of-conduct/\n\nPeople\n******\n\nThe assigned maintainers for this component and other project details may be\nfound in `Backstage`_. Backstage pulls this data from the ``catalog-info.yaml``\nfile in this repo.\n\n.. _Backstage: https://backstage.openedx.org/catalog/default/component/translatable-xblocks\n\nReporting Security Issues\n*************************\n\nPlease do not report security issues in public. Please email security@openedx.org.\n\n.. |pypi-badge| image:: https://img.shields.io/pypi/v/translatable-xblocks.svg\n    :target: https://pypi.python.org/pypi/translatable-xblocks/\n    :alt: PyPI\n\n.. |ci-badge| image:: https://github.com/edx/translatable-xblocks/workflows/Python%20CI/badge.svg?branch=main\n    :target: https://github.com/edx/translatable-xblocks/actions\n    :alt: CI\n\n.. |codecov-badge| image:: https://codecov.io/github/edx/translatable-xblocks/coverage.svg?branch=main\n    :target: https://codecov.io/github/edx/translatable-xblocks?branch=main\n    :alt: Codecov\n\n.. |doc-badge| image:: https://readthedocs.org/projects/translatable-xblocks/badge/?version=latest\n    :target: https://docs.openedx.org/projects/translatable-xblocks\n    :alt: Documentation\n\n.. |pyversions-badge| image:: https://img.shields.io/pypi/pyversions/translatable-xblocks.svg\n    :target: https://pypi.python.org/pypi/translatable-xblocks/\n    :alt: Supported Python versions\n\n.. |license-badge| image:: https://img.shields.io/github/license/edx/translatable-xblocks.svg\n    :target: https://github.com/edx/translatable-xblocks/blob/main/LICENSE.txt\n    :alt: License\n\n.. TODO: Choose one of the statuses below and remove the other status-badge lines.\n.. |status-badge| image:: https://img.shields.io/badge/Status-Experimental-yellow\n.. .. |status-badge| image:: https://img.shields.io/badge/Status-Maintained-brightgreen\n.. .. |status-badge| image:: https://img.shields.io/badge/Status-Deprecated-orange\n.. .. |status-badge| image:: https://img.shields.io/badge/Status-Unsupported-red\n\n\nChange Log\n##########\n\n..\n   All enhancements and patches to translatable_xblocks will be documented\n   in this file.  It adheres to the structure of https://keepachangelog.com/ ,\n   but in reStructuredText instead of Markdown (for ease of incorporation into\n   Sphinx documentation and the PyPI description).\n\n   This project adheres to Semantic Versioning (https://semver.org/).\n\n.. There should always be an \"Unreleased\" section for changes pending release.\n\nUnreleased\n**********\n\n*\n\n0.1.0 \u2013 2024-03-06\n**********************************************\n\nAdded\n=====\n\n* First release on PyPI.\n", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Home-Page", "Keywords", "License", "License-File", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/edx/translatable-xblocks", "keywords": "Python edx", "license": "AGPL-3.0-or-later", "license_expression": null, "license_files": ["LICENSE", "LICENSE.txt"], "maintainer": null, "maintainer_email": null, "name": "translatable-xblocks", "package_url": "https://pypi.org/project/translatable-xblocks/", "platform": null, "project_url": "https://pypi.org/project/translatable-xblocks/", "project_urls": {"Homepage": "https://github.com/edx/translatable-xblocks"}, "provides_extra": null, "release_url": "https://pypi.org/project/translatable-xblocks/1.5.0/", "requires_dist": ["Django", "django-oauth-toolkit", "djangorestframework", "edx-opaque-keys", "edx-rest-api-client", "edx_django_utils", "openedx-atlas", "openedx-filters", "xblock>=5.1.0"], "requires_python": ">=3.11", "summary": "Plugin for adding translation behavior to XBlocks", "version": "1.5.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388485, "urls": [{"comment_text": null, "digests": {"blake2b_256": "83cf277012589deea3fcdfa236dc8bffe47e43f9fdccb5a8f2ec1e575b934d30", "md5": "8db119aa6b15daa3197f8c9bd7c94163", "sha256": "466f9f361719d9aa722db26a69c6684406a9c253926b32200bc63340dae35603"}, "downloads": -1, "filename": "translatable_xblocks-1.5.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "8db119aa6b15daa3197f8c9bd7c94163", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.11", "size": 46720, "upload_time": "2025-07-28T18:11:58", "upload_time_iso_8601": "2025-07-28T18:11:58.703285Z", "url": "https://files.pythonhosted.org/packages/83/cf/277012589deea3fcdfa236dc8bffe47e43f9fdccb5a8f2ec1e575b934d30/translatable_xblocks-1.5.0-py2.py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "6437223cafed6a43c85b505ece445f92bdcb42a9894a0cd8276abf6b3dc041c8", "md5": "0143102e91f71a00abe656e1582d5213", "sha256": "40c0fab8b348777b1198ff9e39d957db325f820c939423d8c87617e6fb0d1c93"}, "downloads": -1, "filename": "translatable_xblocks-1.5.0.tar.gz", "has_sig": false, "md5_digest": "0143102e91f71a00abe656e1582d5213", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.11", "size": 50774, "upload_time": "2025-07-28T18:11:59", "upload_time_iso_8601": "2025-07-28T18:11:59.673592Z", "url": "https://files.pythonhosted.org/packages/64/37/223cafed6a43c85b505ece445f92bdcb42a9894a0cd8276abf6b3dc041c8/translatable_xblocks-1.5.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:11:55 GMT", "package": "findpeaks", "version": "2.7.4", "json": {"info": {"author": null, "author_email": "Erdogan Taskesen <erdogant@gmail.com>", "bugtrack_url": null, "classifiers": ["Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# findpeaks\r\n\r\n[![Python](https://img.shields.io/pypi/pyversions/findpeaks)](https://img.shields.io/pypi/pyversions/findpeaks)\r\n[![Pypi](https://img.shields.io/pypi/v/findpeaks)](https://pypi.org/project/findpeaks/)\r\n[![Docs](https://img.shields.io/badge/Sphinx-Docs-Green)](https://erdogant.github.io/findpeaks/)\r\n[![LOC](https://sloc.xyz/github/erdogant/findpeaks/?category=code)](https://github.com/erdogant/findpeaks/)\r\n[![Downloads](https://static.pepy.tech/personalized-badge/findpeaks?period=month&units=international_system&left_color=grey&right_color=brightgreen&left_text=PyPI%20downloads/month)](https://pepy.tech/project/findpeaks)\r\n[![Downloads](https://static.pepy.tech/personalized-badge/findpeaks?period=total&units=international_system&left_color=grey&right_color=brightgreen&left_text=Downloads)](https://pepy.tech/project/findpeaks)\r\n[![License](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/erdogant/findpeaks/blob/master/LICENSE)\r\n[![Forks](https://img.shields.io/github/forks/erdogant/findpeaks.svg)](https://github.com/erdogant/findpeaks/network)\r\n[![Issues](https://img.shields.io/github/issues/erdogant/findpeaks.svg)](https://github.com/erdogant/findpeaks/issues)\r\n[![Project Status](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\r\n[![DOI](https://zenodo.org/badge/260400472.svg)](https://zenodo.org/badge/latestdoi/260400472)\r\n[![Colab](https://colab.research.google.com/assets/colab-badge.svg?logo=github%20sponsors)](https://erdogant.github.io/findpeaks/pages/html/Documentation.html#colab-notebook)\r\n[![Medium](https://img.shields.io/badge/Medium-Blog-green)](https://erdogant.github.io/findpeaks/pages/html/Documentation.html#medium-blog)\r\n[![Donate](https://img.shields.io/badge/Support%20this%20project-grey.svg?logo=github%20sponsors)](https://erdogant.github.io/findpeaks/pages/html/Documentation.html#)\r\n\r\n<div>\r\n<a href=\"https://erdogant.github.io/findpeaks/\"><img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/logo.png\" width=\"125\" align=\"left\" /></a>\r\nfindpeaks is a comprehensive Python library for robust detection and analysis of peaks and valleys in both 1D vectors and 2D arrays (images). The library provides multiple detection algorithms including topology-based persistent homology (most robust), mask-based local maximum filtering, and traditional peakdetect approaches. It can be used for time series analysis, signal processing, image analysis, and spatial data. \u2b50\ufe0fStar it if you like it\u2b50\ufe0f\r\n</div>\r\n\r\n---\r\n\r\n### Key Features\r\n\r\n| Feature | Description |\r\n|--------|-------------|\r\n| [**Topology Detection**](https://erdogant.github.io/findpeaks/pages/html/Topology.html) | Mathematically grounded peak detection using persistent homology. |\r\n| [**Peakdetect Method**](https://erdogant.github.io/findpeaks/pages/html/Peakdetect.html) | Traditional peak detection algorithm for noisy signals. |\r\n| [**Mask Detection**](https://erdogant.github.io/findpeaks/pages/html/Mask.html) | Local maximum filtering for 2D image analysis. |\r\n| [**Caerus Method**](https://erdogant.github.io/findpeaks/pages/html/Caerus.html) | Specialized algorithm for financial time series analysis. |\r\n| [**Preprocessing**](https://erdogant.github.io/findpeaks/pages/html/Pre-processing.html) | Denoising, scaling, interpolation, and image preprocessing. |\r\n| [**Visualization**](https://erdogant.github.io/findpeaks/pages/html/Plots.html) | Rich plotting capabilities including persistence diagrams and 3D mesh plots. |\r\n\r\n---\r\n\r\n### Resources and Links\r\n- **Example Notebooks:** [Examples](https://erdogant.github.io/findpeaks/pages/html/Examples.html)\r\n- **Blog Posts:** [Medium](https://erdogant.medium.com)\r\n- **Documentation:** [Website](https://erdogant.github.io/findpeaks)\r\n- **Bug Reports and Feature Requests:** [GitHub Issues](https://github.com/erdogant/findpeaks/issues)\r\n\r\n---\r\n\r\n### Background\r\n\r\n* **Topology Method**: The most robust detection method based on persistent homology from topological data analysis. It quantifies peak significance through persistence scores and provides mathematically stable results even in noisy data.\r\n\r\n* **Peakdetect Method**: Traditional algorithm that excels at finding local maxima and minima in noisy signals without requiring extensive preprocessing. Uses a lookahead approach to distinguish between true peaks and noise-induced fluctuations.\r\n\r\n* **Mask Method**: Local maximum filtering approach specifically designed for 2D data (images). Employs 8-connected neighborhood analysis and background removal for spatial peak detection.\r\n\r\n* **Preprocessing Pipeline**: Comprehensive preprocessing capabilities including interpolation, denoising (Lee, Frost, Kuan filters), scaling, and image resizing to improve detection accuracy.\r\n\r\n---\r\n\r\n### Installation\r\n\r\n##### Install findpeaks from PyPI\r\n```bash\r\npip install findpeaks\r\n```\r\n\r\n##### Install from Github source\r\n```bash\r\npip install git+https://github.com/erdogant/findpeaks\r\n```  \r\n\r\n##### Import Library\r\n```python\r\nimport findpeaks\r\nprint(findpeaks.__version__)\r\n\r\n# Import library\r\nfrom findpeaks import findpeaks\r\n```\r\n\r\n---\r\n\r\n### Quick Start\r\n\r\n```python\r\n# Import library\r\nfrom findpeaks import findpeaks\r\n\r\n# Initialize with topology method (most robust)\r\nfp = findpeaks(method='topology')\r\n\r\n# Example data\r\nX = fp.import_example('1dpeaks')\r\n\r\n# Peak detection\r\nresults = fp.fit(X)\r\n\r\n# Plot results\r\nfp.plot()\r\n\r\n# Plot persistence diagram\r\nfp.plot_persistence()\r\n```\r\n\r\n---\r\n\r\n### Examples\r\n\r\n#### 1D Signal Analysis\r\n* [Find peaks in low sampled dataset](https://erdogant.github.io/findpeaks/pages/html/Examples.html#find-peaks-in-low-sampled-dataset)\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://erdogant.github.io/findpeaks/pages/html/Examples.html#find-peaks-in-low-sampled-dataset\">\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/fig1_raw.png\" width=\"400\" />\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/fig1_interpol.png\" width=\"400\" />  \r\n  </a>\r\n</p>\r\n\r\n* [Comparison of peak detection methods](https://erdogant.github.io/findpeaks/pages/html/Examples.html#comparison-methods-1)\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://erdogant.github.io/findpeaks/pages/html/Examples.html#comparison-methods-1\">\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/fig2_peakdetect_int.png\" width=\"400\" />  \r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/fig2_topology_int.png\" width=\"400\" />    \r\n  </a>\r\n</p>\r\n\r\n* [Find peaks in high sampled dataset](https://erdogant.github.io/findpeaks/pages/html/Examples.html#find-peaks-in-high-sampled-dataset)\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://erdogant.github.io/findpeaks/pages/html/Examples.html#find-peaks-in-high-sampled-dataset\">\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/fig3.png\" width=\"600\" />\r\n  </a>\r\n</p>\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://erdogant.github.io/findpeaks/pages/html/Examples.html#find-peaks-in-high-sampled-dataset\">\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/fig3_persistence_limit.png\" width=\"600\" />\r\n  </a>\r\n</p>\r\n\r\n#### 2D Image Analysis\r\n* [Find peaks in an image (2D-array)](https://erdogant.github.io/findpeaks/pages/html/Examples.html#d-array-image)\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://erdogant.github.io/findpeaks/pages/html/Examples.html#d-array-image\">\r\n <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/2dpeaks_raw.png\" width=\"115\" />\r\n <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/2dpeaks_mask.png\" width=\"500\" />\r\n  </a>\r\n</p>\r\n\r\n* [3D mesh visualization](https://erdogant.github.io/findpeaks/pages/html/Plots.html#d-mesh)\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://erdogant.github.io/findpeaks/pages/html/Plots.html#d-mesh\">\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/2dpeaks_mesh1.png\" width=\"400\" />\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/2dpeaks_mesh2.png\" width=\"400\" />\r\n  </a>\r\n</p>\r\n\r\n#### Financial Time Series\r\n* [Bitcoin price analysis](https://erdogant.github.io/findpeaks/pages/html/Use-cases.html#bitcoin)\r\n* [Facebook stock analysis](https://erdogant.github.io/findpeaks/pages/html/Use-cases.html#facebook-stocks)\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://erdogant.github.io/findpeaks/pages/html/Use-cases.html#facebook-stocks\">\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/fig_facebook_minperc5.png\" width=\"600\" />\r\n  </a>\r\n</p>\r\n\r\n#### SAR/SONAR Image Processing\r\n* [Peak detection in SAR/SONAR images](https://erdogant.github.io/findpeaks/pages/html/Use-cases.html#sonar)\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://erdogant.github.io/findpeaks/pages/html/Use-cases.html#sonar\">\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/sonar_plot.png\" width=\"600\" />\r\n  </a>\r\n</p>\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://erdogant.github.io/findpeaks/pages/html/Use-cases.html#sonar\">\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/sonar_mesh1.png\" width=\"300\" />\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/sonar_mesh2.png\" width=\"300\" />\r\n  </a>\r\n</p>\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://erdogant.github.io/findpeaks/pages/html/Use-cases.html#sonar\">\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/sonar_mesh3.png\" width=\"300\" />\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/sonar_mesh4.png\" width=\"300\" />\r\n  </a>\r\n</p>\r\n\r\n#### Image Denoising\r\n* [Denoising with Lee, Kuan, Fastnl, Bilateral, Frost, Mean, Median filters](https://erdogant.github.io/findpeaks/pages/html/Denoise.html#)\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://erdogant.github.io/findpeaks/pages/html/Denoise.html#\">\r\n  <img src=\"https://github.com/erdogant/findpeaks/blob/master/docs/figs/noise_distr_examples.png\" width=\"600\" />\r\n  </a>\r\n</p>\r\n\r\n<hr>\r\n\r\n### References\r\n* https://github.com/erdogant/findpeaks\r\n* https://github.com/Anaxilaus/peakdetect\r\n* https://www.sthu.org/blog/13-perstopology-peakdetection/index.html\r\n\r\n### Contributors\r\nSpecial thanks to the contributors!\r\n\r\n<p align=\"left\">\r\n  <a href=\"https://github.com/erdogant/findpeaks/graphs/contributors\">\r\n  <img src=\"https://contrib.rocks/image?repo=erdogant/findpeaks\" />\r\n  </a>\r\n</p>\r\n\r\n### Maintainer\r\n* Erdogan Taskesen, github: [erdogant](https://github.com/erdogant)\r\n* Contributions are welcome.\r\n* Yes! This library is entirely **free** but it runs on coffee! :) Feel free to support with a <a href=\"https://erdogant.github.io/donate/?currency=USD&amount=5\">Coffee</a>.\r\n\r\n[![Buy me a coffee](https://img.buymeacoffee.com/button-api/?text=Buy+me+a+coffee&emoji=&slug=erdogant&button_colour=FFDD00&font_colour=000000&font_family=Cookie&outline_colour=000000&coffee_colour=ffffff)](https://www.buymeacoffee.com/erdogant)\r\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": "Python, topology, mesh, sonar, mask, sar, topological data analysis, peak-detection, denoise images, peak analysis, speckle-noise removal", "license": "MIT License\r\n        \r\n        Copyright (c) 2020 Erdogan Taskesen\r\n        findpeaks - Python package\r\n        \r\n        Permission is hereby granted, free of charge, to any person obtaining a copy\r\n        of this software and associated documentation files (the \"Software\"), to deal\r\n        in the Software without restriction, including without limitation the rights\r\n        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n        copies of the Software, and to permit persons to whom the Software is\r\n        furnished to do so, subject to the following conditions:\r\n        \r\n        The above copyright notice and this permission notice shall be included in all\r\n        copies or substantial portions of the Software.\r\n        \r\n        THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n        SOFTWARE.\r\n        ", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "findpeaks", "package_url": "https://pypi.org/project/findpeaks/", "platform": null, "project_url": "https://pypi.org/project/findpeaks/", "project_urls": {"Download": "https://github.com/erdogant/findpeaks/archive/{version}.tar.gz", "Homepage": "https://erdogant.github.io/findpeaks"}, "provides_extra": null, "release_url": "https://pypi.org/project/findpeaks/2.7.4/", "requires_dist": ["scipy", "matplotlib", "numpy", "pandas", "tqdm", "requests", "caerus>=0.1.9", "xarray", "joblib", "adjustText"], "requires_python": ">=3", "summary": "findpeaks is for the detection of peaks and valleys in a 1D vector and 2D array (image).", "version": "2.7.4", "yanked": false, "yanked_reason": null}, "last_serial": 30388482, "urls": [{"comment_text": null, "digests": {"blake2b_256": "d586f73e0898a4046d492addd828ee07266a6c0c2349a42f4596867c77d16ee4", "md5": "4dcc6341cfa594d8105139ed9a62c8c4", "sha256": "f43e61364dcb60ffda6c6e6e46d06a6ac6ff6a2272f2d12dd40c40e38946a827"}, "downloads": -1, "filename": "findpeaks-2.7.4-py3-none-any.whl", "has_sig": false, "md5_digest": "4dcc6341cfa594d8105139ed9a62c8c4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 72648, "upload_time": "2025-07-28T18:11:55", "upload_time_iso_8601": "2025-07-28T18:11:55.753664Z", "url": "https://files.pythonhosted.org/packages/d5/86/f73e0898a4046d492addd828ee07266a6c0c2349a42f4596867c77d16ee4/findpeaks-2.7.4-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "6b18ab83cc6960ac4c885e2d8cbf9295462a391d59f370cd427cfa78122c21a4", "md5": "5f4cf37d4e75af4881def84b1ceb02a1", "sha256": "92f6afab7033eedf230dca5a5f77dc9eaf1e9f4cbb803efb01f6fe59d656de29"}, "downloads": -1, "filename": "findpeaks-2.7.4.tar.gz", "has_sig": false, "md5_digest": "5f4cf37d4e75af4881def84b1ceb02a1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 63361, "upload_time": "2025-07-28T18:11:56", "upload_time_iso_8601": "2025-07-28T18:11:56.802841Z", "url": "https://files.pythonhosted.org/packages/6b/18/ab83cc6960ac4c885e2d8cbf9295462a391d59f370cd427cfa78122c21a4/findpeaks-2.7.4.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:11:41 GMT", "package": "shandy-sqlfmt", "version": "0.27.0", "json": {"info": {"author": "Ted Conbeer", "author_email": "ted@shandy.io", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: Apache Software License", "Operating System :: MacOS :: MacOS X", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Programming Language :: SQL", "Topic :: Software Development :: Quality Assurance", "Typing :: Typed"], "description": "# sqlfmt\n\n[![PyPI](https://img.shields.io/pypi/v/shandy-sqlfmt)](https://pypi.org/project/shandy-sqlfmt/)\n[![Downloads](https://static.pepy.tech/personalized-badge/shandy-sqlfmt?period=month&units=international_system&left_color=grey&right_color=orange&left_text=downloads/mo)](https://pepy.tech/project/shandy-sqlfmt)\n[![Test](https://github.com/tconbeer/sqlfmt/actions/workflows/test.yml/badge.svg?branch=main&event=push)](https://github.com/tconbeer/sqlfmt/actions/workflows/test.yml)\n\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/shandy-sqlfmt)\n![Runs on Linux | MacOS | Windows](https://img.shields.io/badge/runs%20on-Linux%20%7C%20MacOS%20%7C%20Windows-blue)\n\n\nsqlfmt formats your dbt SQL files so you don't have to. It is similar in nature to black, gofmt, \nand rustfmt (but for SQL). \n\n1. **sqlfmt promotes collaboration.** An auto-formatter makes it easier to collaborate with your team and solicit contributions from new people. You will never have to mention (or argue about) code style in code reviews again.\n2. **sqlfmt is fast.** Forget about formatting your code, and spend your time on business logic instead. sqlfmt processes hundreds of files per second and only operates on files that have changed since the last run.\n3. **sqlfmt works with Jinja.** It formats the code that users look at, and therefore doesn't need to know anything about what happens after the templates are rendered.\n3. **sqlfmt integrates with your workflow.** As a CLI written in Python, it's easy to install locally on any OS and run in CI. Plays well with dbt, pre-commit, SQLFluff, VSCode, and GitHub Actions. sqlfmt powers the dbt Cloud IDE's Format button.\n\nsqlfmt is not configurable, except for line length. It enforces a single style. sqlfmt maintains comments and some extra newlines, but largely ignores all indentation and line breaks in the input file.\n\nsqlfmt is not a linter. It does not parse your code into an AST; it just lexes it and tracks a small subset of tokens that impact formatting. This lets us \"do one thing and do it well:\" sqlfmt is very fast, and easier to maintain and extend than linters that need a full SQL grammar.\n\nFor now, sqlfmt only works on `select`, `delete`, `grant`, `revoke`, and `create function` statements (which is all you need if you use sqlfmt with a dbt project). It is being extended to additional DDL and DML. Visit [this tracking issue](https://github.com/tconbeer/sqlfmt/issues/262) for more information.\n\n## Documentation\n\nPlease visit [docs.sqlfmt.com](https://docs.sqlfmt.com) for more information on Getting Started, Integrations, the sqlfmt Style, and an API Reference. Or keep reading for an excerpt from the full docs.\n\n### Installation\n\n#### Try it first\nWant to test out sqlfmt on a query before you install it? Go to [sqlfmt.com](https://sqlfmt.com) to use the interactive, web-based version.\n\n#### Install Using pipx (recommended)\nsqlfmt is a pip-installable Python package listed on PyPI under the name `shandy-sqlfmt`. You should install it into a virtual environment, which `pipx` does automatically:\n\n```\npipx install shandy-sqlfmt\n```\n\nTo install with the jinjafmt extra (which will also install the Python code formatter, *black*):\n\n```\npipx install shandy-sqlfmt[jinjafmt]\n```\n\nFor more installation options, [read the docs](https://docs.sqlfmt.com/getting-started/installation).\n\n### Getting Started\n\n#### Other prerequisites\n**sqlfmt will not always produce the formatted output you might want.** It might even break your SQL syntax. It is **highly recommended** to only run sqlfmt on files in a version control system (like git), so that it is easy for you to revert any changes made by sqlfmt. On your first run, be sure to make a commit before running sqlfmt.\n\nThere are certain situations where sqlfmt can be considered to be in Beta, or even more mature than that. Those are:\n\n1. Using sqlfmt to format select statements for one of the major dialects (PostgresSQL, MySQL, Snowflake, BQ, Redshift).\n\n1. Using sqlfmt to format a dbt project (which may also include jinja and some minimal DDL/DML, like grants, create function, etc.) for one of the major dialects.\n\nHowever, there are other use cases where sqlfmt is very much alpha:\n\n1. Formatting some dialects that deviate from ANSI or Postgres, like T-SQL (SQLServer).\n\n1. Formatting other DDL (create table, insert, etc.) (sqlfmt attempts to be no-op on these statements as much as possible).\n\nIn these domains sqlfmt is nowhere near \"feature complete\" and caution is highly advised.\n\n#### Using sqlfmt\nTo list commands and options:\n\n```bash\nsqlfmt --help\n```\n\nIf you want to format all `.sql` and `.sql.jinja` files in your current working directory (and all nested directories), simply type:\n```bash\n$ sqlfmt .\n```\n\nIf you don't want to format the files you have on disk, you can run sqlfmt with the `--check` option. sqlfmt will exit with code 1 if the files on disk are not properly formatted:\n```bash\n$ sqlfmt --check .\n```\nIf you want to print a diff of changes that sqlfmt would make to format a file (but not update the file on disk), you can use the `--diff` option. `--diff` also exits with 1 on changes:\n```bash\n$ sqlfmt --diff .\n```\n\nFor more commands, see [the docs](https://docs.sqlfmt.com/getting-started/using-sqlfmt).\n\n#### Configuring sqlfmt using pyproject.toml\n\nAny command-line option for sqlfmt can also be set in a `pyproject.toml` file, under a `[tool.sqlfmt]` section header. Options passed at the command line will override the settings in the config file. [See the docs](https://docs.sqlfmt.com/getting-started/configuring-sqlfmt) for more information.\n\n#### The jinjafmt extra\n\nsqlfmt loves properly-formatted jinja, too.\n\n[See the docs](https://docs.sqlfmt.com/getting-started/formatting-jinja) for more information about using the `jinjafmt` extra or disabling jinja formatting.\n\n### Using sqlfmt with different SQL dialects\n\nsqlfmt's rules are simple, which means it does not have to parse every single token in your query. This allows nearly all SQL dialects to be formatted using sqlfmt's default \"polyglot\" dialect, which requires no configuration.\n\nThe exception to this is [ClickHouse](https://docs.sqlfmt.com/dialects/#clickhouse), which is case-sensitive where other dialects are not. To prevent the lowercasing of function names, database identifiers, and aliases, use the `--dialect clickhouse` option when running sqlfmt. For example,\n\n```bash\n$ sqlfmt . --dialect clickhouse\n```\n\nThis can also be configured using the `pyproject.toml` file:\n\n```toml\n[tool.sqlfmt]\ndialect = \"clickhouse\"\n```\n\nNote that with this option, sqlfmt will not lowercase **most** non-reserved keywords, even common ones like `sum` or `count`. See (and please join) [this discussion](https://github.com/tconbeer/sqlfmt/discussions/229) for more on this topic.\n\n### Integrations\n\nsqlfmt plays nicely with other analytics engineering tools. For more information, [see the docs](https://docs.sqlfmt.com/category/integrations).\n\n#### dbt\n\nsqlfmt was built for dbt, so only [minimal configuration](https://docs.sqlfmt.com/integrations/dbt) is required. We recommend excluding your `target` and `dbt_packages` directories from formatting. You can do this with the command-line `--exclude` option, or by setting `exclude` in your `pyproject.toml` file:\n\n```toml\n[tool.sqlfmt]\nexclude=[\"target/**/*\", \"dbt_packages/**/*\"]\n```\n\n#### Other Integrations\n\nConfig for other integrations is detailed in the docs linked below:\n\n- [pre-commit](https://docs.sqlfmt.com/integrations/pre-commit)\n- [SQLFluff](https://docs.sqlfmt.com/integrations/sqlfluff)\n- [VSCode](https://docs.sqlfmt.com/integrations/vs-code)\n\n\n## The sqlfmt style\nThe only thing you can configure with sqlfmt is the desired line length of the formatted file. You can do this with the `--line-length` or `-l` options. The default is 88.\n\nsqlfmt borrows elements from well-accepted styles from other programming languages. It places opening brackets on the same line as preceding function names (like *black* for python and *1TBS* for C). It indents closing brackets to the same depth as the opening bracket (this is extended to statements that must be closed, like `case` and `end`).\n\nThe sqlfmt style is as simple as possible, with little-to-no special-casing of formatting concerns. While at first blush, this may not create a format that is as \"nice\" or \"expressive\" as hand-crafted indentation, over time, as you grow accustomed to the style, formatting becomes transparent and the consistency will allow you to jump between files, projects, and even companies much faster.\n\n[Read More](https://docs.sqlfmt.com/style/)\n\n### Why lowercase?\nBecause SQL is code! But there are [other good reasons too](https://docs.sqlfmt.com/style/#why-lowercase).\n\n### Why trailing commas?\nUsing trailing commas follows the convention of every other written language and programming language. [But wait, there's more.](https://docs.sqlfmt.com/style/#why-trailing-commas)\n\n## Contributing\n\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Checked with mypy](http://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)\n[![Maintainability](https://api.codeclimate.com/v1/badges/8928f6662a67b8eaf092/maintainability)](https://codeclimate.com/github/tconbeer/sqlfmt/maintainability)\n[![Test Coverage](https://api.codeclimate.com/v1/badges/8928f6662a67b8eaf092/test_coverage)](https://codeclimate.com/github/tconbeer/sqlfmt/test_coverage)\n\n### Providing Feedback\n\nWe'd love to hear from you! [Open an Issue](https://github.com/tconbeer/sqlfmt/issues/new/choose) to request new features, report bad formatting, or say hello.\n\n### Setting up Your Dev Environment and Running Tests\n\n1. Install [Poetry](https://python-poetry.org/docs/#installation) v1.2 or higher if you don't have it already. You may also need or want pyenv, make, and gcc. A complete setup from a fresh install of Ubuntu can be found [here](https://github.com/tconbeer/linux_setup).\n1. Clone this repo into a directory (let's call it `sqlfmt`), then `cd sqlfmt`.\n1. Use `poetry install --all-extras --sync` to install the project (editable) and its dependencies (including the `jinjafmt` and `sqlfmt_primer` extras) into a new virtual env.\n1. Use `poetry shell` to spawn a subshell.\n1. Type `make` to run all tests and linters, or run `pytest`, `black`, `flake8`, `isort`, and `mypy` individually.\n\n### Updating primer repos to reflect formatting changes\n\n1. Make sure all changes are committed to sqlfmt.\n1. Check out `main` in the repo and make sure you `pull` changes locally.\n1. Check out the `unformatted` tag in the repo with `git checkout -b chore/apply-abc123 unformatted` where `abc123` is the hash of the most recent sqlfmt commit (from 1).\n1. Run sqlfmt against the working tree, then `git add .` and `git commit -m \"chore: apply sqlfmt abc123\"`.\n1. We will have conflicts with main that we want to ignore, so merge main into this branch, ignoring anything on main: `git merge -s ours main`.\n1. Push and open a PR; squash and merge. Grab the commit SHA.\n1. Paste the commit SHA as a ref into `primer.py`.\n1. Run `sqlfmt_primer -k` to clear the cache, then update the stats in `primer.py` to match the results.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://sqlfmt.com", "keywords": null, "license": "Apache-2.0", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "shandy-sqlfmt", "package_url": "https://pypi.org/project/shandy-sqlfmt/", "platform": null, "project_url": "https://pypi.org/project/shandy-sqlfmt/", "project_urls": {"Documentation": "https://docs.sqlfmt.com", "Homepage": "https://sqlfmt.com", "Repository": "https://github.com/tconbeer/sqlfmt"}, "provides_extra": ["jinjafmt", "sqlfmt-primer"], "release_url": "https://pypi.org/project/shandy-sqlfmt/0.27.0/", "requires_dist": ["black; extra == \"jinjafmt\"", "click<9.0,>=8.0", "gitpython<4.0.0,>=3.1.24; extra == \"sqlfmt-primer\"", "importlib_metadata; python_version < \"3.8\"", "jinja2<4.0,>=3.0", "platformdirs<5.0,>=2.4", "tomli<3.0,>=2.0; python_version < \"3.11\"", "tqdm<5.0,>=4.0"], "requires_python": "<4.0,>=3.8", "summary": "sqlfmt formats your dbt SQL files so you don't have to.", "version": "0.27.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388479, "urls": [{"comment_text": null, "digests": {"blake2b_256": "e6b7c63d537a502dab52ba24a81f29608641950ab0efe2a3b552729b0c30ee1a", "md5": "78979a49aad03970871829ef0706d1ae", "sha256": "84359f5e4b8754852019e0b7dde6ba438594adbdfdc0131bc17cb5b51f88b72e"}, "downloads": -1, "filename": "shandy_sqlfmt-0.27.0-py3-none-any.whl", "has_sig": false, "md5_digest": "78979a49aad03970871829ef0706d1ae", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<4.0,>=3.8", "size": 71115, "upload_time": "2025-07-28T18:11:41", "upload_time_iso_8601": "2025-07-28T18:11:41.535674Z", "url": "https://files.pythonhosted.org/packages/e6/b7/c63d537a502dab52ba24a81f29608641950ab0efe2a3b552729b0c30ee1a/shandy_sqlfmt-0.27.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "b388db0e588e7de42eb56f4052f6392011d09748396669ae2eab815487035f52", "md5": "49e7e3f7182e05f2ad9b31948d610162", "sha256": "1c9ac6beb5f17f66e7b87b4c70be586e87db538cd7d72f42dedb85132e8b3a96"}, "downloads": -1, "filename": "shandy_sqlfmt-0.27.0.tar.gz", "has_sig": false, "md5_digest": "49e7e3f7182e05f2ad9b31948d610162", "packagetype": "sdist", "python_version": "source", "requires_python": "<4.0,>=3.8", "size": 62380, "upload_time": "2025-07-28T18:11:42", "upload_time_iso_8601": "2025-07-28T18:11:42.774086Z", "url": "https://files.pythonhosted.org/packages/b3/88/db0e588e7de42eb56f4052f6392011d09748396669ae2eab815487035f52/shandy_sqlfmt-0.27.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:11:39 GMT", "package": "cua-agent", "version": "0.4.0", "json": {"info": {"author": null, "author_email": "TryCua <gh@trycua.com>", "bugtrack_url": null, "classifiers": [], "description": "<div align=\"center\">\n<h1>\n  <div class=\"image-wrapper\" style=\"display: inline-block;\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" alt=\"logo\" height=\"150\" srcset=\"../../../img/logo_white.png\" style=\"display: block; margin: auto;\">\n      <source media=\"(prefers-color-scheme: light)\" alt=\"logo\" height=\"150\" srcset=\"../../../img/logo_black.png\" style=\"display: block; margin: auto;\">\n      <img alt=\"Shows my svg\">\n    </picture>\n  </div>\n\n  [![Python](https://img.shields.io/badge/Python-333333?logo=python&logoColor=white&labelColor=333333)](#)\n  [![macOS](https://img.shields.io/badge/macOS-000000?logo=apple&logoColor=F0F0F0)](#)\n  [![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&logo=discord&logoColor=white)](https://discord.com/invite/mVnXXpdE85)\n  [![PyPI](https://img.shields.io/pypi/v/cua-computer?color=333333)](https://pypi.org/project/cua-computer/)\n</h1>\n</div>\n\n**cua-agent** is a general Computer-Use framework with liteLLM integration for running agentic workflows on macOS, Windows, and Linux sandboxes. It provides a unified interface for computer-use agents across multiple LLM providers with advanced callback system for extensibility.\n\n## Features\n\n- **Safe Computer-Use/Tool-Use**: Using Computer SDK for sandboxed desktops\n- **Multi-Agent Support**: Anthropic Claude, OpenAI computer-use-preview, UI-TARS, Omniparser + any LLM\n- **Multi-API Support**: Take advantage of liteLLM supporting 100+ LLMs / model APIs, including local models (`huggingface-local/`, `ollama_chat/`, `mlx/`)\n- **Cross-Platform**: Works on Windows, macOS, and Linux with cloud and local computer instances\n- **Extensible Callbacks**: Built-in support for image retention, cache control, PII anonymization, budget limits, and trajectory tracking\n\n## Install\n\n```bash\npip install \"cua-agent[all]\"\n\n# or install specific providers\npip install \"cua-agent[openai]\"        # OpenAI computer-use-preview support\npip install \"cua-agent[anthropic]\"     # Anthropic Claude support\npip install \"cua-agent[omni]\"          # Omniparser + any LLM support\npip install \"cua-agent[uitars]\"        # UI-TARS\npip install \"cua-agent[uitars-mlx]\"    # UI-TARS + MLX support\npip install \"cua-agent[uitars-hf]\"     # UI-TARS + Huggingface support\npip install \"cua-agent[ui]\"            # Gradio UI support\n```\n\n## Quick Start\n\n```python\nimport asyncio\nimport os\nfrom agent import ComputerAgent\nfrom computer import Computer\n\nasync def main():\n    # Set up computer instance\n    async with Computer(\n        os_type=\"linux\",\n        provider_type=\"cloud\",\n        name=os.getenv(\"CUA_CONTAINER_NAME\"),\n        api_key=os.getenv(\"CUA_API_KEY\")\n    ) as computer:\n        \n        # Create agent\n        agent = ComputerAgent(\n            model=\"anthropic/claude-3-5-sonnet-20241022\",\n            tools=[computer],\n            only_n_most_recent_images=3,\n            trajectory_dir=\"trajectories\",\n            max_trajectory_budget=5.0  # $5 budget limit\n        )\n        \n        # Run agent\n        messages = [{\"role\": \"user\", \"content\": \"Take a screenshot and tell me what you see\"}]\n        \n        async for result in agent.run(messages):\n            for item in result[\"output\"]:\n                if item[\"type\"] == \"message\":\n                    print(item[\"content\"][0][\"text\"])\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## Supported Models\n\n### Anthropic Claude (Computer Use API)\n```python\nmodel=\"anthropic/claude-3-5-sonnet-20241022\"\nmodel=\"anthropic/claude-3-5-sonnet-20240620\"\nmodel=\"anthropic/claude-opus-4-20250514\"\nmodel=\"anthropic/claude-sonnet-4-20250514\"\n```\n\n### OpenAI Computer Use Preview\n```python\nmodel=\"openai/computer-use-preview\"\n```\n\n### UI-TARS (Local or Huggingface Inference)\n```python\nmodel=\"huggingface-local/ByteDance-Seed/UI-TARS-1.5-7B\"\nmodel=\"ollama_chat/0000/ui-tars-1.5-7b\"\n```\n\n### Omniparser + Any LLM\n```python\nmodel=\"omniparser+ollama_chat/mistral-small3.2\"\nmodel=\"omniparser+vertex_ai/gemini-pro\"\nmodel=\"omniparser+anthropic/claude-3-5-sonnet-20241022\"\nmodel=\"omniparser+openai/gpt-4o\"\n```\n\n## Custom Tools\n\nDefine custom tools using decorated functions:\n\n```python\nfrom computer.helpers import sandboxed\n\n@sandboxed()\ndef read_file(location: str) -> str:\n    \"\"\"Read contents of a file\n    \n    Parameters\n    ----------\n    location : str\n        Path to the file to read\n        \n    Returns\n    -------\n    str\n        Contents of the file or error message\n    \"\"\"\n    try:\n        with open(location, 'r') as f:\n            return f.read()\n    except Exception as e:\n        return f\"Error reading file: {str(e)}\"\n\ndef calculate(a: int, b: int) -> int:\n    \"\"\"Calculate the sum of two integers\"\"\"\n    return a + b\n\n# Use with agent\nagent = ComputerAgent(\n    model=\"anthropic/claude-3-5-sonnet-20241022\",\n    tools=[computer, read_file, calculate]\n)\n```\n\n## Callbacks System\n\nagent provides a comprehensive callback system for extending functionality:\n\n### Built-in Callbacks\n\n```python\nfrom agent.callbacks import (\n    ImageRetentionCallback,\n    TrajectorySaverCallback, \n    BudgetManagerCallback,\n    LoggingCallback\n)\n\nagent = ComputerAgent(\n    model=\"anthropic/claude-3-5-sonnet-20241022\",\n    tools=[computer],\n    callbacks=[\n        ImageRetentionCallback(only_n_most_recent_images=3),\n        TrajectorySaverCallback(trajectory_dir=\"trajectories\"),\n        BudgetManagerCallback(max_budget=10.0, raise_error=True),\n        LoggingCallback(level=logging.INFO)\n    ]\n)\n```\n\n### Custom Callbacks\n\n```python\nfrom agent.callbacks.base import AsyncCallbackHandler\n\nclass CustomCallback(AsyncCallbackHandler):\n    async def on_llm_start(self, messages):\n        \"\"\"Preprocess messages before LLM call\"\"\"\n        # Add custom preprocessing logic\n        return messages\n    \n    async def on_llm_end(self, messages):\n        \"\"\"Postprocess messages after LLM call\"\"\"\n        # Add custom postprocessing logic\n        return messages\n    \n    async def on_usage(self, usage):\n        \"\"\"Track usage information\"\"\"\n        print(f\"Tokens used: {usage.total_tokens}\")\n```\n\n## Budget Management\n\nControl costs with built-in budget management:\n\n```python\n# Simple budget limit\nagent = ComputerAgent(\n    model=\"anthropic/claude-3-5-sonnet-20241022\",\n    max_trajectory_budget=5.0  # $5 limit\n)\n\n# Advanced budget configuration\nagent = ComputerAgent(\n    model=\"anthropic/claude-3-5-sonnet-20241022\",\n    max_trajectory_budget={\n        \"max_budget\": 10.0,\n        \"raise_error\": True,  # Raise error when exceeded\n        \"reset_after_each_run\": False  # Persistent across runs\n    }\n)\n```\n\n## Trajectory Management\n\nSave and replay agent conversations:\n\n```python\nagent = ComputerAgent(\n    model=\"anthropic/claude-3-5-sonnet-20241022\",\n    trajectory_dir=\"trajectories\",  # Auto-save trajectories\n    tools=[computer]\n)\n\n# Trajectories are saved with:\n# - Complete conversation history\n# - Usage statistics and costs\n# - Timestamps and metadata\n# - Screenshots and computer actions\n```\n\n## Configuration Options\n\n### ComputerAgent Parameters\n\n- `model`: Model identifier (required)\n- `tools`: List of computer objects and decorated functions\n- `callbacks`: List of callback handlers for extensibility\n- `only_n_most_recent_images`: Limit recent images to prevent context overflow\n- `verbosity`: Logging level (logging.INFO, logging.DEBUG, etc.)\n- `trajectory_dir`: Directory to save conversation trajectories\n- `max_retries`: Maximum API call retries (default: 3)\n- `screenshot_delay`: Delay between actions and screenshots (default: 0.5s)\n- `use_prompt_caching`: Enable prompt caching for supported models\n- `max_trajectory_budget`: Budget limit configuration\n\n### Environment Variables\n\n```bash\n# Computer instance (cloud)\nexport CUA_CONTAINER_NAME=\"your-container-name\"\nexport CUA_API_KEY=\"your-cua-api-key\"\n\n# LLM API keys\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\nexport OPENAI_API_KEY=\"your-openai-key\"\n```\n\n## Advanced Usage\n\n### Streaming Responses\n\n```python\nasync for result in agent.run(messages, stream=True):\n    # Process streaming chunks\n    for item in result[\"output\"]:\n        if item[\"type\"] == \"message\":\n            print(item[\"content\"][0][\"text\"], end=\"\", flush=True)\n        elif item[\"type\"] == \"computer_call\":\n            action = item[\"action\"]\n            print(f\"\\n[Action: {action['type']}]\")\n```\n\n### Interactive Chat Loop\n\n```python\nhistory = []\nwhile True:\n    user_input = input(\"> \")\n    if user_input.lower() in ['quit', 'exit']:\n        break\n        \n    history.append({\"role\": \"user\", \"content\": user_input})\n    \n    async for result in agent.run(history):\n        history += result[\"output\"]\n        \n        # Display assistant responses\n        for item in result[\"output\"]:\n            if item[\"type\"] == \"message\":\n                print(item[\"content\"][0][\"text\"])\n```\n\n### Error Handling\n\n```python\ntry:\n    async for result in agent.run(messages):\n        # Process results\n        pass\nexcept BudgetExceededException:\n    print(\"Budget limit exceeded\")\nexcept Exception as e:\n    print(f\"Agent error: {e}\")\n```\n\n## API Reference\n\n### ComputerAgent.run()\n\n```python\nasync def run(\n    self,\n    messages: Messages,\n    stream: bool = False,\n    **kwargs\n) -> AsyncGenerator[Dict[str, Any], None]:\n    \"\"\"\n    Run the agent with the given messages.\n    \n    Args:\n        messages: List of message dictionaries\n        stream: Whether to stream the response\n        **kwargs: Additional arguments\n        \n    Returns:\n        AsyncGenerator that yields response chunks\n    \"\"\"\n```\n\n### Message Format\n\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Take a screenshot and describe what you see\"\n    },\n    {\n        \"role\": \"assistant\", \n        \"content\": \"I'll take a screenshot for you.\"\n    }\n]\n```\n\n### Response Format\n\n```python\n{\n    \"output\": [\n        {\n            \"type\": \"message\",\n            \"role\": \"assistant\",\n            \"content\": [{\"type\": \"output_text\", \"text\": \"I can see...\"}]\n        },\n        {\n            \"type\": \"computer_call\",\n            \"action\": {\"type\": \"screenshot\"},\n            \"call_id\": \"call_123\"\n        },\n        {\n            \"type\": \"computer_call_output\",\n            \"call_id\": \"call_123\",\n            \"output\": {\"image_url\": \"data:image/png;base64,...\"}\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 150,\n        \"completion_tokens\": 75,\n        \"total_tokens\": 225,\n        \"response_cost\": 0.01,\n    }\n}\n```\n\n## License\n\nMIT License - see LICENSE file for details.", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "cua-agent", "package_url": "https://pypi.org/project/cua-agent/", "platform": null, "project_url": "https://pypi.org/project/cua-agent/", "project_urls": null, "provides_extra": ["openai", "anthropic", "omni", "uitars", "uitars-mlx", "uitars-hf", "ui", "cli", "all"], "release_url": "https://pypi.org/project/cua-agent/0.4.0/", "requires_dist": ["httpx>=0.27.0", "aiohttp>=3.9.3", "asyncio", "anyio>=4.4.1", "typing-extensions>=4.12.2", "pydantic>=2.6.4", "rich>=13.7.1", "python-dotenv>=1.0.1", "cua-computer<0.5.0,>=0.3.0", "cua-core<0.2.0,>=0.1.0", "certifi>=2024.2.2", "litellm>=1.74.8", "ultralytics>=8.0.0; extra == \"omni\"", "cua-som<0.2.0,>=0.1.0; extra == \"omni\"", "mlx-vlm>=0.1.27; sys_platform == \"darwin\" and extra == \"uitars-mlx\"", "transformers>=4.54.0; extra == \"uitars-hf\"", "gradio>=5.23.3; extra == \"ui\"", "python-dotenv>=1.0.1; extra == \"ui\"", "yaspin>=3.1.0; extra == \"cli\"", "ultralytics>=8.0.0; extra == \"all\"", "cua-som<0.2.0,>=0.1.0; extra == \"all\"", "mlx-vlm>=0.1.27; sys_platform == \"darwin\" and extra == \"all\"", "transformers>=4.54.0; extra == \"all\"", "gradio>=5.23.3; extra == \"all\"", "python-dotenv>=1.0.1; extra == \"all\"", "yaspin>=3.1.0; extra == \"all\""], "requires_python": ">=3.11", "summary": "CUA (Computer Use) Agent for AI-driven computer interaction", "version": "0.4.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388476, "urls": [{"comment_text": null, "digests": {"blake2b_256": "0292f34dfa15b5364da07ed5996934c1282a2ae67b559b08787c4dc7ead1f878", "md5": "a5ef800a8d002fe24033afb57782f895", "sha256": "bd12881cf0755b5a1455d65b874f8449284515ce5459bc9f3421830af6be56da"}, "downloads": -1, "filename": "cua_agent-0.4.0-py3-none-any.whl", "has_sig": false, "md5_digest": "a5ef800a8d002fe24033afb57782f895", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.11", "size": 67198, "upload_time": "2025-07-28T18:11:39", "upload_time_iso_8601": "2025-07-28T18:11:39.349176Z", "url": "https://files.pythonhosted.org/packages/02/92/f34dfa15b5364da07ed5996934c1282a2ae67b559b08787c4dc7ead1f878/cua_agent-0.4.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "fe90a16bc020dfda98273040eb7d5727a936a00bf8d3177ab1780ea21cbe4575", "md5": "a0f8071322586ceb4324ba679782d6e1", "sha256": "e68bdc0cc9807bf64103d4e016a97efc9bcff8b2be8df10d8ea2fb7cd6a63766"}, "downloads": -1, "filename": "cua_agent-0.4.0.tar.gz", "has_sig": false, "md5_digest": "a0f8071322586ceb4324ba679782d6e1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.11", "size": 57160, "upload_time": "2025-07-28T18:11:40", "upload_time_iso_8601": "2025-07-28T18:11:40.809310Z", "url": "https://files.pythonhosted.org/packages/fe/90/a16bc020dfda98273040eb7d5727a936a00bf8d3177ab1780ea21cbe4575/cua_agent-0.4.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:11:29 GMT", "package": "pageai-sdk", "version": "0.6.0.168", "json": {"info": {"author": "OpenAPI Generator community", "author_email": "team@openapitools.org", "bugtrack_url": null, "classifiers": [], "description": "    # Introduction The PageAI (short for Synthetic EPUB) API is capapble of transforming multi page image only PDF files into accessible EPUBs.   # noqa: E501\n    \n", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "OpenAPI, OpenAPI-Generator, PageAI API", "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "pageai-sdk", "package_url": "https://pypi.org/project/pageai-sdk/", "platform": null, "project_url": "https://pypi.org/project/pageai-sdk/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/pageai-sdk/0.6.0.168/", "requires_dist": ["urllib3>=1.15", "six>=1.10", "certifi", "python-dateutil"], "requires_python": null, "summary": "PageAI API", "version": "0.6.0.168", "yanked": false, "yanked_reason": null}, "last_serial": 30388472, "urls": [{"comment_text": null, "digests": {"blake2b_256": "d4d1e357de5bbb2c5d37df37ba58651f6f13d5549577715347f2bc9af8021081", "md5": "81e892191bbddafecbd0d11d5251bf3b", "sha256": "9765629fc8a805949699cf526e3ce746b3d67f1472faef464e1b0ed5f5baf9c1"}, "downloads": -1, "filename": "pageai_sdk-0.6.0.168-py3-none-any.whl", "has_sig": false, "md5_digest": "81e892191bbddafecbd0d11d5251bf3b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 20194, "upload_time": "2025-07-28T18:11:29", "upload_time_iso_8601": "2025-07-28T18:11:29.395080Z", "url": "https://files.pythonhosted.org/packages/d4/d1/e357de5bbb2c5d37df37ba58651f6f13d5549577715347f2bc9af8021081/pageai_sdk-0.6.0.168-py3-none-any.whl", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:11:28 GMT", "package": "arthur-client", "version": "1.4.1231", "json": {"info": {"author": "Arthur", "author_email": "info@arthur.ai", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Topic :: Software Development :: Libraries :: Application Frameworks"], "description": "[//]: # (user-facing readme)\n\n# Arthur API Client\n[Arthur](https://www.arthur.ai/) is the platform for centralized monitoring of production models. \nWe help data scientists, product owners, and business leaders accelerate model operations to optimize \nfor accuracy, explainability, and fairness. As a model- and infrastructure-agnostic platform, Arthur \nadds a layer of intelligence to your AI stack and scales with your deployments.\n\nOur API Client makes it easy to integrate your models with the Arthur platform. For help getting started or \nusing the SDK, check out [our documentation](https://docs.arthur.ai/).\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "api arthur client ArthurAI sdk ml model monitoring", "license": "MIT", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "arthur-client", "package_url": "https://pypi.org/project/arthur-client/", "platform": null, "project_url": "https://pypi.org/project/arthur-client/", "project_urls": {"Arthur Homepage": "https://arthur.ai", "Documentation": "https://docs.arthur.ai"}, "provides_extra": null, "release_url": "https://pypi.org/project/arthur-client/1.4.1231/", "requires_dist": ["urllib3<3.0.0,>=2.5.0", "python-dateutil<3.0.0,>=2.9.0.post0", "pydantic>=2", "typing-extensions>=4.7.1", "simple-settings>=1.2.0", "authlib<2,>=1.3.2", "click<9,>=8.1", "requests<3,>=2"], "requires_python": "<4.0,>=3.12", "summary": "Arthur Python API Client Library", "version": "1.4.1231", "yanked": false, "yanked_reason": null}, "last_serial": 30388473, "urls": [{"comment_text": "", "digests": {"blake2b_256": "52df58296fe39608f26f4eb79699dbbfb65e4253e9dac49b7518744474d44a2e", "md5": "650cb2c2a3caa7d63c7c755079b97c8a", "sha256": "ef6059289e44421571db7d72471fcffdd8fd079c70d0150d8d8203930b0733a8"}, "downloads": -1, "filename": "arthur_client-1.4.1231-py3-none-any.whl", "has_sig": false, "md5_digest": "650cb2c2a3caa7d63c7c755079b97c8a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<4.0,>=3.12", "size": 1013893, "upload_time": "2025-07-28T18:11:28", "upload_time_iso_8601": "2025-07-28T18:11:28.180848Z", "url": "https://files.pythonhosted.org/packages/52/df/58296fe39608f26f4eb79699dbbfb65e4253e9dac49b7518744474d44a2e/arthur_client-1.4.1231-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "7d2f158dd4e817d085d32efd286d30be03270b3e2a27333d93c3cb83c27dc428", "md5": "0fde5d8bb47f74617d7f4a238f2e5efc", "sha256": "f3987ca7c10802447c8875af9ba04722e38cf369302bf3b6961ea0576d2ad916"}, "downloads": -1, "filename": "arthur_client-1.4.1231.tar.gz", "has_sig": false, "md5_digest": "0fde5d8bb47f74617d7f4a238f2e5efc", "packagetype": "sdist", "python_version": "source", "requires_python": "<4.0,>=3.12", "size": 271069, "upload_time": "2025-07-28T18:11:29", "upload_time_iso_8601": "2025-07-28T18:11:29.670584Z", "url": "https://files.pythonhosted.org/packages/7d/2f/158dd4e817d085d32efd286d30be03270b3e2a27333d93c3cb83c27dc428/arthur_client-1.4.1231.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:11:22 GMT", "package": "voitta", "version": "0.28.0", "json": {"info": {"author": "Voitta", "author_email": "support@voitta.ai", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# Voitta\n\n[![PyPI version](https://img.shields.io/pypi/v/voitta.svg)](https://pypi.org/project/voitta/)\n[![Python Versions](https://img.shields.io/pypi/pyversions/voitta.svg)](https://pypi.org/project/voitta/)\n[![Downloads](https://static.pepy.tech/badge/voitta/month)](https://pepy.tech/project/voitta)\n[![License](https://img.shields.io/github/license/voitta-ai/voitta)](https://github.com/voitta-ai/voitta/blob/main/LICENSE)\n[![GitHub Stars](https://img.shields.io/github/stars/voitta-ai/voitta.svg)](https://github.com/voitta-ai/voitta/stargazers)\n\n<!-- These badges will appear on both GitHub and PyPI pages since this README is used as the long description for PyPI -->\n\nA Python framework for routing, automating, and orchestrating LLM tool calls. Voitta simplifies the integration of AI agents with external tools and APIs, enabling more powerful and flexible AI applications.\n\n## Features\n\n- **Tool Call Routing**: Seamlessly route LLM tool calls to the appropriate handlers\n- **Flexible Configuration**: Define your tools and routing logic using YAML or Python\n- **Framework Agnostic**: Works with any LLM provider or framework\n- **Extensible Architecture**: Easily add custom tools and integrations\n- **Observability**: Monitor and debug tool calls with built-in logging\n\n## Installation\n\n### From PyPI (Stable Release)\n\n```bash\npip install voitta\n```\n\n### From TestPyPI (Pre-release Versions)\n\nTo install the latest pre-release version from TestPyPI:\n\n```bash\npip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ voitta\n```\n\nThe `--extra-index-url` flag is needed to fetch dependencies from the main PyPI repository, as TestPyPI may not have all the required dependencies.\n\nYou can also specify a particular version:\n\n```bash\npip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ voitta==0.2.3\n```\n\nNote: Pre-release versions on TestPyPI may contain experimental features and bugs. Use in production environments at your own risk.\n\n## Quick Start\n\n```python\nfrom voitta import Voitta\n\n# Initialize Voitta\nvoitta = Voitta()\n\n# Register a tool handler\n@voitta.tool(\"get_weather\")\ndef get_weather(location, unit=\"celsius\"):\n    # Implementation to fetch weather data\n    return {\"temperature\": 22, \"condition\": \"sunny\", \"location\": location, \"unit\": unit}\n\n# Process an LLM tool call\nresult = voitta.process_tool_call({\n    \"name\": \"get_weather\",\n    \"arguments\": {\"location\": \"San Francisco\", \"unit\": \"fahrenheit\"}\n})\n\nprint(result)  # Output: {\"temperature\": 72, \"condition\": \"sunny\", \"location\": \"San Francisco\", \"unit\": \"fahrenheit\"}\n```\n\n## Usage\n\nFor detailed usage examples and documentation, please refer to:\n- [Voitta Example Repository](https://github.com/voitta-ai/voitta-example)\n- [Voitta Official Website](https://voitta.com)\n- [Voitta on PyPI](https://pypi.org/project/voitta/)\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "License-File", "Project-Url", "Requires-Dist", "Summary"], "home_page": "https://github.com/voitta-ai/voitta", "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "voitta", "package_url": "https://pypi.org/project/voitta/", "platform": null, "project_url": "https://pypi.org/project/voitta/", "project_urls": {"Bug Reports": "https://github.com/voitta-ai/voitta/issues", "Changelog": "https://github.com/voitta-ai/voitta/blob/master/CHANGELOG.md", "Documentation": "https://voitta.com", "Examples": "https://github.com/voitta-ai/voitta-example", "Homepage": "https://github.com/voitta-ai/voitta", "Source Code": "https://github.com/voitta-ai/voitta", "Website": "https://voitta.ai"}, "provides_extra": null, "release_url": "https://pypi.org/project/voitta/0.28.0/", "requires_dist": ["dspy>=2.6.16", "fastapi>=0.95.0", "uvicorn>=0.21.1", "python-dotenv>=1.0.0", "pydantic>=1.10.7", "requests", "pandas", "setuptools", "pyyaml", "jsonpath_ng", "httpx", "asgiref", "uuid", "pyjwt"], "requires_python": null, "summary": "A Python framework for LLM tool calls routing, automation, and orchestration", "version": "0.28.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388468, "urls": [{"comment_text": null, "digests": {"blake2b_256": "fad075c382e976e021e9382dfc97b15085bb9eff59a6d4d47d463acf5aaefe5b", "md5": "cefc35a66da2501693a9092685363380", "sha256": "f75cc1ebff43a82bd34caa77353f039cc6713762315b379f364be6942ceae926"}, "downloads": -1, "filename": "voitta-0.28.0-py3-none-any.whl", "has_sig": false, "md5_digest": "cefc35a66da2501693a9092685363380", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 15971, "upload_time": "2025-07-28T18:11:22", "upload_time_iso_8601": "2025-07-28T18:11:22.941224Z", "url": "https://files.pythonhosted.org/packages/fa/d0/75c382e976e021e9382dfc97b15085bb9eff59a6d4d47d463acf5aaefe5b/voitta-0.28.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "afa40b531dfbfadc1a07758eb7b91d697395e61bf9a7eeb1921cc6ff01bcfbe6", "md5": "de98a95ebf820ac207522d446b8e8772", "sha256": "2a51c583f6ffd4412e6b31b43ba1bc8382e22ade66f8a4549d964790a05b4dc6"}, "downloads": -1, "filename": "voitta-0.28.0.tar.gz", "has_sig": false, "md5_digest": "de98a95ebf820ac207522d446b8e8772", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16677, "upload_time": "2025-07-28T18:11:23", "upload_time_iso_8601": "2025-07-28T18:11:23.913110Z", "url": "https://files.pythonhosted.org/packages/af/a4/0b531dfbfadc1a07758eb7b91d697395e61bf9a7eeb1921cc6ff01bcfbe6/voitta-0.28.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:11:14 GMT", "package": "radex-booking", "version": "2025.7.28a1", "json": {"info": {"author": null, "author_email": "Ernesto Santana <nesti.s.dev@gmail.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Internet :: WWW/HTTP :: Dynamic Content", "Topic :: Software Development :: Libraries"], "description": "# Python API client for radex booking service\r\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File", "Requires-Dist"], "home_page": null, "keywords": "radex, booking", "license": null, "license_expression": "MIT", "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "radex-booking", "package_url": "https://pypi.org/project/radex-booking/", "platform": null, "project_url": "https://pypi.org/project/radex-booking/", "project_urls": {"changelog": "https://github.com/radex-tech/booking-client-py/blob/master/CHANGELOG.md", "documentation": "https://github.com/radex-tech/booking-client-py/wiki", "homepage": "https://github.com/radex-tech/booking-client-py", "issues": "https://github.com/radex-tech/booking-client-py/issues", "source": "https://github.com/radex-tech/booking-client-py"}, "provides_extra": null, "release_url": "https://pypi.org/project/radex-booking/2025.7.28a1/", "requires_dist": ["httpx"], "requires_python": ">=3.10", "summary": "Python API client for radex booking service", "version": "2025.7.28a1", "yanked": false, "yanked_reason": null}, "last_serial": 30388465, "urls": [{"comment_text": null, "digests": {"blake2b_256": "6dff3c94ebe0014da9622f405eb90980671bd44823a2ab4e77ff2d3651106610", "md5": "1e40ffaf53ea8ef3c171c4402014a97f", "sha256": "dbc376652bf645c89fb841f51e10a94eb70636afd5f4426099119840391df6b2"}, "downloads": -1, "filename": "radex_booking-2025.7.28a1-py3-none-any.whl", "has_sig": false, "md5_digest": "1e40ffaf53ea8ef3c171c4402014a97f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.10", "size": 9022, "upload_time": "2025-07-28T18:11:14", "upload_time_iso_8601": "2025-07-28T18:11:14.212198Z", "url": "https://files.pythonhosted.org/packages/6d/ff/3c94ebe0014da9622f405eb90980671bd44823a2ab4e77ff2d3651106610/radex_booking-2025.7.28a1-py3-none-any.whl", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:10:40 GMT", "package": "vacancycalculator", "version": "0.3.1.4", "json": {"info": {"author": "E.Bringa-S.Bergamin-SiMaF", "author_email": "santiagobergamin@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Home-Page", "License", "Requires-Dist", "Summary"], "home_page": "https://github.com/TiagoBe0/VFScript-SiMaF", "keywords": null, "license": "MIT", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "vacancycalculator", "package_url": "https://pypi.org/project/vacancycalculator/", "platform": null, "project_url": "https://pypi.org/project/vacancycalculator/", "project_urls": {"Homepage": "https://github.com/TiagoBe0/VFScript-SiMaF"}, "provides_extra": null, "release_url": "https://pypi.org/project/vacancycalculator/0.3.1.4/", "requires_dist": ["scikit-learn", "pandas", "xgboost", "ovito", "numpy"], "requires_python": null, "summary": "Defect analysis and vacancy calculation for materials science", "version": "0.3.1.4", "yanked": false, "yanked_reason": null}, "last_serial": 30388504, "urls": [{"comment_text": null, "digests": {"blake2b_256": "41daa962346b0074fedd6bc62b8621259fab0ed2131ec16a32ffe341ef873ff3", "md5": "04f0014189e56b2666dbc4059614d22c", "sha256": "3548912d03e38ddc9cd4d2eb2abea62779908ef5da2e54754fd6b37039a9ebe6"}, "downloads": -1, "filename": "vacancycalculator-0.3.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "04f0014189e56b2666dbc4059614d22c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 59372, "upload_time": "2025-07-28T18:10:40", "upload_time_iso_8601": "2025-07-28T18:10:40.869555Z", "url": "https://files.pythonhosted.org/packages/41/da/a962346b0074fedd6bc62b8621259fab0ed2131ec16a32ffe341ef873ff3/vacancycalculator-0.3.1.4-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "dbad3dc548571cf08e5ea2dd883f522256c0de1752ae670a35716c1716475122", "md5": "13f327414c781cdf2bba9dfe36c05e49", "sha256": "cab8183b03a386ed321f755da1edf632c4937783a865314ff0d46e724c39c8db"}, "downloads": -1, "filename": "vacancycalculator-0.3.1.4.tar.gz", "has_sig": false, "md5_digest": "13f327414c781cdf2bba9dfe36c05e49", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 36490, "upload_time": "2025-07-28T18:10:42", "upload_time_iso_8601": "2025-07-28T18:10:42.174344Z", "url": "https://files.pythonhosted.org/packages/db/ad/3dc548571cf08e5ea2dd883f522256c0de1752ae670a35716c1716475122/vacancycalculator-0.3.1.4.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:10:36 GMT", "package": "airbyte-source-notion", "version": "3.0.16", "json": {"info": {"author": "Airbyte", "author_email": "contact@airbyte.io", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11"], "description": "# Notion source connector\n\nThis is the repository for the Notion source connector, written in Python.\nFor information about how to use this connector within Airbyte, see [the documentation](https://docs.airbyte.com/integrations/sources/notion).\n\n## Local development\n\n### Prerequisites\n\n- Python (~=3.9)\n- Poetry (~=1.7) - installation instructions [here](https://python-poetry.org/docs/#installation)\n\n### Installing the connector\n\nFrom this connector directory, run:\n\n```bash\npoetry install --with dev\n```\n\n### Create credentials\n\n**If you are a community contributor**, follow the instructions in the [documentation](https://docs.airbyte.com/integrations/sources/notion)\nto generate the necessary credentials. Then create a file `secrets/config.json` conforming to the `source_notion/spec.yaml` file.\nNote that any directory named `secrets` is gitignored across the entire Airbyte repo, so there is no danger of accidentally checking in sensitive information.\nSee `sample_files/sample_config.json` for a sample config file.\n\n### Locally running the connector\n\n```\npoetry run source-notion spec\npoetry run source-notion check --config secrets/config.json\npoetry run source-notion discover --config secrets/config.json\npoetry run source-notion read --config secrets/config.json --catalog integration_tests/configured_catalog.json\n```\n\n### Running unit tests\n\nTo run unit tests locally, from the connector directory run:\n\n```\npoetry run pytest unit_tests\n```\n\n### Building the docker image\n\n1. Install [`airbyte-ci`](https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md)\n2. Run the following command to build the docker image:\n\n```bash\nairbyte-ci connectors --name=source-notion build\n```\n\nAn image will be available on your host with the tag `airbyte/source-notion:dev`.\n\n### Running as a docker container\n\nThen run any of the connector commands as follows:\n\n```\ndocker run --rm airbyte/source-notion:dev spec\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-notion:dev check --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets airbyte/source-notion:dev discover --config /secrets/config.json\ndocker run --rm -v $(pwd)/secrets:/secrets -v $(pwd)/integration_tests:/integration_tests airbyte/source-notion:dev read --config /secrets/config.json --catalog /integration_tests/configured_catalog.json\n```\n\n### Running our CI test suite\n\nYou can run our full test suite locally using [Poe the Poet](Poe the Poet) (see [Airbyte documentation](https://docs.airbyte.com/platform/connector-development/local-connector-development)):\n\nIn connector directory, run:\n```zsh\npoe test-all\n```\n\nYou can also run the tests using [`airbyte-ci`](https://github.com/airbytehq/airbyte/blob/master/airbyte-ci/connectors/pipelines/README.md): (DEPRECATED)\n\n```zsh\nairbyte-ci connectors --name=source-notion test\n```\n\n### Customizing acceptance Tests\n\nCustomize `acceptance-test-config.yml` file to configure acceptance tests. See [Connector Acceptance Tests](https://docs.airbyte.com/connector-development/testing-connectors/connector-acceptance-tests-reference) for more information.\nIf your connector requires to create or destroy resources for use during acceptance tests create fixtures for it and place them inside integration_tests/acceptance.py.\n\n### Dependency Management\n\nAll of your dependencies should be managed via Poetry.\nTo add a new dependency, run:\n\n```bash\npoetry add <package-name>\n```\n\nPlease commit the changes to `pyproject.toml` and `poetry.lock` files.\n\n## Publishing a new version of the connector\n\nYou've checked out the repo, implemented a million dollar feature, and you're ready to share your changes with the world. Now what?\n\n1. Make sure your changes are passing our test suite: `airbyte-ci connectors --name=source-notion test`\n2. Bump the connector version (please follow [semantic versioning for connectors](https://docs.airbyte.com/contributing-to-airbyte/resources/pull-requests-handbook/#semantic-versioning-for-connectors)):\n   - bump the `dockerImageTag` value in in `metadata.yaml`\n   - bump the `version` value in `pyproject.toml`\n3. Make sure the `metadata.yaml` content is up to date.\n4. Make sure the connector documentation and its changelog is up to date (`docs/integrations/sources/notion.md`).\n5. Create a Pull Request: use [our PR naming conventions](https://docs.airbyte.com/contributing-to-airbyte/resources/pull-requests-handbook/#pull-request-title-convention).\n6. Pat yourself on the back for being an awesome contributor.\n7. Someone from Airbyte will take a look at your PR and iterate with you to merge it into master.\n8. Once your PR is merged, the new version of the connector will be automatically published to Docker Hub and our connector registry.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://airbyte.com", "keywords": null, "license": "MIT", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "airbyte-source-notion", "package_url": "https://pypi.org/project/airbyte-source-notion/", "platform": null, "project_url": "https://pypi.org/project/airbyte-source-notion/", "project_urls": {"Documentation": "https://docs.airbyte.com/integrations/sources/notion", "Homepage": "https://airbyte.com", "Repository": "https://github.com/airbytehq/airbyte"}, "provides_extra": null, "release_url": "https://pypi.org/project/airbyte-source-notion/3.0.16/", "requires_dist": ["airbyte-cdk<7,>=6"], "requires_python": "<3.12,>=3.10", "summary": "Source implementation for Notion.", "version": "3.0.16", "yanked": false, "yanked_reason": null}, "last_serial": 30388459, "urls": [{"comment_text": "", "digests": {"blake2b_256": "4a8ff69208de76c0f860fb9a919e9bd59f99141aa9e05048f5f442992eee3679", "md5": "db16acbd00d4994a214a452dbfef50da", "sha256": "4273b76027ba2640c0ac428ea8731bf2483f573ed079179212f43937eeb9126e"}, "downloads": -1, "filename": "airbyte_source_notion-3.0.16-py3-none-any.whl", "has_sig": false, "md5_digest": "db16acbd00d4994a214a452dbfef50da", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<3.12,>=3.10", "size": 28910, "upload_time": "2025-07-28T18:10:36", "upload_time_iso_8601": "2025-07-28T18:10:36.016749Z", "url": "https://files.pythonhosted.org/packages/4a/8f/f69208de76c0f860fb9a919e9bd59f99141aa9e05048f5f442992eee3679/airbyte_source_notion-3.0.16-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "f38035b3313fd0ea83d89ff7285f69b62bb2b569115c2f2709665dd2769b8fac", "md5": "d8d73cf3bdca1494a5cfff4386f7764c", "sha256": "64d38708fed4ae682deeab52301398211ca2f770bdc57c0949c5a762868e94f8"}, "downloads": -1, "filename": "airbyte_source_notion-3.0.16.tar.gz", "has_sig": false, "md5_digest": "d8d73cf3bdca1494a5cfff4386f7764c", "packagetype": "sdist", "python_version": "source", "requires_python": "<3.12,>=3.10", "size": 26782, "upload_time": "2025-07-28T18:10:37", "upload_time_iso_8601": "2025-07-28T18:10:37.207823Z", "url": "https://files.pythonhosted.org/packages/f3/80/35b3313fd0ea83d89ff7285f69b62bb2b569115c2f2709665dd2769b8fac/airbyte_source_notion-3.0.16.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:09:49 GMT", "package": "recce-nightly", "version": "1.13.0.20250728", "json": {"info": {"author": "InfuseAI Dev Team", "author_email": "dev@infuseai.io", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Programming Language :: Python :: 3.9"], "description": "<p align=\"center\">\n    <a href=\"https://datarecce.io\">\n        <picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://datarecce.io/assets/images/recce-logo-stacked.avif\">\n            <source media=\"(prefers-color-scheme: light)\" srcset=\"https://datarecce.io/assets/images/recce-logo-stacked.avif\">\n            <img alt=\"Recce: DataRecce.io\" src=\"https://datarecce.io/assets/images/recce-logo-stacked.avif\" width=\"200\" style=\"display: block; margin: 0 auto 20px;\">\n        </picture>\n    </a>\n</p>\n\n<h3 align=\"center\">Helping data teams preview, validate, and ship data changes with confidence.</h3>\n\n<p align=\"center\">\n    <a href=\"https://pypi.org/project/recce/\"><img src=\"https://img.shields.io/badge/pip_install-recce-006DAD?style=flat-square\" alt=\"install\"></a> &nbsp;\n    <a href=\"https://pypi.org/project/recce/\"><img src=\"https://img.shields.io/pypi/v/recce?style=flat-square\" alt=\"pipy\"></a> &nbsp;\n    <a href=\"https://pypi.org/project/recce/\"><img src=\"https://img.shields.io/pypi/pyversions/recce?style=flat-square\" alt=\"Python\"></a> &nbsp;\n    <a href=\"https://pypi.org/project/recce/#files\"><img src=\"https://img.shields.io/pypi/dw/recce?style=flat-square\" alt=\"downloads\"></a> &nbsp;\n    <a href=\"https://github.com/DataRecce/recce/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/DataRecce/recce?style=flat-square\" alt=\"license\"></a> &nbsp;\n    <a href=\"https://getdbt.slack.com/archives/C05C28V7CPP\"><img src=\"https://img.shields.io/badge/Slack-4A154B?style=flat-square&amp;logo=slack&amp;logoColor=white\" alt=\"Slack\"></a> &nbsp;\n    <a href=\"https://discord.com/invite/5zb2aK9KBV\"><img src=\"https://img.shields.io/discord/664381609771925514?color=%237289DA&amp;label=chat&amp;logo=discord&amp;logoColor=white&amp;style=flat-square\" alt=\"InfuseAI Discord Invite\"></a> &nbsp;\n</p>\n\n<p align=\"center\">\n    <a href=\"https://cal.com/team/recce/chat?utm_source=banner&utm_campaign=oss\">\n        <img alt=\"Book us with Cal.com\" src=\"https://cal.com/book-with-cal-light.svg\" />\n    </a>\n</p>\n\n## Trust, Verify, Ship\nCut dbt review time by 90% and ship accurate data fast\n\nRecce gives data teams a faster, more reliable way to understand, review, and ship changes without all the guesswork or manual overhead.\n\n## Quick Start\nYou can launch Recce in any dbt project in just two commands:\n\n```Bash\n# cd into your dbt project\npip install -U recce\nrecce server\n```\nThis starts Recce locally, where you can explore lineage and run queries. To unlock the full set of diffing tools, such as data comparisons and impact checks, you\u2019ll need to prepare two environments to compare against. You can follow our [Getting Started](https://docs.datarecce.io/get-started/) and [5-minute Jaffle Shop tutorial](https://docs.datarecce.io/get-started-jaffle-shop/) to try it out step-by-step.\n\n## What You Get\n\nRecce gives you a clear, fast way to understand what your data changes are doing and why they matter. It helps you catch problems early, verify metrics, and share your findings with others, all as part of your normal workflow.\n\n<a href=\"https://pr46.demo.datarecce.io/\"><img width=\"1347\" alt=\"readme\" src=\"https://github.com/user-attachments/assets/773e4c3a-0a15-49e0-8d1b-38a55af17cb0\" /></a>\n\n<a href=\"https://datarecce.io\"><img src=\"https://docs.datarecce.io/assets/images/home/diff-readme2.png\" style=\"width: 100%; max-width: 600px; display: block; margin: 0 auto 20px;\" alt=\"Model and column level diff\"/></a>\n\n<a href=\"https://datarecce.io\"><img src=\"https://docs.datarecce.io/assets/images/home/checklist-readme3.png\" style=\"width: 100%; max-width: 600px; display: block; margin: 0 auto 20px;\" alt=\"Checklist for collaboration\"/></a>\n\n### Using Recce for Impact Assessment in dbt PR Review\n\n- Select nodes in the lineage to perform Checks (diffs) as part of your impact assessment during development or PR\n  review.\n- Add Checks to your Checklist to note observed impacts.\n- Share your Checklist with the PR reviewer.\n- (`Recce Cloud`) Automatically sync Check status between Recce Instances\n- (`Recce Cloud`) Block PR merging until all Recce Checks have been approved\n\nRead more about using [Recce for Impact Assessment](https://datarecce.io/blog/hands-on-data-impact-analysis-recce/) on\nthe Recce blog.\n\n### What\u2019s Included\n\n- [Lineage and impact mapping](https://docs.datarecce.io/features/lineage/): Quickly see which models and columns are affected by a change. Navigate lineage down to the column level, and spot breaking changes with clear visual cues.\n- Metric and data comparisons: Use [Profile, Value, Top-K, and Histogram Diffs](https://docs.datarecce.io/features/lineage/#node-details) to compare results before and after changes. Validate things like row counts, category distributions, and numeric ranges without writing extra SQL.\n- [Query diff](https://docs.datarecce.io/features/query/): Write and compare any two queries side by side. This is helpful when validating fixes or reviewing changes with teammates.\n- [Checklist for reviews and approvals](https://docs.datarecce.io/features/checklist/): Turn your validation steps into a checklist. Add notes, rerun checks, and share the results with reviewers or stakeholders. In Recce Cloud, checklists can sync automatically and even block PRs until checks are approved.\n- Secure by design: Recce is [SOC 2 compliant](https://trust.cloud.datarecce.io/) to meet enterprise security standards. It runs locally or in your private environment, and your data stays in your warehouse.\n\n\ud83d\udc49 Want to dive deeper? Check out the [full documentation](https://docs.datarecce.io/) like [running Recce in CI/CD](https://docs.datarecce.io/guides/scenario-ci/)\n\n## Recce Cloud\n\nReady to collaborate and move faster as a team? Recce Cloud adds real-time collaboration, automatic checklist sync, and PR gating, so nothing gets merged without a full review.\n\n- Share checklists across environments\n- Invite stakeholders to review data changes\n- Block merges until all Checks are approved\n- Launch demo links from your CI with full context\n\nRecce Cloud is a hosted version of Recce that standardizes your workflow, keeps teams aligned, and reduces errors\u2014so you can ship data changes with confidence.\n\ud83d\udc49\u00a0[View Pricing and Plans](https://datarecce.io/pricing)\n\n## Community & Support\n\nHere's where you can get in touch with the Recce team and find support:\n\n- [dbt Slack](https://www.getdbt.com/community/join-the-community) in\n  the [#tools-recce](https://getdbt.slack.com/archives/C05C28V7CPP) channel\n- Email us [product@datarecce.io](mailto:product@datarecce.io)\n\nIf you believe you have found a bug, or there is some missing functionality in Recce, please open\na [GitHub Issue](https://github.com/DataRecce/recce/issues).\n\n## Recce on the web\n\nYou can follow along with news about Recce and blogs from our team in the following places:\n\n- [RecceHQ.com](https://reccehq.com/)\n- [LinkedIn](https://www.linkedin.com/company/datarecce)\n- [Medium blog](https://medium.com/inthepipeline)\n- [@datarecce](https://x.com/DataRecce) on Twitter/X\n- [@DataRecce@mastodon.social](https://mastodon.social/@DataRecce) on Mastodon\n- [@datarecce.bsky.social](https://bsky.app/profile/datarecce.bsky.social) on BlueSky\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "License-File", "Project-Url", "Provides-Extra", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/InfuseAI/recce", "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "recce-nightly", "package_url": "https://pypi.org/project/recce-nightly/", "platform": null, "project_url": "https://pypi.org/project/recce-nightly/", "project_urls": {"Bug Tracker": "https://github.com/InfuseAI/recce/issues", "Homepage": "https://github.com/InfuseAI/recce"}, "provides_extra": ["dev"], "release_url": "https://pypi.org/project/recce-nightly/1.13.0.20250728/", "requires_dist": ["boto3", "requests>=2.28.1", "ruamel.yaml>=0.18.6", "click>=7.1", "deepdiff<8.0,>=7.0", "portalocker", "fastapi", "itsdangerous", "uvicorn", "pydantic", "jinja2", "requests>=2.28.1", "rich>=12.0.0", "sentry-sdk", "watchdog", "websockets", "py-markdown-table", "python-dateutil", "python-multipart", "GitPython", "PyGithub", "sqlglot", "pytest>=4.6; extra == \"dev\"", "pytest-flake8; extra == \"dev\"", "black>=25.1.0; extra == \"dev\"", "isort>=6.0.1; extra == \"dev\"", "flake8>=7.2.0; extra == \"dev\"", "pre-commit>=4.2.0; extra == \"dev\"", "pytest-mypy; extra == \"dev\"", "pytest-cov; extra == \"dev\"", "twine; extra == \"dev\"", "tox; extra == \"dev\"", "pandas; extra == \"dev\"", "httpx; extra == \"dev\""], "requires_python": ">=3.9", "summary": "Environment diff tool for dbt", "version": "1.13.0.20250728", "yanked": false, "yanked_reason": null}, "last_serial": 30388450, "urls": [{"comment_text": null, "digests": {"blake2b_256": "b7bfe9ca7e80499a586249f29b77ecade7e78b4583e927190e29cee8923ce53d", "md5": "08bb4418934588f23ea3768779fc43eb", "sha256": "c01603abdf9cd81f13ebe43575530de5f7dba9ed42f554af8647b3974f897aad"}, "downloads": -1, "filename": "recce_nightly-1.13.0.20250728-py3-none-any.whl", "has_sig": false, "md5_digest": "08bb4418934588f23ea3768779fc43eb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 1569322, "upload_time": "2025-07-28T18:09:49", "upload_time_iso_8601": "2025-07-28T18:09:49.486341Z", "url": "https://files.pythonhosted.org/packages/b7/bf/e9ca7e80499a586249f29b77ecade7e78b4583e927190e29cee8923ce53d/recce_nightly-1.13.0.20250728-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "1476cfc143dbb73de4e71c90558044ae7f2ca11a8aa49a4d3a9a546ee34571f3", "md5": "94f77d2176d5e5073f18ad6bd0cd58c1", "sha256": "70344e42995b67ff703ed03129d997a163ed3352417845acf7dde7bf1b1c1680"}, "downloads": -1, "filename": "recce_nightly-1.13.0.20250728.tar.gz", "has_sig": false, "md5_digest": "94f77d2176d5e5073f18ad6bd0cd58c1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 1530704, "upload_time": "2025-07-28T18:09:51", "upload_time_iso_8601": "2025-07-28T18:09:51.973260Z", "url": "https://files.pythonhosted.org/packages/14/76/cfc143dbb73de4e71c90558044ae7f2ca11a8aa49a4d3a9a546ee34571f3/recce_nightly-1.13.0.20250728.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:09:32 GMT", "package": "bdkpython", "version": "2.0.0", "json": {"info": {"author": "Bitcoin Dev Kit Developers <dev@bitcoindevkit.org>", "author_email": null, "bugtrack_url": null, "classifiers": [], "description": "# bdkpython\nThe Python language bindings for the [Bitcoin Dev Kit](https://github.com/bitcoindevkit).\n\n## Install the package\n```shell\npip install bdkpython\n```\n\n## Simple example\n```python\nfrom bdkpython import Wallet\n```\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://github.com/bitcoindevkit/bdkpython", "keywords": null, "license": "MIT or Apache 2.0", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "bdkpython", "package_url": "https://pypi.org/project/bdkpython/", "platform": null, "project_url": "https://pypi.org/project/bdkpython/", "project_urls": {"Homepage": "https://github.com/bitcoindevkit/bdkpython"}, "provides_extra": null, "release_url": "https://pypi.org/project/bdkpython/2.0.0/", "requires_dist": null, "requires_python": null, "summary": "The Python language bindings for the Bitcoin Development Kit", "version": "2.0.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388455, "urls": [{"comment_text": null, "digests": {"blake2b_256": "b8740110a15ff42f6cc83b0faf1ea4a219eb76abb72555c568713ce503a48202", "md5": "b6387cc705d3f84860a6abe58f8da923", "sha256": "85589109c2fd6d45a603bb28c2db299c42ca512f3f743b4361fd098fc771928f"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp310-cp310-macosx_11_0_arm64.whl", "has_sig": false, "md5_digest": "b6387cc705d3f84860a6abe58f8da923", "packagetype": "bdist_wheel", "python_version": "cp310", "requires_python": null, "size": 5142580, "upload_time": "2025-07-28T18:09:32", "upload_time_iso_8601": "2025-07-28T18:09:32.682018Z", "url": "https://files.pythonhosted.org/packages/b8/74/0110a15ff42f6cc83b0faf1ea4a219eb76abb72555c568713ce503a48202/bdkpython-2.0.0-cp310-cp310-macosx_11_0_arm64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "f0b8e50f9a6f30af0482017d1761d73b913d3d735563d7f0b9982190b4a1ecde", "md5": "dbffda2bf50566602ddd29b01f7eb596", "sha256": "b9b2250f3c29982d0527c2d653e2c94aff6824eae5406abf7c2a4516575f562c"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp310-cp310-macosx_11_0_x86_64.whl", "has_sig": false, "md5_digest": "dbffda2bf50566602ddd29b01f7eb596", "packagetype": "bdist_wheel", "python_version": "cp310", "requires_python": null, "size": 5014506, "upload_time": "2025-07-28T18:09:39", "upload_time_iso_8601": "2025-07-28T18:09:39.156437Z", "url": "https://files.pythonhosted.org/packages/f0/b8/e50f9a6f30af0482017d1761d73b913d3d735563d7f0b9982190b4a1ecde/bdkpython-2.0.0-cp310-cp310-macosx_11_0_x86_64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "1342ce2d7f11ed99e5e2c10143ef9f7227bb931cac70ad1bc4fcf7f7b84d2ddc", "md5": "39c5e6a4a9a338deaf4cd0646eee56ae", "sha256": "7344ca9ae0dad6f2d47d0a34c49b1c3324b2aa7c6cb9809e31a2cd7367fce86e"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp310-cp310-manylinux_2_28_x86_64.whl", "has_sig": false, "md5_digest": "39c5e6a4a9a338deaf4cd0646eee56ae", "packagetype": "bdist_wheel", "python_version": "cp310", "requires_python": null, "size": 5362952, "upload_time": "2025-07-28T18:09:46", "upload_time_iso_8601": "2025-07-28T18:09:46.076314Z", "url": "https://files.pythonhosted.org/packages/13/42/ce2d7f11ed99e5e2c10143ef9f7227bb931cac70ad1bc4fcf7f7b84d2ddc/bdkpython-2.0.0-cp310-cp310-manylinux_2_28_x86_64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "dd6a5281634670bee6b66802647c88f086042132052bb9b2b3186f6c4e0320db", "md5": "c4d547d6e7f646b8fa7d748a7b1aeb89", "sha256": "1f1113be576dcffddaafd26b500eed805bee95397c32c19a7bcf34b75d4a5b57"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp310-cp310-win_amd64.whl", "has_sig": false, "md5_digest": "c4d547d6e7f646b8fa7d748a7b1aeb89", "packagetype": "bdist_wheel", "python_version": "cp310", "requires_python": null, "size": 4122217, "upload_time": "2025-07-28T18:09:53", "upload_time_iso_8601": "2025-07-28T18:09:53.780693Z", "url": "https://files.pythonhosted.org/packages/dd/6a/5281634670bee6b66802647c88f086042132052bb9b2b3186f6c4e0320db/bdkpython-2.0.0-cp310-cp310-win_amd64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "10ebcf9249be5119231130b20c98e42c08722d240aca998d90e92a0b40153369", "md5": "a7af5c8185b82317ee8bd3b40c0f6ad0", "sha256": "a2de453fc0fcdec4492cb768bc5157b6f046d8b1b5f2a464dcf87b36898f2fd0"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp311-cp311-macosx_11_0_arm64.whl", "has_sig": false, "md5_digest": "a7af5c8185b82317ee8bd3b40c0f6ad0", "packagetype": "bdist_wheel", "python_version": "cp311", "requires_python": null, "size": 5142576, "upload_time": "2025-07-28T18:09:34", "upload_time_iso_8601": "2025-07-28T18:09:34.738894Z", "url": "https://files.pythonhosted.org/packages/10/eb/cf9249be5119231130b20c98e42c08722d240aca998d90e92a0b40153369/bdkpython-2.0.0-cp311-cp311-macosx_11_0_arm64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "2e8a74c9333cb768c6f11042c52b46fe20301526da17cbad61cdeab7c38d45ea", "md5": "5b52ee95eed91f4bca5979f72b255d36", "sha256": "cef53d02d6c9dc54ba931dd797461996e37926e17b276bf17a1d65473de075f0"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp311-cp311-macosx_11_0_x86_64.whl", "has_sig": false, "md5_digest": "5b52ee95eed91f4bca5979f72b255d36", "packagetype": "bdist_wheel", "python_version": "cp311", "requires_python": null, "size": 5014510, "upload_time": "2025-07-28T18:09:40", "upload_time_iso_8601": "2025-07-28T18:09:40.942229Z", "url": "https://files.pythonhosted.org/packages/2e/8a/74c9333cb768c6f11042c52b46fe20301526da17cbad61cdeab7c38d45ea/bdkpython-2.0.0-cp311-cp311-macosx_11_0_x86_64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "993e05b6ed397b50b99cac0510e9544df886da4d1aa41ec02380695531fd5e84", "md5": "9242c2e4cb1b97888fe8f5ac1719244f", "sha256": "6be01ba1db861cb982040509a64111d3f3517f93a21f93230d9652184d88d752"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp311-cp311-manylinux_2_28_x86_64.whl", "has_sig": false, "md5_digest": "9242c2e4cb1b97888fe8f5ac1719244f", "packagetype": "bdist_wheel", "python_version": "cp311", "requires_python": null, "size": 5362936, "upload_time": "2025-07-28T18:09:47", "upload_time_iso_8601": "2025-07-28T18:09:47.595529Z", "url": "https://files.pythonhosted.org/packages/99/3e/05b6ed397b50b99cac0510e9544df886da4d1aa41ec02380695531fd5e84/bdkpython-2.0.0-cp311-cp311-manylinux_2_28_x86_64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "59b6ac594721546ebc11d28d177707c452e2d432a79ca5ceb0a79a4905d53e95", "md5": "cb77f80263f9c39ec4b7bf4dba56637a", "sha256": "e4000ae25a614568d6551a2b07864c4efe83c2529e407acece6a6e8b5b8ca417"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp311-cp311-win_amd64.whl", "has_sig": false, "md5_digest": "cb77f80263f9c39ec4b7bf4dba56637a", "packagetype": "bdist_wheel", "python_version": "cp311", "requires_python": null, "size": 4122201, "upload_time": "2025-07-28T18:09:55", "upload_time_iso_8601": "2025-07-28T18:09:55.233630Z", "url": "https://files.pythonhosted.org/packages/59/b6/ac594721546ebc11d28d177707c452e2d432a79ca5ceb0a79a4905d53e95/bdkpython-2.0.0-cp311-cp311-win_amd64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "b37d55ffbdb18f47265e5e196d43512bc75fc6692a2aaf551b8b38ed95c05651", "md5": "b6ed05e333a67c615a94f266a8a00bb5", "sha256": "ea4113cb957d747183e4dd7516d688d4cb17d9d064ab58bc0412d3d378b3abbf"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp312-cp312-macosx_11_0_arm64.whl", "has_sig": false, "md5_digest": "b6ed05e333a67c615a94f266a8a00bb5", "packagetype": "bdist_wheel", "python_version": "cp312", "requires_python": null, "size": 5142589, "upload_time": "2025-07-28T18:09:36", "upload_time_iso_8601": "2025-07-28T18:09:36.121533Z", "url": "https://files.pythonhosted.org/packages/b3/7d/55ffbdb18f47265e5e196d43512bc75fc6692a2aaf551b8b38ed95c05651/bdkpython-2.0.0-cp312-cp312-macosx_11_0_arm64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "418d3ff3287aebfa4bf10622a37128adaa84f323f87855cbafa744963bf09e71", "md5": "c77da15494e4e23e8abdeed6394b29a6", "sha256": "ddb461048c5a9714aff0858b988952efea114630b0757b7483d0341448042c2b"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp312-cp312-macosx_11_0_x86_64.whl", "has_sig": false, "md5_digest": "c77da15494e4e23e8abdeed6394b29a6", "packagetype": "bdist_wheel", "python_version": "cp312", "requires_python": null, "size": 5014514, "upload_time": "2025-07-28T18:09:42", "upload_time_iso_8601": "2025-07-28T18:09:42.950722Z", "url": "https://files.pythonhosted.org/packages/41/8d/3ff3287aebfa4bf10622a37128adaa84f323f87855cbafa744963bf09e71/bdkpython-2.0.0-cp312-cp312-macosx_11_0_x86_64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "a9cac73d1e0d96d29f2e7b66dba8bd002993e54c830475716f43df50e4936da9", "md5": "2499110660ffadc7afe3cd39ecd51b3f", "sha256": "f4be77e2ef8cc1b53ab8357afad023055a063c98447ad73964e39da2dbdb2c86"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp312-cp312-manylinux_2_28_x86_64.whl", "has_sig": false, "md5_digest": "2499110660ffadc7afe3cd39ecd51b3f", "packagetype": "bdist_wheel", "python_version": "cp312", "requires_python": null, "size": 5362935, "upload_time": "2025-07-28T18:09:49", "upload_time_iso_8601": "2025-07-28T18:09:49.348279Z", "url": "https://files.pythonhosted.org/packages/a9/ca/c73d1e0d96d29f2e7b66dba8bd002993e54c830475716f43df50e4936da9/bdkpython-2.0.0-cp312-cp312-manylinux_2_28_x86_64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "d775c3465daefa3b2328cf05078cd26a2d1fbd7c4bf6f7899bb108066cfe4d9c", "md5": "925c17df99c44f41acfb04763aeef050", "sha256": "a311d7a76be7dea50f69349f2649dbfce142e4a7302dc8747c6f1a313cb19c48"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp312-cp312-win_amd64.whl", "has_sig": false, "md5_digest": "925c17df99c44f41acfb04763aeef050", "packagetype": "bdist_wheel", "python_version": "cp312", "requires_python": null, "size": 4122213, "upload_time": "2025-07-28T18:09:56", "upload_time_iso_8601": "2025-07-28T18:09:56.648162Z", "url": "https://files.pythonhosted.org/packages/d7/75/c3465daefa3b2328cf05078cd26a2d1fbd7c4bf6f7899bb108066cfe4d9c/bdkpython-2.0.0-cp312-cp312-win_amd64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "1f917d425564952952dea2ba06c0ffb72cb8121b0d6eec0b3f356136e443bcaa", "md5": "7b2f76845cb1c878df800df361f0fd07", "sha256": "17bc0a170dffd3106291176e0be37d974296868b4d2b45f163ef9f626332afe5"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp313-cp313-macosx_11_0_arm64.whl", "has_sig": false, "md5_digest": "7b2f76845cb1c878df800df361f0fd07", "packagetype": "bdist_wheel", "python_version": "cp313", "requires_python": null, "size": 5142580, "upload_time": "2025-07-28T18:09:37", "upload_time_iso_8601": "2025-07-28T18:09:37.834443Z", "url": "https://files.pythonhosted.org/packages/1f/91/7d425564952952dea2ba06c0ffb72cb8121b0d6eec0b3f356136e443bcaa/bdkpython-2.0.0-cp313-cp313-macosx_11_0_arm64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "91d85c57c65a3f55168b1b30558f30c1f77d70a413a9a06370b734f02b273034", "md5": "5cb3d982d6ff7de8dc2830b6f9481b6a", "sha256": "d484660eac248cba7f06bde924deff54c545feb95dde372a3fc80f943f526927"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp313-cp313-macosx_11_0_x86_64.whl", "has_sig": false, "md5_digest": "5cb3d982d6ff7de8dc2830b6f9481b6a", "packagetype": "bdist_wheel", "python_version": "cp313", "requires_python": null, "size": 5014515, "upload_time": "2025-07-28T18:09:44", "upload_time_iso_8601": "2025-07-28T18:09:44.682752Z", "url": "https://files.pythonhosted.org/packages/91/d8/5c57c65a3f55168b1b30558f30c1f77d70a413a9a06370b734f02b273034/bdkpython-2.0.0-cp313-cp313-macosx_11_0_x86_64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "449bbf754a1fd19c2c696dba66eca4271c599981e069decb82c923c2ad5f77dd", "md5": "6ce5a47a32ace09dd3ac8150f8a608e8", "sha256": "7eaa1b83c335c66c2622b1b9892f144dceb9f3feeac5b4f48299baa744a42d62"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp313-cp313-manylinux_2_28_x86_64.whl", "has_sig": false, "md5_digest": "6ce5a47a32ace09dd3ac8150f8a608e8", "packagetype": "bdist_wheel", "python_version": "cp313", "requires_python": null, "size": 5362940, "upload_time": "2025-07-28T18:09:52", "upload_time_iso_8601": "2025-07-28T18:09:52.064463Z", "url": "https://files.pythonhosted.org/packages/44/9b/bf754a1fd19c2c696dba66eca4271c599981e069decb82c923c2ad5f77dd/bdkpython-2.0.0-cp313-cp313-manylinux_2_28_x86_64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "af01f02a0ce60f109cc69e0c04b842f3146fa5bbaba9bafa866e842249f109fe", "md5": "601ae1fb8441e52a766447f4dab87084", "sha256": "b49cce09fb241bf7bef642c1d88a5603120ef447011131610af73a439359f49e"}, "downloads": -1, "filename": "bdkpython-2.0.0-cp313-cp313-win_amd64.whl", "has_sig": false, "md5_digest": "601ae1fb8441e52a766447f4dab87084", "packagetype": "bdist_wheel", "python_version": "cp313", "requires_python": null, "size": 4122213, "upload_time": "2025-07-28T18:09:58", "upload_time_iso_8601": "2025-07-28T18:09:58.013429Z", "url": "https://files.pythonhosted.org/packages/af/01/f02a0ce60f109cc69e0c04b842f3146fa5bbaba9bafa866e842249f109fe/bdkpython-2.0.0-cp313-cp313-win_amd64.whl", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:09:30 GMT", "package": "pragma-sdk", "version": "2.8.1", "json": {"info": {"author": null, "author_email": "0xevolve <matthias@pragma.build>", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Pragma SDK\n\nMain repository containing our SDK code.\n\nFor more information, please check the documentation:\n\nhttps://pragma-docs.readthedocs.io/en/latest/index.html\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "Oracle, Pragma, Starknet", "license": null, "license_expression": "MIT", "license_files": null, "maintainer": null, "maintainer_email": null, "name": "pragma-sdk", "package_url": "https://pypi.org/project/pragma-sdk/", "platform": null, "project_url": "https://pypi.org/project/pragma-sdk/", "project_urls": {"Documentation": "https://docs.pragma.build", "Homepage": "https://pragma.build", "Repository": "https://github.com/Astraly-Labs/pragma-sdk"}, "provides_extra": ["dev", "docs", "typing"], "release_url": "https://pypi.org/project/pragma-sdk/2.8.1/", "requires_dist": ["aioresponses>=0.7.4", "deprecated>=1.2.14", "grpcio-tools<1.71.0,>=1.60.0", "pydantic>=2.7.4", "python-dotenv>=1.0.0", "pyyaml>=6.0.1", "redis[hiredis]>=5.0.7", "requests-mock>=1.11.0", "starknet-py==0.27.0", "typer==0.6.1", "websockets>=14.0", "boto3>=1.28.61; extra == \"dev\"", "coverage>=7.2.1; extra == \"dev\"", "fakeredis[json]>=2.26.0; extra == \"dev\"", "moto[s3,secretsmanager]>=4.2.5; extra == \"dev\"", "poethepoet>=0.21.1; extra == \"dev\"", "pytest-asyncio>=0.21.1; extra == \"dev\"", "pytest-cov>=4.0.0; extra == \"dev\"", "pytest-mock>=3.6.1; extra == \"dev\"", "pytest-rerunfailures>=12.0; extra == \"dev\"", "pytest-xdist>=3.2.1; extra == \"dev\"", "pytest>=7.2.2; extra == \"dev\"", "ruff>=0.4; extra == \"dev\"", "setuptools>=68.0.0; extra == \"dev\"", "enum-tools[sphinx]>=0.12.0; extra == \"docs\"", "furo>=2024.5.6; extra == \"docs\"", "pallets-sphinx-themes>=2.1.3; extra == \"docs\"", "sphinx>=7.3.7; extra == \"docs\"", "mypy>=1.10; extra == \"typing\"", "types-deprecated>=1.2.9; extra == \"typing\"", "types-pyyaml>=6.0.12.20240311; extra == \"typing\"", "types-requests>=2.26.0; extra == \"typing\""], "requires_python": "<3.13,>=3.11", "summary": "Core package for rollup-native Pragma Oracle", "version": "2.8.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388436, "urls": [{"comment_text": null, "digests": {"blake2b_256": "670c596c37f2d9ae70f553f3c89954eb136836bbb2dcc78bcaf181369629cc5c", "md5": "fb78af3d3b6d81f5bcb1e94f0692d86e", "sha256": "437e9463b1e19f830cce267427f0db0cda814383740d47159d88fd73177bde5a"}, "downloads": -1, "filename": "pragma_sdk-2.8.1-py3-none-any.whl", "has_sig": false, "md5_digest": "fb78af3d3b6d81f5bcb1e94f0692d86e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<3.13,>=3.11", "size": 1069356, "upload_time": "2025-07-28T18:09:30", "upload_time_iso_8601": "2025-07-28T18:09:30.880915Z", "url": "https://files.pythonhosted.org/packages/67/0c/596c37f2d9ae70f553f3c89954eb136836bbb2dcc78bcaf181369629cc5c/pragma_sdk-2.8.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "4e6912cddc3560853211fa13cd26e3220bf02ab1f1484100da86e5ee291f8f94", "md5": "67af89160ab2af5cc1ebb90422716196", "sha256": "e23d14bb27a9afd7509793d9f504811591b683ad7ecb28b8680ebfc06fdedb60"}, "downloads": -1, "filename": "pragma_sdk-2.8.1.tar.gz", "has_sig": false, "md5_digest": "67af89160ab2af5cc1ebb90422716196", "packagetype": "sdist", "python_version": "source", "requires_python": "<3.13,>=3.11", "size": 1296378, "upload_time": "2025-07-28T18:09:32", "upload_time_iso_8601": "2025-07-28T18:09:32.125696Z", "url": "https://files.pythonhosted.org/packages/4e/69/12cddc3560853211fa13cd26e3220bf02ab1f1484100da86e5ee291f8f94/pragma_sdk-2.8.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:08:43 GMT", "package": "trendify", "version": "1.1.5", "json": {"info": {"author": null, "author_email": "Talbot Knighton <talbotknighton@gmail.org>, David Gable <david@david-gable.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.11"], "description": "\n[Read the Docs](https://talbotknighton.github.io/trendify/)\n\n[View on PyPi](https://pypi.org/project/trendify/)\n\n[View on GitHub](https://github.com/TalbotKnighton/trendify)", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "Data, Database, Post, Process, Products", "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": "Talbot Knighton <talbotknighton@gmail.org>, David Gable <david@david-gable.com>", "name": "trendify", "package_url": "https://pypi.org/project/trendify/", "platform": null, "project_url": "https://pypi.org/project/trendify/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/trendify/1.1.5/", "requires_dist": ["dash", "filelock", "flask", "grafana-api", "jinja2", "matplotlib", "numpy", "numpydantic", "pandas", "plotly", "pydantic", "scipy", "strenum", "supervisor", "waitress"], "requires_python": ">=3.9", "summary": "Tools for generating data products, storing, and interacting with them", "version": "1.1.5", "yanked": false, "yanked_reason": null}, "last_serial": 30388429, "urls": [{"comment_text": null, "digests": {"blake2b_256": "e1003277319a1f1322ed9c5ab71417b00cd3a7d27a8a519dfaf492e095bb28a0", "md5": "b404809f9c1cb4fbbed1addc9dda485e", "sha256": "8d50c44220294505d0a92103906822a27664338a5451bed9e52f44da3580f3d5"}, "downloads": -1, "filename": "trendify-1.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "b404809f9c1cb4fbbed1addc9dda485e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 56127, "upload_time": "2025-07-28T18:08:43", "upload_time_iso_8601": "2025-07-28T18:08:43.676766Z", "url": "https://files.pythonhosted.org/packages/e1/00/3277319a1f1322ed9c5ab71417b00cd3a7d27a8a519dfaf492e095bb28a0/trendify-1.1.5-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "a02cc5ef3d5635192aaa7214344ccd21de41b51cf253404689447d8696b14c19", "md5": "edb5c64df088ed0102009811723efc77", "sha256": "9996c35c51f27dd5efd84786d799469ad0fa3b1fc42c1682a09de71f5a84fcb2"}, "downloads": -1, "filename": "trendify-1.1.5.tar.gz", "has_sig": false, "md5_digest": "edb5c64df088ed0102009811723efc77", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 3113664, "upload_time": "2025-07-28T18:08:44", "upload_time_iso_8601": "2025-07-28T18:08:44.844465Z", "url": "https://files.pythonhosted.org/packages/a0/2c/c5ef3d5635192aaa7214344ccd21de41b51cf253404689447d8696b14c19/trendify-1.1.5.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:08:40 GMT", "package": "robosapiensio", "version": "0.4.3", "json": {"info": {"author": "Sahar Nasimi Nezhad & Bert Van Acker", "author_email": "bert.vanacker@uantwerpen.be", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3"], "description": "\n`robosapiensIO <https://rpio.readthedocs.io/en/latest/index.html>`_: developing trustworthy self-adaptive robotics made easy.\n\nThis repository provides a flexible software architecture framework for building self-adaptive, trustworthy robotic applications using the RoboSapiens Adaptive Platform. It includes modular building blocks for runtime adaptation, trustworthiness monitoring, and knowledge management, enabling the seamless deployment of adaptive systems in diverse environments. The platform supports both resource-constrained and high-performance computing setups, facilitating reliable, automated responses to changing operational conditions. \n", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://rpio.readthedocs.io/en/latest/index.html", "keywords": "Robotics, Trustworthiness, self-adaptive, Model-based", "license": "Apache License 2.0", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "robosapiensio", "package_url": "https://pypi.org/project/robosapiensio/", "platform": null, "project_url": "https://pypi.org/project/robosapiensio/", "project_urls": {"Homepage": "https://rpio.readthedocs.io/en/latest/index.html"}, "provides_extra": null, "release_url": "https://pypi.org/project/robosapiensio/0.4.3/", "requires_dist": ["alabaster==1.0.0", "altgraph==0.17.4", "async-timeout==4.0.3", "babel==2.16.0", "certifi==2024.8.30", "charset-normalizer==3.4.0", "click==8.1.7", "colorama==0.4.6", "contourpy==1.3.0", "cycler==0.12.1", "docutils==0.21.2", "fonttools==4.54.1", "idna==3.10", "imagesize==1.4.1", "Jinja2==3.1.4", "jsonpickle==3.3.0", "kiwisolver==1.4.7", "Markdown==3.7", "markdown-it-py==3.0.0", "MarkupSafe==3.0.2", "matplotlib==3.9.2", "mdit-py-plugins==0.4.2", "mdurl==0.1.2", "myst-parser==4.0.0", "numpy==2.1.2", "packaging==24.0", "paho-mqtt==2.1.0", "pefile==2023.2.7", "pillow==11.0.0", "portion==2.6.0", "Pygments==2.18.0", "pyinstaller==6.11.0", "pyinstaller-hooks-contrib==2024.8", "pyparsing==3.2.0", "python-dateutil==2.9.0.post0", "pywin32-ctypes==0.2.2", "PyYAML==6.0.2", "redis==5.2.0", "requests==2.32.3", "six==1.16.0", "snowballstemmer==2.2.0", "sortedcontainers==2.4.0", "Sphinx==8.1.3", "sphinx-markdown-tables==0.0.17", "sphinx-rtd-theme==3.0.1", "sphinxcontrib-applehelp==2.0.0", "sphinxcontrib-devhelp==2.0.0", "sphinxcontrib-htmlhelp==2.1.0", "sphinxcontrib-jquery==4.1", "sphinxcontrib-jsmath==1.0.1", "sphinxcontrib-qthelp==2.0.0", "sphinxcontrib-serializinghtml==2.0.0", "tomli==2.0.2", "urllib3==2.2.3", "textX==4.1.0", "path==17.0.0"], "requires_python": ">=3.10", "summary": "robosapiensIO - your gateway to trustworthy self-adaptive robotics.", "version": "0.4.3", "yanked": false, "yanked_reason": null}, "last_serial": 30388426, "urls": [{"comment_text": null, "digests": {"blake2b_256": "ed503497c2952c82304e691b435190298b38e60c3d770d74ad28dab20dc7d28e", "md5": "5ff570f83a2ec0564dd6fffc2ec30bbf", "sha256": "5ddd9bab200497795e027177e000dc861dc93c213d78fc1e4da2c56234bea600"}, "downloads": -1, "filename": "robosapiensio-0.4.3-py3-none-any.whl", "has_sig": false, "md5_digest": "5ff570f83a2ec0564dd6fffc2ec30bbf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.10", "size": 85326, "upload_time": "2025-07-28T18:08:40", "upload_time_iso_8601": "2025-07-28T18:08:40.631159Z", "url": "https://files.pythonhosted.org/packages/ed/50/3497c2952c82304e691b435190298b38e60c3d770d74ad28dab20dc7d28e/robosapiensio-0.4.3-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "a85489efaaa64ff72489a51476a018155a5748301dd510c566dfa1390f7bda39", "md5": "70e9ba7cae6298589aafb0fc2d9b7073", "sha256": "2e4f027183f24edc74fdcc2f3d189d2893482fb01084165b2fc3b72089720a55"}, "downloads": -1, "filename": "robosapiensio-0.4.3.tar.gz", "has_sig": false, "md5_digest": "70e9ba7cae6298589aafb0fc2d9b7073", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.10", "size": 68019, "upload_time": "2025-07-28T18:08:42", "upload_time_iso_8601": "2025-07-28T18:08:42.217658Z", "url": "https://files.pythonhosted.org/packages/a8/54/89efaaa64ff72489a51476a018155a5748301dd510c566dfa1390f7bda39/robosapiensio-0.4.3.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:08:31 GMT", "package": "gtfs-station-stop", "version": "0.9.6", "json": {"info": {"author": null, "author_email": "Benjamin Pearce <gtfs@bcpearce.com>", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# GTFS Station Stop\n\nA project for organizing GTFS Real-Time data for use as a homeassistant sensor.\n\n![test coverage](./coverage.svg)'\n\n## Usage\n\nThis is designed for use with [Home Assistant GTFS Realtime Custom Component](https://github.com/bcpearce/homeassistant-gtfs-realtime).\n\nIt can also be used for general GTFS update purposes.\n\n### Feed Subjects and Station Stops\n\nAll updates go through the Feed Subject which is setup to call updates from one or more feed URLS.\n\nCreate a feed subject like so, then pass it in the constructor for a Station Stop\n\n```python\nfrom gtfs_station_stop.feed_subject import FeedSubject\nfrom gtfs_station_stop.station_stop import StationStop\n\n# Obtain the API keep from your GTFS provider if needed, otherwise leave blank.\napi_key = \"YOUR_API_KEY_HERE\"\nurls = [\"https://gtfs.example.com/feed1\", \"https://gtfs.example.com/feed2\"]\nfeed_subject = FeedSubject(urls, api_key)\n\n# Obtain the Stop ID from GTFS static data from your provider.\n# This must match those provided by the realtime feed.\nstation_stop_nb = StationStop(\"STOP_ID_NORTHBOUND\", feed_subject)\nstation_stop_sb = StationStop(\"STOP_ID_SOUTHBOUND\", feed_subject)\n```\n\nCalling `feed_subject.update()` will update all registered listeners.\n\n```python\nfeed_subject.update()\n\nfor arrival in station_stop_nb.arrivals:\n    minutes_to = (arrival.time - time.time()) / 60.0\n    print(f\"{arrival.route} in {minutes_to}\")\n```\n\nActive service alerts are also supported for station stops and for routes.\n\n```python\nroute_status = RouteStatus(\"Line 1\", feed_subject)\n\nfeed_subject.update()\n\nfor alert in route_status.alerts:\n    print(f\"{route_status.id} alert {alert.header_text['en']}\")\n\nfor alert in station_stop_nb.alerts:\n    print(f\"{station_stop_nb.id} alert {alert.header_text['en']}\")\n```\n\nAs the update will make one or more http requests, this may improve performance or integrate better with an asynchronous project.\n\n### GTFS Static Info\n\nStatic data can be loaded into a Dataset for convenient lookup to use alongside GTFS Realtime data. GTFS data can be read from a file or a URL from your service provider. The GTFS file must be provided as a .zip containing the requisite .txt files as defined by [GTFS Static Reference](https://developers.google.com/transit/gtfs/reference).\n\n```python\nfrom gtfs_station_stop.station_stop_info import StationStopInfoDataset\n\nstation_stop_infods = StationStopInfoDataset(\"gtfs_static.zip\")\nprint(f\"{station_stop_infods['STOP_ID']}\")\n```\n\nStatic info can be queried through the `station_stop_info`, `route_info`, `calendar`, and `trip_info` submodules.\n\nGTFS providers will regularly update their static feeds. In order to account for this, the library will attempt to cache zip file downloads for static info.\n\n### Async Updates\n\nAsynchronous updates are also supported through the `async_update()` method.\n\n```python\nawait feed_subject.async_update()\n```\n\nStatic data can also be obtained similarly with `gtfs_station_stop.static_Dataset.async_factory`.\n\n```python\nstation_stop_info_Dataset = await async_get_gtfs_Dataset(StationStopInfoDataset, \"https://gtfsprovider.example.com/static.zip\")\n```\n\n### Command Line Interface\n\nThis can be run as a Python module on the command line using\n\n```bash\n$ python -m gtfs_station_stop\n```\n\nThis must be installed with the optional group `[cli]`.\n\n```bash\n$ pip install gtfs_station_stop[cli]\n```\n\nUse `python -m gtfs_station_stop --help` for details.\n\n## Development Setup\n\nIt is recommended to use [uv](https://docs.astral.sh/uv/) to develop in this project.\n\n```bash\n$ uv pip install --group dev\n```\n\nRun tests with:\n\n```bash\n$ uv run pytest\n```\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "gtfs-station-stop", "package_url": "https://pypi.org/project/gtfs-station-stop/", "platform": null, "project_url": "https://pypi.org/project/gtfs-station-stop/", "project_urls": {"Homepage": "https://github.com/bcpearce/gtfs-station-stop", "Issues": "https://github.com/bcpearce/gtfs-station-stop/issues"}, "provides_extra": null, "release_url": "https://pypi.org/project/gtfs-station-stop/0.9.6/", "requires_dist": ["aiofiles>=24.1.0", "aiohttp-client-cache[sqlite]>=0.13", "aiohttp>=3.11.11", "coverage-badge>=1.1.2", "gtfs-realtime-bindings>=1.0.0", "protobuf!=6.31.0", "requests-cache>=1.2.1", "requests>=2.32.3"], "requires_python": ">=3.13.2", "summary": "Package for reading and organizing GTFS data for a given station stop.", "version": "0.9.6", "yanked": false, "yanked_reason": null}, "last_serial": 30388423, "urls": [{"comment_text": null, "digests": {"blake2b_256": "9a045bf77cfef40eb71f9cd0d54462f4cf7c1f902ca60ccfdccdc8769cb1abab", "md5": "490c5addb05a962d83dc17cdf6128c01", "sha256": "f5644095d0fae756f12db175c65ffd35122355536729d5dc463697a4fa3cd5ed"}, "downloads": -1, "filename": "gtfs_station_stop-0.9.6-py3-none-any.whl", "has_sig": false, "md5_digest": "490c5addb05a962d83dc17cdf6128c01", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.13.2", "size": 18698, "upload_time": "2025-07-28T18:08:31", "upload_time_iso_8601": "2025-07-28T18:08:31.256808Z", "url": "https://files.pythonhosted.org/packages/9a/04/5bf77cfef40eb71f9cd0d54462f4cf7c1f902ca60ccfdccdc8769cb1abab/gtfs_station_stop-0.9.6-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "befdfecd0fe49f499d35874531a3335678c72d5795b215488c557df0fa286e21", "md5": "61ffd1cb75a47bfaa84e85cf43bfc70e", "sha256": "453b72bac83cbc3716117fc6746fcd893c8c062e2bda88b95b0f85c1e4fd3e0a"}, "downloads": -1, "filename": "gtfs_station_stop-0.9.6.tar.gz", "has_sig": false, "md5_digest": "61ffd1cb75a47bfaa84e85cf43bfc70e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.13.2", "size": 109837, "upload_time": "2025-07-28T18:08:32", "upload_time_iso_8601": "2025-07-28T18:08:32.533222Z", "url": "https://files.pythonhosted.org/packages/be/fd/fecd0fe49f499d35874531a3335678c72d5795b215488c557df0fa286e21/gtfs_station_stop-0.9.6.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:08:28 GMT", "package": "kimina-client", "version": "0.1.7", "json": {"info": {"author": null, "author_email": "Kimi Team - Project Numina <contact@projectnumina.com>", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Kimina client\n\nClient SDK to interact with Kimina Lean server. \n\nExample use:\n```python\nfrom kimina_client import KiminaClient\n\n# Specify LEAN_SERVER_API_KEY in your .env or pass `api_key`.\n# Default `api_url` is https://projectnumina.ai\nclient = KiminaClient()\n\n# If running locally use:\n# client = KiminaClient(api_url=\"http://localhost:80\")\n\nclient.check(\"#check Nat\")\n```\n\n## Backward client\n\n```python\nfrom kimina_client import Lean4Client\n\nclient = Lean4Client()\n\nclient.verify(\"#check Nat\")\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": "MIT", "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "kimina-client", "package_url": "https://pypi.org/project/kimina-client/", "platform": null, "project_url": "https://pypi.org/project/kimina-client/", "project_urls": {"Homepage": "https://github.com/project-numina/kimina-lean-server", "Issues": "https://github.com/project-numina/kimina-lean-server/issues"}, "provides_extra": ["server"], "release_url": "https://pypi.org/project/kimina-client/0.1.7/", "requires_dist": ["colorama>=0.4.6", "datasets>=4.0.0", "httpx>=0.28.1", "loguru>=0.7.3", "pip>=25.1.1", "pydantic-settings>=2.10.0", "python-dotenv>=1.1.0", "requests>=2.31.0", "tabulate>=0.9.0", "tenacity>=9.1.2", "fastapi>=0.115.13; extra == \"server\"", "google-cloud-logging>=3.12.1; extra == \"server\"", "prisma>=0.15.0; extra == \"server\"", "psutil>=7.0.0; extra == \"server\"", "rich>=14.0.0; extra == \"server\"", "uvicorn>=0.34.3; extra == \"server\""], "requires_python": ">=3.9", "summary": "Client SDK to interact with Kimina Lean server.", "version": "0.1.7", "yanked": false, "yanked_reason": null}, "last_serial": 30388419, "urls": [{"comment_text": null, "digests": {"blake2b_256": "dbf55fc9d4e5427c1a3807683884fc0a5b58f496f63ab972553c81d60d88e2a6", "md5": "6a0e5ce6be019558e6b07587b99dfd56", "sha256": "b84820ff11bee190716d069672a82219292b43c7556d9cb931dbc0c005aa49d0"}, "downloads": -1, "filename": "kimina_client-0.1.7-py3-none-any.whl", "has_sig": false, "md5_digest": "6a0e5ce6be019558e6b07587b99dfd56", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 28870, "upload_time": "2025-07-28T18:08:28", "upload_time_iso_8601": "2025-07-28T18:08:28.375036Z", "url": "https://files.pythonhosted.org/packages/db/f5/5fc9d4e5427c1a3807683884fc0a5b58f496f63ab972553c81d60d88e2a6/kimina_client-0.1.7-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "1e9c91ec23e96d81cb39ca24670eebcd7ea119b917b595dcf572ec88847376a6", "md5": "151d54f9b485fd478313c7a0d316fbc9", "sha256": "95774fde0259a0280e6ce72090cf59d2659492160ed0be1a617d1eb2641abab1"}, "downloads": -1, "filename": "kimina_client-0.1.7.tar.gz", "has_sig": false, "md5_digest": "151d54f9b485fd478313c7a0d316fbc9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 25162, "upload_time": "2025-07-28T18:08:29", "upload_time_iso_8601": "2025-07-28T18:08:29.729849Z", "url": "https://files.pythonhosted.org/packages/1e/9c/91ec23e96d81cb39ca24670eebcd7ea119b917b595dcf572ec88847376a6/kimina_client-0.1.7.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:03Z", "published": "Mon, 28 Jul 2025 18:08:26 GMT", "package": "friTap", "version": "1.3.8.5", "json": {"info": {"author": "Daniel Baier, Francois Egner, Max Ufer", "author_email": "daniel.baier@fkie.fraunhofer.de", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: JavaScript", "Programming Language :: Python :: 3 :: Only", "Topic :: Security", "Topic :: Software Development :: Debuggers"], "description": "<div align=\"center\">\n    <img src=\"assets/logo.png\" alt=\"friTap Logo\" width=\"300\"/>\n    <p></p><strong>Real-time key extraction and traffic decryption for security research</strong></div></p>\n</div>\n\n# friTap\n![version](https://img.shields.io/badge/version-1.3.8.5-blue) [![PyPI version](https://d25lcipzij17d.cloudfront.net/badge.svg?id=py&r=r&ts=1683906897&type=6e&v=1.3.8.5&x2=0)](https://badge.fury.io/py/friTap) [![CI](https://github.com/fkie-cad/friTap/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/fkie-cad/friTap/actions/workflows/ci.yml)\n[![Ruff](https://github.com/fkie-cad/friTap/actions/workflows/lint.yml/badge.svg?branch=main)](https://github.com/fkie-cad/friTap/actions/workflows/lint.yml)\n[![Publish status](https://github.com/fkie-cad/friTap/actions/workflows/publish.yml/badge.svg?branch=main)](https://github.com/fkie-cad/friTap/actions/workflows/publish.yml)\n\nfriTap is a powerful tool designed to assist researchers in analyzing network traffic encapsulated in SSL/TLS. With its ability to automate key extraction, friTap is especially valuable when dealing with malware analysis or investigating privacy issues in applications. By simplifying the process of decrypting and inspecting encrypted traffic, friTap empowers researchers to uncover critical insights with ease.\n\nKey features include seamless support for automated SSL/TLS key extraction, making it an ideal choice for scenarios requiring rapid and accurate traffic analysis. Whether you\u2019re dissecting malicious network behavior or assessing data privacy compliance, friTap streamlines your workflow.\n\nFor more details, explore the [OSDFCon webinar slides](assets/friTapOSDFConwebinar.pdf) or check out our [blog post](https://lolcads.github.io/posts/2022/08/fritap/).\n\n\nThis project was inspired by [SSL_Logger](https://github.com/google/ssl_logger ) and currently supports all major operating systems (Linux, Windows, Android). More platforms and libraries will be added in future releases.\n\n## Key Features\n\nThe main features of friTap are:\n\n- TLS key extraction in real time (`-k key.log`)\n- Decryption of TLS payload as PCAP in real time (`-p plaintext.pcap`)\n- Library analysis and debugging (`--list-libraries`)\n- Integration with Python. [Learn more](https://github.com/fkie-cad/friTap/blob/main/INTEGRATION.md)\n- Support for custom Frida scripts. [Details](https://github.com/fkie-cad/friTap/blob/main/USAGE.md#custom-script-example)\n- Support of most common SSL libraries (OpenSSL, BoringSSL, NSS, GnuTLS, etc.)\n\n## Installation\n\nInstallation is simply a matter of `pip3 install fritap`. This will give you the `fritap` command. You can update an existing `fritap` installation with `pip3 install --upgrade fritap`.\n\n## Usage\n\nOn Linux/Windows/MacOS we can easily attach to a process by entering its name or its PID:\n\n```bash\n$ sudo fritap --pcap mycapture.pcap thunderbird\n```\n\nFor mobile applications we just have to add the `-m` parameter to indicate that we are now attaching (or spawning) an Android or iOS app:\n\n```bash\n$ fritap -m -k keys.log com.example.app\n```\n\nFurther ensure that the frida-server is running on the Android/iOS device. \n\n\nRemember when working with the pip installation you have to invoke the `fritap` command with sudo a little bit different. Either as module:\n```bash\n$ sudo -E python3 -m friTap.friTap --pcap mycapture.pcap thunderbird\n```\nor directly invoking the script:\n```bash\n$ which friTap\n/home/daniel/.local/bin/friTap\n\n$ sudo -E /home/daniel/.local/bin/friTap\n```\n\nfriTap can also be used as a Python library within your project:\n```python\nfrom friTap import SSL_Logger\n```\nFor more details on integrating friTap into your Python project, check out the [INTEGRATION.md](./INTEGRATION.md) guide.\n\nfriTap allows you to enhance its functionality by providing a custom Frida script during your session. This custom script will be invoked just before friTap applies its own hooks. To do so, use the `-c` parameter ([more](./USAGE.md#custom-script-example)).\nMore examples on using friTap can be found in the [USAGE.md](./USAGE.md). A detailed introduction using friTap on Android is under [EXAMPLE.md](./EXAMPLE.md) as well.\n\n## Hooking Libraries Without Symbols\n\nIn certain scenarios, the library we want to hook offers no symbols or is statically linked with other libraries, making it challenging to directly hook functions. For example Cronet (`libcronet.so`) and Flutter (`libflutter.so`) are often statically linked with **BoringSSL**.\n\nDespite the absence of symbols, we can still use friTap for parsing and hooking.\n\n### Hooking by Byte Patterns\n\nTo solve this, we can use friTap with byte patterns to hook the desired functions. You can provide friTap with a JSON file that contains byte patterns for hooking specific functions, based on architecture and platform using the `--patterns <byte-pattern-file.json>` option.\nIn order to apply the apprioate hooks for the various byte patterns we distinguish between different hooking categories.\nThese categories include:\n\n  -  Dump-Keys\n  -  Install-Key-Log-Callback\n  -  KeyLogCallback-Function\n  -  SSL_Read\n  -  SSL_Write\n\nEach category has a primary and fallback byte pattern, allowing flexibility when the primary pattern fails.\nFor libraries like BoringSSL, where TLS functionality is often statically linked into other binaries, we developed a tool called [BoringSecretHunter](https://github.com/monkeywave/BoringSecretHunter). This tool automatically identifies the necessary byte patterns to hook BoringSSL by byte-pattern matching. BoringSecretHunter is available as a Docker container with pre-configured Ghidra environment:\n\n```bash\n# Create directories and copy target libraries\nmkdir -p binary results\ncp /path/to/libflutter.so binary/\n\n# Run BoringSecretHunter\ndocker run --rm -v \"$(pwd)/binary\":/usr/local/src/binaries -v \"$(pwd)/results\":/host_output boringsecrethunter\n\n# Use generated patterns with friTap\nfritap --patterns results/libflutter.so_patterns.json -k keys.log target_app\n```\n\nMore about the different hooking categories can be found in [usage of byte-patterns in friTap](./USAGE.md#hooking-by-byte-patterns).\n\n### Hooking by Offsets\n\nAlternatively, you can use the `--offsets <offset-file.json>` option to hook functions using known offsets. friTap allows you to specify user-defined offsets (relative to the base address of the targeting SSL/socket library) or absolute virtual addresses for function resolution. This is done through a JSON file, which is passed using the `--offsets` parameter.\n\nIf the `--offsets` parameter is used, friTap will only overwrite the function addresses specified in the JSON file. For functions that are not specified, friTap will attempt to detect the addresses automatically (using symbols).\n\n\n## Problems\n\nThe absence of traffic or incomplete traffic capture in the resulting pcap file (-p <your.pcap>) may stem from various causes. Before submitting a new issue, consider attempting the following solutions:\n\n### Default Socket Information\n\nThere might be instances where friTap fails to retrieve socket information. In such scenarios, running friTap with default socket information (`--enable_default_fd`) could resolve the issue. This approach utilizes default socket information (127.0.0.1:1234 to 127.0.0.1:2345) for all traffic when the file descriptor (FD) cannot be used to obtain socket details:\n\n```bash\nfritap -m --enable_default_fd -p plaintext.pcap com.example.app\n```\n\n### Handling Subprocess Traffic\n\nTraffic originating from a subprocess could be another contributing factor. To capture this traffic, friTap can leverage Frida's spawn gating feature, which intercepts newly spawned processes using the `--enable_spawn_gating` parameter:\n\n```bash\nfritap -m -p log.pcap --enable_spawn_gating com.example.app\n```\n\n### Library Support exist only for Key Extraction\n\nIn cases where the target library solely supports key extraction (cf. the table below), you can utilize the `-k <key.log>` parameter alongside full packet capture:\n\n```bash\nfritap -m -p log.pcap --full_capture -k keys.log com.example.app\n```\n\n### Seeking Further Assistance\n\nIf these approaches do not address your issue, please create a detailed issue report to aid in troubleshooting. To facilitate a more effective diagnosis, include the following information in your report:\n\n- The operating system and its version\n- The specific application encountering the issue or a comparable application that exhibits similar problems\n- The output from executing friTap with the specified parameters, augmented with friTap's debug output:\n```bash\nfritap -do -v com.example.app\n```\n\n\n## Supported SSL/TLS implementations and corresponding logging capabilities\n\n```markdown\n| Library                   | Linux         | Windows       | MacOSX   | Android  | iOS          |\n|---------------------------|---------------|---------------|----------|----------|--------------|\n| OpenSSL                   |     Full      | R/W-Hook only |  TBI     |   Full   | TBI          |\n| BoringSSL                 |     Full      | R/W-Hook only |  KeyEo   |   Full   | KeyEo        |\n| NSS                       |     Full      | R/W-Hook only |  TBI     |   TBA    | TBI          |\n| GnuTLS                    | R/W-Hook only | R/W-Hook only |  TBI     |   Full   | TBI          |\n| WolfSSL                   | R/W-Hook only | R/W-Hook only |  TBI     |   Full   | TBI          |\n| MbedTLS                   | R/W-Hook only | R/W-Hook only |  TBI     |   Full   | TBI          |\n| Bouncycastle/Spongycastle |     TBA       |    TBA        |  TBA     |   Full   | TBA          |\n| Conscrypt                 |     TBA       |    TBA        |  TBA     |   Full   | TBA          |\n| S2n-tls                   |     Full      |    LibNO      |  TBA     |   Full   | LibNO        |\n| RusTLS                    |     KeyEo     |    TBI        |  TBI     |   KeyEo  | TBI          |\n```\n**R/W-Hook only** = Logging data sent and received by process<br>\n**KeyEo** = Only the keying material can be extracted<br>\n**Full** = Logging data send and received by process + Logging keys used for secure connection<br>\n**TBA** = To be answered<br>\n**TBI** = To be implemented<br>\n**LibNO** = This library is not supported for this plattform<br>\n\n**We verified the Windows implementations only for Windows 10**\n\n## Dependencies\n\n- [frida](https://frida.re) (`>= 17`)\n- `>= python3.7`\n- click (`python3 -m pip install click`)\n- hexdump (`python3 -m pip install hexdump`)\n- scapy (`python3 -m pip install scapy`)\n- watchdog (`python3 -m pip install watchdog`)\n- importlib.resources  (`python3 -m pip install importlib-resources`)\n- AndroidFridaManager (`python3 -m pip install AndroidFridaManager`)\n- for hooking on Android ensure that the `adb`-command is in your PATH\n\n## Planned features\n\n- [ ] add the capability to alter the decrypted payload\n  - integration with https://github.com/mitmproxy/mitmproxy\n  - integration with http://portswigger.net/burp/\n- [ ] add wine support\n- [x] <strike>add Flutter support</strike>\n- [ ] add further libraries (have a look at this [Wikipedia entry](https://en.wikipedia.org/wiki/Comparison_of_TLS_implementations)):\n  - Botan (BSD license, Jack Lloyd)\n  - LibreSSL (OpenBSD)\n  - Cryptlib (Peter Gutmann)\n  - JSSE (Java Secure Socket Extension, Oracle)\n  - [MatrixSSL](https://github.com/matrixssl/matrixssl) \n  - ...\n- [x] <strike>Working with static linked libraries</strike>\n- [x] <strike>Add feature to prototype TLS-Read/Write/SSLKEY functions</strike>\n- [ ] improve iOS/MacOS support (currently under development)\n\n## Development\n\n### Quick Development Setup\n\nFor developers who want to contribute to friTap, we provide an automated setup:\n\n```bash\n# Clone and setup development environment\ngit clone https://github.com/fkie-cad/friTap.git\ncd friTap\n\n# Automated setup (recommended)\npython setup_dev.py\n\n# Manual setup\npip install -r requirements-dev.txt\npip install -e .\nnpm install  # For TypeScript agent compilation\n```\n\n### Testing\n\nfriTap includes a comprehensive testing framework:\n\n```bash\n# Run all fast tests\npython run_tests.py --fast\n\n# Run specific test categories\npython run_tests.py unit           # Unit tests\npython run_tests.py agent          # Agent compilation tests  \npython run_tests.py integration    # Mock integration tests\n\n# Generate coverage report\npython run_tests.py coverage\n```\n\n### Development Dependencies\n\n- **Python 3.7+** with development dependencies (`requirements-dev.txt`)\n- **Node.js 16+** for TypeScript agent compilation\n- **Testing framework**: pytest with comprehensive mocking\n- **Code quality**: black, flake8, mypy, pre-commit hooks\n\nSee [DEVELOPMENT.md](./DEVELOPMENT.md) for detailed development setup and testing guide.\n\n## Contribute\n\nContributions are always welcome. Just fork it and open a pull request!\nMore details can be found in the [CONTRIBUTION.md](./CONTRIBUTION.md).\n___\n\n## Changelog\n\nSee the wiki for [release notes](https://github.com/fkie-cad/friTap/releases).\n\n## How to Cite friTap\n\nIf you use **friTap** in your research, please cite the following paper:\n\n> **Daniel Baier, Alexander Basse, Jan-Niclas Hilgert, Martin Lambertz**  \n> *TLS key material identification and extraction in memory: current state and future challenges*  \n> Forensic Science International: Digital Investigation, Volume 49, 2024, 301766.  \n> [https://doi.org/10.1016/j.fsidi.2024.301766](https://doi.org/10.1016/j.fsidi.2024.301766)\n\n### \ud83d\udcc4 BibTeX\n\n```bibtex\n@article{baier2024tls,\n  title={TLS key material identification and extraction in memory: current state and future challenges},\n  author={Baier, Daniel and Basse, Alexander and Hilgert, Jan-Niclas and Lambertz, Martin},\n  journal={Forensic Science International: Digital Investigation},\n  volume={49},\n  pages={301766},\n  year={2024},\n  publisher={Elsevier},\n  doi={10.1016/j.fsidi.2024.301766}\n}\n```\n\nAlternatively, you can find a citation file in `CITATION.cff` or use the \u201cCite this repository\u201d button on GitHub.\n\n## Support\n\nIf you have any suggestions, or bug reports, please create an issue in the Issue Tracker.\n\nIn case you have any questions or other problems, feel free to send an email to:\n\n[daniel.baier@fkie.fraunhofer.de](mailto:daniel.baier@fkie.fraunhofer.de).\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "Keywords", "License", "License-File", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/fkie-cad/friTap", "keywords": "mobile, instrumentation, frida, hook, SSL decryption", "license": "GPL v3", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "friTap", "package_url": "https://pypi.org/project/friTap/", "platform": null, "project_url": "https://pypi.org/project/friTap/", "project_urls": {"Homepage": "https://github.com/fkie-cad/friTap"}, "provides_extra": null, "release_url": "https://pypi.org/project/friTap/1.3.8.5/", "requires_dist": ["frida>=16.0.0", "frida-tools>=11.0.0", "AndroidFridaManager", "hexdump", "scapy", "watchdog", "click", "importlib-resources", "psutil", "rich>=13.0.0"], "requires_python": ">=3.6", "summary": "Simplifying SSL/TLS traffic analysis for researchers by making SSL/TLS decryption effortless. Decrypts and logs a process's SSL/TLS traffic on all major platforms. Further it allows the SSL/TLS key extraction.", "version": "1.3.8.5", "yanked": false, "yanked_reason": null}, "last_serial": 30388420, "urls": [{"comment_text": null, "digests": {"blake2b_256": "867f7b57fcf15fc798d58774b5258455b472071a68f8e33e66f862f3035da110", "md5": "3092b70b37d67b0d14c27277e9ece8c8", "sha256": "64efc13759aaffa8a3185fb320b771f2da29b1438c4214ed0e9188c9aabbf643"}, "downloads": -1, "filename": "fritap-1.3.8.5-py3-none-any.whl", "has_sig": false, "md5_digest": "3092b70b37d67b0d14c27277e9ece8c8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 439406, "upload_time": "2025-07-28T18:08:26", "upload_time_iso_8601": "2025-07-28T18:08:26.984031Z", "url": "https://files.pythonhosted.org/packages/86/7f/7b57fcf15fc798d58774b5258455b472071a68f8e33e66f862f3035da110/fritap-1.3.8.5-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "4c027bb47161121f92410500c75051b2c3022e164eb562dde5dadc8a40edecb9", "md5": "a60b92a41a97c46ca855a7d3cdee0fcc", "sha256": "ea5ce48016bd9cec1598d5328ac33a429ed12b922d4a41d2fc66c6b7da84913d"}, "downloads": -1, "filename": "fritap-1.3.8.5.tar.gz", "has_sig": false, "md5_digest": "a60b92a41a97c46ca855a7d3cdee0fcc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 432176, "upload_time": "2025-07-28T18:08:28", "upload_time_iso_8601": "2025-07-28T18:08:28.939648Z", "url": "https://files.pythonhosted.org/packages/4c/02/7bb47161121f92410500c75051b2c3022e164eb562dde5dadc8a40edecb9/fritap-1.3.8.5.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:07:54 GMT", "package": "dissect.cstruct", "version": "4.6.dev5", "json": {"info": {"author": null, "author_email": "Dissect Team <dissect@fox-it.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Console", "Intended Audience :: Developers", "Intended Audience :: Information Technology", "License :: OSI Approved", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Internet :: Log Analysis", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Security", "Topic :: Utilities"], "description": "# dissect.cstruct\n\nA Dissect module implementing a parser for C-like structures. Structure parsing in Python made easy. With cstruct, you\ncan write C-like structures and use them to parse binary data, either as file-like objects or bytestrings.\n\nParsing binary data with cstruct feels familiar and easy. No need to learn a new syntax or the quirks of a new parsing\nlibrary before you can start parsing data. The syntax isn't strict C but it's compatible with most common structure\ndefinitions. You can often use structure definitions from open-source C projects and use them out of the box with little\nto no changes. Need to parse an EXT4 super block? Just copy the structure definition from the Linux kernel source code.\nNeed to parse some custom file format? Write up a simple structure and immediately start parsing data, tweaking the\nstructure as you go.\n\nBy design cstruct is incredibly simple. No complex syntax, filters, pre- or post-processing steps. Just structure\nparsing. For more information, please see [the documentation](https://docs.dissect.tools/en/latest/projects/dissect.cstruct/index.html).\n\n## Requirements\n\nThis project is part of the Dissect framework and requires Python.\n\nInformation on the supported Python versions can be found in the Getting Started section of [the documentation](https://docs.dissect.tools/en/latest/index.html#getting-started).\n\n## Installation\n\n`dissect.cstruct` is available on [PyPI](https://pypi.org/project/dissect.cstruct/).\n\n```bash\npip install dissect.cstruct\n```\n\nThis module is also automatically installed if you install the `dissect` package.\n\n## Build and test instructions\n\nThis project uses `tox` to build source and wheel distributions. Run the following command from the root folder to build\nthese:\n\n```bash\ntox -e build\n```\n\nThe build artifacts can be found in the `dist/` directory.\n\n`tox` is also used to run linting and unit tests in a self-contained environment. To run both linting and unit tests\nusing the default installed Python version, run:\n\n```bash\ntox\n```\n\nFor a more elaborate explanation on how to build and test the project, please see [the\ndocumentation](https://docs.dissect.tools/en/latest/contributing/tooling.html).\n\n## Contributing\n\nThe Dissect project encourages any contribution to the codebase. To make your contribution fit into the project, please\nrefer to [the development guide](https://docs.dissect.tools/en/latest/contributing/developing.html).\n\n## Usage\nAll you need to do is instantiate a new cstruct instance and load some structure definitions in there. After that you can start using them from your Python code.\n\n```python\nfrom dissect.cstruct import cstruct\n\n# Default endianness is LE, but can be configured using a kwarg or setting the 'endian' attribute\n# e.g. cstruct(endian='>') or c_parser.endian = '>'\nparser_def = \"\"\"\n#define SOME_CONSTANT   5\n\nenum Example : uint16 {\n    A, B = 0x5, C\n};\n\nstruct some_struct {\n    uint8   field_1;\n    char    field_2[SOME_CONSTANT];\n    char    field_3[(field_1 & 1) * 5];  // Some random expression to calculate array length\n    Example field_4[2];\n};\n\"\"\"\nc_parser = cstruct().load(parser_def)\n\ndata = b\"\\x01helloworld\\x00\\x00\\x06\\x00\"\nresult = c_parser.some_struct(data)  # Also accepts file-like objects\nassert result.field_1 == 0x01\nassert result.field_2 == b\"hello\"\nassert result.field_3 == b\"world\"\nassert result.field_4 == [c_parser.Example.A, c_parser.Example.C]\n\nassert c_parser.Example.A == 0\nassert c_parser.Example.C == 6\nassert c_parser.Example(5) == c_parser.Example.B\n\nassert result.dumps() == data\n\n# You can also instantiate structures from Python by using kwargs\n# Note that array sizes are not enforced\ninstance = c_parser.some_struct(\n    field_1=5, field_2=\"lorem\", field_3=\"ipsum\", field_4=[c_parser.Example.B, c_parser.Example.A]\n)\nassert instance.dumps() == b\"\\x05loremipsum\\x05\\x00\\x00\\x00\"\n\n```\n\nBy default, all structures are compiled into classes that provide optimised performance. You can disable this by passing a `compiled=False` keyword argument to the `.load()` call. You can also inspect the resulting source code by accessing the source attribute of the structure: `print(c_parser.some_struct.source)`.\n\nMore examples can be found in the `examples` directory.\n\n## Features\n### Structure parsing\nWrite simple C-like structures and use them to parse binary data, as can be seen in the examples.\n\n### Type parsing\nAside from loading structure definitions, any of the supported types can be used individually for parsing data. For example, the following is all supported:\n\n```python\nfrom dissect.cstruct import cstruct\n\ncs = cstruct()\n# Default endianness is LE, but can be configured using a kwarg or setting the attribute\n# e.g. cstruct(endian='>') or cs.endian = '>'\nassert cs.uint32(b\"\\x05\\x00\\x00\\x00\") == 5\nassert cs.uint24[2](b\"\\x01\\x00\\x00\\x02\\x00\\x00\") == [1, 2]  # You can also parse arrays using list indexing\nassert cs.char[None](b\"hello world!\\x00\") == b\"hello world!\"  # A list index of None means null terminated\n```\n\n### Unions and nested structures\nUnions and nested structures are supported, both anonymous and named.\n\n```python\nfrom dissect.cstruct import cstruct\n\nc_def = \"\"\"\nstruct test_union {\n    char magic[4];\n    union {\n        struct {\n            uint32 a;\n            uint32 b;\n        } a;\n        struct {\n            char   b[8];\n        } b;\n    } c;\n};\n\nstruct test_anonymous {\n    char magic[4];\n    struct {\n        uint32 a;\n        uint32 b;\n    };\n    struct {\n        char   c[8];\n    };\n};\n\"\"\"\n# Default endianness is LE, but can be configured using a kwarg or setting the attribute\n# e.g. cstruct(endian='>') or cs.endian = '>'\nc = cstruct().load(c_def)\n\nassert len(c.test_union) == 12\n\na = c.test_union(b\"ohaideadbeef\")\nassert a.magic == b\"ohai\"\nassert a.c.a.a == 0x64616564\nassert a.c.a.b == 0x66656562\nassert a.c.b.b == b\"deadbeef\"\n\nassert a.dumps() == b\"ohaideadbeef\"\n\nb = c.test_anonymous(b\"ohai\\x39\\x05\\x00\\x00\\x28\\x23\\x00\\x00deadbeef\")\nassert b.magic == b\"ohai\"\nassert b.a == 1337\nassert b.b == 9000\nassert b.c == b\"deadbeef\"\n\n```\n\n### Parse bit fields\nBit fields are supported as part of structures. They are properly aligned to their boundaries.\n\n```python\nfrom dissect.cstruct import cstruct\n\nbit_def = \"\"\"\nstruct test {\n    uint16  a:1;\n    uint16  b:1;  // Read 2 bits from an uint16\n    uint32  c;    // The next field is properly aligned\n    uint16  d:2;\n    uint16  e:3;\n};\n\"\"\"\nbitfields = cstruct().load(bit_def)\n\nd = b\"\\x03\\x00\\xff\\x00\\x00\\x00\\x1f\\x00\"\na = bitfields.test(d)\n\nassert a.a == 0b1\nassert a.b == 0b1\nassert a.c == 0xFF\nassert a.d == 0b11\nassert a.e == 0b111\nassert a.dumps() == d\n```\n\n### Enums and Flags\nThe API to access enum and flag members and their values in the same way as the native Enum types in Python 3. Functionally, it's best comparable to the IntEnum or IntFlag type.\n\n### Custom types\nYou can implement your own types by subclassing `BaseType`, and adding them to your cstruct instance with `add_custom_type(name, type, size, alignment, ...)`\n\n### Custom definition parsers\nDon't like the C-like definition syntax? Write your own syntax parser!\n\n## Copyright and license\n\nDissect is released as open source by Fox-IT (<https://www.fox-it.com>) part of NCC Group Plc\n(<https://www.nccgroup.com>).\n\nDeveloped by the Dissect Team (<dissect@fox-it.com>) and made available at <https://github.com/fox-it/dissect>.\n\nLicense terms: Apache License 2.0 (<https://www.apache.org/licenses/LICENSE-2.0>). For more information, see the LICENSE file.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": null, "license": "Apache License 2.0", "license_expression": null, "license_files": ["LICENSE", "COPYRIGHT"], "maintainer": null, "maintainer_email": null, "name": "dissect.cstruct", "package_url": "https://pypi.org/project/dissect.cstruct/", "platform": null, "project_url": "https://pypi.org/project/dissect.cstruct/", "project_urls": {"changelog": "https://github.com/fox-it/dissect.cstruct/blob/main/CHANGELOG.md", "documentation": "https://docs.dissect.tools/en/latest/projects/dissect.cstruct", "homepage": "https://dissect.tools", "repository": "https://github.com/fox-it/dissect.cstruct"}, "provides_extra": ["dev"], "release_url": "https://pypi.org/project/dissect.cstruct/4.6.dev5/", "requires_dist": ["typing_extensions; extra == \"dev\""], "requires_python": "~=3.9", "summary": "A Dissect module implementing a parser for C-like structures: structure parsing in Python made easy", "version": "4.6.dev5", "yanked": false, "yanked_reason": null}, "last_serial": 30388414, "urls": [{"comment_text": null, "digests": {"blake2b_256": "e744f767a02917c1a72b48e4f5e8941bee6728ae4f407de4691cc22c13783dad", "md5": "496351e9d92f205e149f70beb37d2804", "sha256": "71c8776539de8309295a5722dfe81bfade7059d351203de9bf6ec20a5300df42"}, "downloads": -1, "filename": "dissect_cstruct-4.6.dev5-py3-none-any.whl", "has_sig": false, "md5_digest": "496351e9d92f205e149f70beb37d2804", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "~=3.9", "size": 53764, "upload_time": "2025-07-28T18:07:54", "upload_time_iso_8601": "2025-07-28T18:07:54.837855Z", "url": "https://files.pythonhosted.org/packages/e7/44/f767a02917c1a72b48e4f5e8941bee6728ae4f407de4691cc22c13783dad/dissect_cstruct-4.6.dev5-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "d28bef820591c20b3b92cfb67a6b9b22ffa7dd4482c7b19c1dd91d0d5ffad1df", "md5": "aff354b555900b156663d23c161fe989", "sha256": "599e3f5d4351935cf7bbb347ca58a95ff6f10184b7123288108d4e0e19276c70"}, "downloads": -1, "filename": "dissect_cstruct-4.6.dev5.tar.gz", "has_sig": false, "md5_digest": "aff354b555900b156663d23c161fe989", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.9", "size": 84041, "upload_time": "2025-07-28T18:07:56", "upload_time_iso_8601": "2025-07-28T18:07:56.840832Z", "url": "https://files.pythonhosted.org/packages/d2/8b/ef820591c20b3b92cfb67a6b9b22ffa7dd4482c7b19c1dd91d0d5ffad1df/dissect_cstruct-4.6.dev5.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:07:54 GMT", "package": "probely", "version": "0.0.1a12", "json": {"info": {"author": null, "author_email": "Probely <hello@probely.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Intended Audience :: Developers", "Intended Audience :: Other Audience", "Intended Audience :: System Administrators", "License :: OSI Approved :: BSD License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Software Development", "Topic :: Software Development :: Libraries"], "description": "\n# Probely\n\n## Installation\n\nProbely CLI requires Python 3.8 or higher.  \nInstall the package using pip:  \n\n```sh\npip install probely==0.0.1a11\n```\n\nThis project is currently in its alpha stage and undergoing rapid development.  \nAs a result, significant changes that could impact existing implementations may occur without notice.  \nTo maintain stability, **we strongly recommend pinning the project version you are using.**\n\n## Overview\n\nProbely is a vulnerability scanner for APIs and web applications, designed to empower security teams and software developers in scanning and securing their web applications and APIs. \n\nThis package provides a Command-Line Interface (CLI) to interact with the Probely API, allowing you to perform actions like scanning targets, listing findings, and managing targets directly from your terminal.\n\n**For detailed documentation and usage examples, please visit our [CLI documentation](https://developers.probely.com/cli/overview-cli-documentation).**\n\n## Contributing\n\nWe appreciate your interest in Probely! While we are not currently accepting external contributions, we welcome feedback and bug reports. If you encounter any issues or have suggestions, please create an issue on our GitHub repository.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "api, cli, client, probely, wrapper", "license": "BSD 3-Clause License\n        \n        Copyright (c) 2024, Probe.ly Solucoes de Ciberseguran\u00e7a LDA\n        All rights reserved.\n        \n        Redistribution and use in source and binary forms, with or without\n        modification, are permitted provided that the following conditions are met:\n        \n        1. Redistributions of source code must retain the above copyright notice, this\n           list of conditions and the following disclaimer.\n        \n        2. Redistributions in binary form must reproduce the above copyright notice,\n           this list of conditions and the following disclaimer in the documentation\n           and/or other materials provided with the distribution.\n        \n        3. Neither the name of the copyright holder nor the names of its\n           contributors may be used to endorse or promote products derived from\n           this software without specific prior written permission.\n        \n        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n        AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n        IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n        DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n        FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n        DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n        SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n        CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n        OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n        OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": "Jo\u00e3o Silva <joao.silva@probely.com>, Vladimir Kontic <vladimir@probely.com>, Temidire Adams <temidire@probely.com>", "name": "probely", "package_url": "https://pypi.org/project/probely/", "platform": null, "project_url": "https://pypi.org/project/probely/", "project_urls": {"Bug Tracker": "https://github.com/probely/probely-python/issues", "Documentation": "https://developers.probely.com", "Homepage": "https://probely.com", "Repository": "https://github.com/probely/probely-python.git"}, "provides_extra": ["dev", "docs"], "release_url": "https://pypi.org/project/probely/0.0.1a12/", "requires_dist": ["environs", "marshmallow", "mergedeep", "pydantic[email]", "python-dateutil", "pyyaml", "requests", "rich", "rich-argparse", "toml", "datamodel-code-generator; extra == \"dev\"", "hatch; extra == \"dev\"", "pre-commit; extra == \"dev\"", "pytest; extra == \"dev\"", "ruff==0.8.6; extra == \"dev\"", "tox; extra == \"dev\"", "sphinx; extra == \"docs\"", "sphinx-argparse-cli; extra == \"docs\"", "sphinx-markdown-builder; extra == \"docs\""], "requires_python": ">=3.8", "summary": "CLI wrapper for Probely's API", "version": "0.0.1a12", "yanked": false, "yanked_reason": null}, "last_serial": 30388413, "urls": [{"comment_text": null, "digests": {"blake2b_256": "b1d927ba0982c741060c01e232440ace00ccc330df8cfdb765b94a965272376d", "md5": "7a31d86b02d92b14a3c88906cba273ff", "sha256": "5a39f912ac0a3c71341fd4011ffb707e0db7119d39251e5c0755180e9f8a63a6"}, "downloads": -1, "filename": "probely-0.0.1a12-py3-none-any.whl", "has_sig": false, "md5_digest": "7a31d86b02d92b14a3c88906cba273ff", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 91904, "upload_time": "2025-07-28T18:07:56", "upload_time_iso_8601": "2025-07-28T18:07:56.059974Z", "url": "https://files.pythonhosted.org/packages/b1/d9/27ba0982c741060c01e232440ace00ccc330df8cfdb765b94a965272376d/probely-0.0.1a12-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "e63abdf9e704658eb9d92fade738eb83feb69c168a80ec2b82d8e59be77286b5", "md5": "3e2cbefc51086720d919d86aafe71bc8", "sha256": "4456e4af33353177fc545c893044ea98225860b5c270dbc3a9d3f911f816ad3f"}, "downloads": -1, "filename": "probely-0.0.1a12.tar.gz", "has_sig": false, "md5_digest": "3e2cbefc51086720d919d86aafe71bc8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 50080, "upload_time": "2025-07-28T18:07:54", "upload_time_iso_8601": "2025-07-28T18:07:54.617523Z", "url": "https://files.pythonhosted.org/packages/e6/3a/bdf9e704658eb9d92fade738eb83feb69c168a80ec2b82d8e59be77286b5/probely-0.0.1a12.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:07:33 GMT", "package": "meridianalgo", "version": "0.3.2", "json": {"info": {"author": "MeridianAlgo", "author_email": "meridianalgo@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Financial and Insurance Industry", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Office/Business :: Financial :: Investment", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# \ud83d\ude80 MeridianAlgo - Advanced Stock Prediction System\r\n\r\n[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\r\n[![PyPI version](https://badge.fury.io/py/meridianalgo.svg)](https://badge.fury.io/py/meridianalgo)\r\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n[![Yahoo Finance](https://img.shields.io/badge/Data-Yahoo%20Finance-purple.svg)](https://finance.yahoo.com/)\r\n\r\n**Advanced AI-powered stock prediction system using Yahoo Finance - Zero setup, no API keys required!**\r\n\r\n## \u26a1 Quick Start\r\n\r\n```bash\r\npip install meridianalgo\r\n```\r\n\r\n```python\r\nfrom meridianalgo import MLPredictor\r\n\r\n# Initialize predictor (no API keys needed!)\r\npredictor = MLPredictor()\r\n\r\n# Get predictions for any stock\r\nresult = predictor.predict_ml('AAPL', days=60, epochs=10)\r\n\r\nprint(f\"Current Price: ${result['current_price']:.2f}\")\r\nprint(f\"Day +1 Prediction: ${result['predictions'][0]:.2f}\")\r\nprint(f\"Confidence: {result['confidence']:.1f}%\")\r\n```\r\n\r\n## \ud83c\udfaf Key Features\r\n\r\n- **\ud83c\udd93 Zero Setup**: No API keys, no registration - uses free Yahoo Finance data\r\n- **\ud83e\udde0 Advanced AI**: 62 sophisticated features with deep neural networks\r\n- **\u26a1 Real-Time Learning**: Automated accuracy validation and model adaptation\r\n- **\ud83d\udcca Comprehensive Analysis**: Technical indicators, market sentiment, and volatility analysis\r\n- **\ud83d\udd04 Smart Caching**: Intelligent prediction caching to avoid redundant analysis\r\n- **\ud83d\udcc8 Multi-Symbol Support**: Analyze any stock symbol with persistent data storage\r\n- **\ud83d\udee1\ufe0f Prediction Validation**: Multi-tier accuracy system with intelligent failsafes\r\n\r\n## \ud83d\udcca System Performance\r\n\r\n```\r\n\ud83c\udfaf Data Source: Yahoo Finance (Free, Real-time)\r\n\ud83d\udd27 Setup Time: 0 seconds (no API keys required)\r\n\ud83d\udcc8 Features: 62 advanced technical indicators\r\n\ud83e\udde0 Model: Deep neural networks with attention mechanisms\r\n\u26a1 Speed: Instant analysis with smart caching\r\n\ud83d\udd04 Learning: Continuous model improvement\r\n```\r\n\r\n## \ud83d\udcbb Usage Examples\r\n\r\n### Basic Stock Prediction\r\n\r\n```python\r\nfrom meridianalgo import MLPredictor\r\n\r\npredictor = MLPredictor()\r\n\r\n# Simple prediction\r\nresult = predictor.predict_simple('NVDA', days=30)\r\nprint(f\"NVIDIA Prediction: ${result['predictions'][0]:.2f}\")\r\n\r\n# Advanced ML prediction\r\nresult = predictor.predict_ml('TSLA', days=60, epochs=15)\r\nprint(f\"Tesla Confidence: {result['confidence']:.1f}%\")\r\n```\r\n\r\n### Technical Analysis\r\n\r\n```python\r\nfrom meridianalgo import Indicators\r\n\r\nindicators = Indicators()\r\n\r\n# Calculate technical indicators\r\ndata = indicators.get_stock_data('AAPL', period='1y')\r\nrsi = indicators.calculate_rsi(data['Close'])\r\nmacd = indicators.calculate_macd(data['Close'])\r\n\r\nprint(f\"Current RSI: {rsi[-1]:.2f}\")\r\nprint(f\"MACD Signal: {macd['signal'][-1]:.4f}\")\r\n```\r\n\r\n### Ensemble Models\r\n\r\n```python\r\nfrom meridianalgo import EnsembleModels\r\n\r\nensemble = EnsembleModels()\r\n\r\n# Train ensemble models\r\ndata = ensemble.get_training_data('GOOGL', days=90)\r\ntraining_results = ensemble.train_ensemble(data['X'], data['y'], epochs=20)\r\n\r\n# Make predictions\r\npredictions = ensemble.predict_ensemble(data['X'][-1:], forecast_days=5)\r\nprint(f\"5-day predictions: {predictions['ensemble_predictions']}\")\r\n```\r\n\r\n## \ud83d\udee1\ufe0f Prediction Validation & Failsafes\r\n\r\n### Multi-Tier Accuracy System\r\n- **\ud83c\udfaf Excellent (<1% error)**: Highest quality predictions\r\n- **\u2705 Good (<2% error)**: Strong prediction reliability\r\n- **\u26a0\ufe0f Acceptable (<3% error)**: Minimum acceptable threshold\r\n- **\u274c Poor (>3% error)**: Triggers conservative fallback system\r\n\r\n### Intelligent Failsafes\r\n1. **Extreme Change Detection**: Flags predictions >50% change as unreliable\r\n2. **Consistency Validation**: Ensures smooth day-to-day prediction transitions\r\n3. **Confidence Thresholds**: Requires minimum 60% model confidence\r\n4. **Volatility Context**: Adjusts expectations based on stock stability\r\n5. **Volume Validation**: Considers trading volume for prediction reliability\r\n6. **Conservative Fallbacks**: Applies ultra-safe predictions when validation fails\r\n\r\n## \ud83d\udce6 Installation\r\n\r\n### Standard Installation\r\n```bash\r\npip install meridianalgo\r\n```\r\n\r\n### With ML Dependencies\r\n```bash\r\npip install meridianalgo[ml]\r\n```\r\n\r\n### With Visualization\r\n```bash\r\npip install meridianalgo[visualization]\r\n```\r\n\r\n### Full Installation\r\n```bash\r\npip install meridianalgo[ml,visualization]\r\n```\r\n\r\n## \ud83d\udd27 System Requirements\r\n\r\n- **Python**: 3.8 or higher\r\n- **Memory**: 4GB RAM minimum, 8GB recommended\r\n- **Storage**: 500MB free space\r\n- **Internet**: Required for real-time stock data\r\n- **OS**: Windows, macOS, Linux\r\n\r\n## \ud83d\udcda API Reference\r\n\r\n### MLPredictor Class\r\n\r\n```python\r\nclass MLPredictor:\r\n    def predict_simple(self, symbol: str, days: int = 60, forecast_days: int = 5) -> Dict\r\n    def predict_ml(self, symbol: str, days: int = 60, epochs: int = 10, forecast_days: int = 5) -> Dict\r\n```\r\n\r\n### Indicators Class\r\n\r\n```python\r\nclass Indicators:\r\n    def calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series\r\n    def calculate_macd(self, prices: pd.Series) -> Dict\r\n    def calculate_bollinger_bands(self, prices: pd.Series, period: int = 20) -> Dict\r\n```\r\n\r\n### EnsembleModels Class\r\n\r\n```python\r\nclass EnsembleModels:\r\n    def train_ensemble(self, X: np.ndarray, y: np.ndarray, epochs: int = 10) -> Dict\r\n    def predict_ensemble(self, X: np.ndarray, forecast_days: int = 5) -> Dict\r\n```\r\n\r\n## \ud83c\udfaf Advanced Features\r\n\r\n### 62 Technical Features\r\n1. **Market Microstructure**: VWAP, price ranges, volume-price trends\r\n2. **Multi-Timeframe Momentum**: 7 different time horizons (1, 2, 3, 5, 8, 13, 21 days)\r\n3. **Advanced Volatility**: GARCH-like modeling with clustering\r\n4. **Market Regime Detection**: Trend strength and mean reversion\r\n5. **Fractal Analysis**: Hurst exponent and fractal dimensions\r\n6. **Technical Patterns**: Support/resistance and breakout probability\r\n\r\n### Neural Network Architecture\r\n- **Multi-scale feature extraction** with 1024, 512, 256-dim extractors\r\n- **16-head attention mechanism** for pattern recognition\r\n- **6 deep transformer blocks** for sequential processing\r\n- **7 prediction heads** with uncertainty quantification\r\n- **Advanced weight initialization** for optimal convergence\r\n\r\n## \ud83d\udcca Data Sources & Privacy\r\n\r\n### Data Sources\r\n- **Stock Data**: Yahoo Finance (yfinance)\r\n- **Technical Indicators**: Custom implementations\r\n- **Market Data**: Real-time price feeds\r\n- **Validation**: Historical price verification\r\n\r\n### Privacy & Security\r\n- **No personal data** collection\r\n- **Local processing** only\r\n- **No data transmission** except for stock price fetching\r\n- **Open source** and transparent\r\n\r\n## \ud83e\udd1d Contributing\r\n\r\nWe welcome contributions! Please see our [Contributing Guide](https://github.com/MeridianAlgo/Packages/blob/main/CONTRIBUTING.md) for details.\r\n\r\n## \ud83d\udcc4 License\r\n\r\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\r\n\r\n## \ud83d\ude4f Acknowledgments\r\n\r\n- **PyTorch** team for the ML framework\r\n- **Yahoo Finance** for stock data API\r\n- **Rich** library for beautiful terminal output\r\n- **Open source community** for inspiration and tools\r\n\r\n---\r\n\r\n**\u26a1 Ready to start predicting stocks with zero setup? Install now!**\r\n\r\n```bash\r\npip install meridianalgo\r\n```\r\n\r\n**\ud83c\udfaf Advanced predictions with rigorous validation and intelligent failsafes!**\r\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "Keywords", "License-File", "Project-Url", "Provides-Extra", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/MeridianAlgo/Packages", "keywords": "stock prediction, yahoo finance, machine learning, AI, financial analysis, no api keys, zero setup", "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "meridianalgo", "package_url": "https://pypi.org/project/meridianalgo/", "platform": null, "project_url": "https://pypi.org/project/meridianalgo/", "project_urls": {"Bug Reports": "https://github.com/MeridianAlgo/Packages/issues", "Documentation": "https://github.com/MeridianAlgo/Packages#readme", "Homepage": "https://github.com/MeridianAlgo/Packages", "Source": "https://github.com/MeridianAlgo/Packages"}, "provides_extra": ["dev"], "release_url": "https://pypi.org/project/meridianalgo/0.3.2/", "requires_dist": ["pandas>=1.5.0", "numpy>=1.21.0", "python-dateutil>=2.8.0", "yfinance>=0.2.0", "scikit-learn>=1.3.0", "python-dotenv>=1.0.0", "rich>=13.0.0", "scipy>=1.9.0", "pytest>=6.0; extra == \"dev\"", "pytest-cov>=2.0; extra == \"dev\"", "black>=21.0; extra == \"dev\"", "flake8>=3.8; extra == \"dev\"", "mypy>=0.800; extra == \"dev\""], "requires_python": ">=3.8", "summary": "Advanced stock prediction system using Yahoo Finance - Zero setup, no API keys required", "version": "0.3.2", "yanked": false, "yanked_reason": null}, "last_serial": 30388408, "urls": [{"comment_text": null, "digests": {"blake2b_256": "3b78bfc8809f9692fe76436b43aed26f8ec71f0dc50c7fda299cd213d654d427", "md5": "b8b88fa62752d8019cabd5a5327cb603", "sha256": "f0901661ad299c2f660816a62d2fd69d170b78fd83c8786ef3bebbb0cb3b8eb9"}, "downloads": -1, "filename": "meridianalgo-0.3.2-py3-none-any.whl", "has_sig": false, "md5_digest": "b8b88fa62752d8019cabd5a5327cb603", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 30189, "upload_time": "2025-07-28T18:07:33", "upload_time_iso_8601": "2025-07-28T18:07:33.594306Z", "url": "https://files.pythonhosted.org/packages/3b/78/bfc8809f9692fe76436b43aed26f8ec71f0dc50c7fda299cd213d654d427/meridianalgo-0.3.2-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "a74344833e0eae559562cbe3792b4d9b066e1027c05e45188c75588e5c49eb37", "md5": "0b422cb73468221b6ab1f278b905549d", "sha256": "3ccac440ad458f0da7ca392e8ee91507bb5c1d68fcfc8ab2a76d7689c42a20f1"}, "downloads": -1, "filename": "meridianalgo-0.3.2.tar.gz", "has_sig": false, "md5_digest": "0b422cb73468221b6ab1f278b905549d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 39335, "upload_time": "2025-07-28T18:07:34", "upload_time_iso_8601": "2025-07-28T18:07:34.713402Z", "url": "https://files.pythonhosted.org/packages/a7/43/44833e0eae559562cbe3792b4d9b066e1027c05e45188c75588e5c49eb37/meridianalgo-0.3.2.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:07:18 GMT", "package": "sense-table", "version": "0.0.1rc9", "json": {"info": {"author": null, "author_email": null, "bugtrack_url": null, "classifiers": [], "description": "# SenseTable\n\n![SenseTable](./sense_table/statics/SenseTable-light.svg)\n\n\nSenseTable helps you explore large-volumn of multi-modal AI data easily.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "sense-table", "package_url": "https://pypi.org/project/sense-table/", "platform": null, "project_url": "https://pypi.org/project/sense-table/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/sense-table/0.0.1rc9/", "requires_dist": ["boto3>=1.39.11", "click>=8.1.8", "duckdb>=1.2.1", "flask>=3.1.0", "pydantic>=2.11.7", "pytz>=2025.2", "requests>=2.32.3"], "requires_python": ">=3.9", "summary": "Smoothly make sense of your large-scale multi-modal tabular data", "version": "0.0.1rc9", "yanked": false, "yanked_reason": null}, "last_serial": 30388405, "urls": [{"comment_text": null, "digests": {"blake2b_256": "0bff4ef6c8997302860c7f88943cb89da564f1ec00c0d0f38aa4b9150e220049", "md5": "0579b4c05ece1ec080268d6a75f91f9a", "sha256": "17cdea4552640257fad72dab15f2bfbebd5ff8c05258af6d87b632da7c4d645d"}, "downloads": -1, "filename": "sense_table-0.0.1rc9-py3-none-any.whl", "has_sig": false, "md5_digest": "0579b4c05ece1ec080268d6a75f91f9a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 2640095, "upload_time": "2025-07-28T18:07:18", "upload_time_iso_8601": "2025-07-28T18:07:18.389362Z", "url": "https://files.pythonhosted.org/packages/0b/ff/4ef6c8997302860c7f88943cb89da564f1ec00c0d0f38aa4b9150e220049/sense_table-0.0.1rc9-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "0ad6cfa28ffdb09ad3f5e46c2aea37cc5a1d48a632cbe3061f7b9127bb31fa41", "md5": "70087f51143ce27a368b7783e650352e", "sha256": "a8f4c70074343187a3bed0a884321b896e532fb303b8e239996db5d92c13be5c"}, "downloads": -1, "filename": "sense_table-0.0.1rc9.tar.gz", "has_sig": false, "md5_digest": "70087f51143ce27a368b7783e650352e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 2610104, "upload_time": "2025-07-28T18:07:20", "upload_time_iso_8601": "2025-07-28T18:07:20.979724Z", "url": "https://files.pythonhosted.org/packages/0a/d6/cfa28ffdb09ad3f5e46c2aea37cc5a1d48a632cbe3061f7b9127bb31fa41/sense_table-0.0.1rc9.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:07:00 GMT", "package": "orthoxml-tools", "version": "1.1.1", "json": {"info": {"author": null, "author_email": "Ali Yazdizadeh <aliyzd1379@gmail.com>", "bugtrack_url": null, "classifiers": [], "description": "# orthoxml-tools\n\nTools for working with OrthoXML files.\n\n## What is OrthoXML Format?\n\n> OrthoXML is a standard for sharing and exchanging orthology predictions. OrthoXML is designed broadly to allow the storage and comparison of orthology data from any ortholog database. It establishes a structure for describing orthology relationships while still allowing flexibility for database-specific information to be encapsulated in the same format.  \n> [OrthoXML](https://github.com/qfo/orthoxml/tree/main)\n\n# Installation\n\n```\npip install orthoxml-tools\n```\n\n# Usage\n\n```bash\northoxml-tools [options] <subcommand> [options]\n```\n\n> Note: Input OrthoXML files can be in plain text or compressed format. Both gzip (.gz) and bzip2 (.bz2) compression are supported.\n\n## Subcommands\n\n### \ud83d\udee0\ufe0f **validate**\nValidate an OrthoXML file against the schema version specified in the file itself.\n\n```bash\northoxml-tools validate --infile path/to/file.orthoxml\n```\n\n**Options:**\n- `--infile <file>`: Specify the input file (required).\n\n**Example:**\n```bash\northoxml-tools validate --infile examples/data/ex1.orthoxml\n```\n\n### \ud83d\udee0\ufe0f **stats**\nDisplay basic statistics.\n\n```bash\northoxml-tools stats --infile path/to/file.orthoxml [--outfile <file>] \n```\n\n**Options:**\n- `--infile <file>`: Specify the input file (required).\n\n**Example:**\n```bash\northoxml-tools stats --infile examples/data/ex3-int-taxon.orthoxml\n```\n\n### \ud83d\udee0\ufe0f **gene-stats**\nDisplay statistics for gene count per taxon.\n\n```bash\northoxml-tools gene-stats --infile path/to/file.orthoxml [--outfile <file>]\n```\n\n**Options:**\n- `--infile <file>`: Specify the input file (required).\n- `--outfile <file>`: Write stats to a txt file.\n\n**Example:**\n```bash\northoxml-tools gene-stats --infile examples/data/ex3-int-taxon.orthoxml --outfile gene_stats.txt\n```\n\n### \ud83d\udee0\ufe0f **filter**\nFilter orthology groups based on CompletenessScore score and a threshold and strategy.\n\n```bash\northoxml-tools filter --infile path/to/file.orthoxml --threshold <value> --strategy <cascade-remove|extract|reparent> --outfile <file>\n```\n\n**Options:**\n- `--infile <file>`: Specify the input file. (required)\n- `--threshold <value>`: Set the threshold for filtering. value below this will be removed. (required)\n- `--strategy <cascade-remove|extract|reparent>`: Choose the filtering strategy (default is `cascade-remove`).\n- `--outfile <file>`: Save output to a file. if not specified, the output will be printed to stdout. (required)\n\n\n**Examples:**\n```bash\n orthoxml-tools filter --infile examples/data/sample-for-filter.orthoxml --score-name CompletenessScore --strategy top-down --threshold 0.24 --outfile tests_output/filtered_stream.orthoxml\n```\n\n### \ud83d\udee0\ufe0f **taxonomy**\nPrint a human-readable taxonomy tree from the OrthoXML file.\n\n```bash\northoxml-tools taxonomy --infile path/to/file.orthoxml\n```\n\n**Example:**\n```bash\n>>> orthoxml-tools taxonomy --infile examples/data/ex3-int-taxon.orthoxml\nRoot\n\u251c\u2500\u2500 Mus musculus\n\u2514\u2500\u2500 Primates\n    \u251c\u2500\u2500 Homo sapiens\n    \u2514\u2500\u2500 Pan troglodytes\n```\n\n### \ud83d\udee0\ufe0f **export-pairs**\nExport pairs (orthologs or paralogs) in TSV form, with configurable chunking and buffering.\n\n```bash\northoxml-tools export-pairs <ortho|para> \\\n    --infile <file> \\\n    --outfile <file> \\\n    [--id <tag>] \\\n    [--chunk-size <number>] \\\n    [--buffer-size <bytes>]\n```\n\n**Positional arguments:**\n<ortho|para>\nChoose which pair type to export:\n- `ortho`: orthologous pairs\n- `para`: paralogous pairs\n\n**Options:**\n- `--infile <file>`: Input OrthoXML file (required).\n- `--outfile <file>`: Write output CSV to this file (required).\n- `--id <tag>`: Gene attribute to use as identifier (default: id).\n- `--chunk-size <number>`: Number of pairs to process per chunk (default: 20_000).\n- `--buffer-size <bytes>`: I/O buffer size in bytes (default: 4194304).\n\n**Examples:**\n\n```bash\n# [5.1] Export ortholog pairs with default chunk & buffer sizes\northoxml-tools export-pairs ortho \\\n    --infile examples/data/ex1-int-taxon.orthoxml \\\n    --outfile orthos.csv\n\n# [5.2] Export paralog pairs with default chunk & buffer sizes\northoxml-tools export-pairs para \\\n    --infile examples/data/ex1-int-taxon.orthoxml \\\n    --outfile paras.csv\n\n# [5.3] Export ortholog pairs using `geneId` as the identifier column\northoxml-tools export-pairs ortho \\\n    --infile examples/data/ex1-int-taxon.orthoxml \\\n    --outfile orthos_geneid.csv \\\n    --id geneId\n\n# [5.4] Export ortholog pairs with custom chunk and buffer sizes\northoxml-tools export-pairs ortho \\\n    --infile examples/data/ex1-int-taxon.orthoxml \\\n    --outfile orthos_custom.csv \\\n    --chunk-size 5000 \\\n    --buffer-size 1048576\n```\n\n\n### \ud83d\udee0\ufe0f **export-ogs**\nExport Orthologous Groups as TSV file.\n\n```bash\northoxml-tools export-ogs --infile path/to/file.orthoxml --outfile path/to/output.tsv [--id <tag>]\n```\n\n**Options:**\n- `--infile <file>`: Input OrthoXML file (required).\n- `--outfile <file>`: Write output CSV to this file (required).\n- `--id <tag>`: Gene attribute to use as identifier (default: id).\n\n**Examples:**\n```bash\northoxml-tools export-ogs --infile examples/data/sample-for-og.orthoxml --outfile tests_output/ogs.tsv --id protId\n```\n\n### \ud83d\udee0\ufe0f **split**\nSplit the tree into multiple trees based on rootHOGs.\n\n```bash\northoxml-tools split --infile path/to/file.orthoxml --outdir path/to/output_folder\n```\n\n**Options:**\n- `--infile <file>`: Specify the input OrthoXML file (required).\n- `--outdir <folder>`: Specify the output folder where the trees will be saved.\n- \n**Examples:**\n```bash\northoxml-tools split --infile examples/data/ex4-int-taxon-multiple-rhogs.orthoxml --outdir tests_output/splits\n```\n\n## File Conversions\n\n### \ud83d\udee0\ufe0f **OrthoXML to Newick Tree (NHX)**\nConvert OrthoXML to Newick (NHX) format.\n\n```bash\northoxml-tools to-nhx --infile path/to/file.orthoxml --outdir path/to/output_folder --xref-tag [geneId,protId,...]    \n```\n\n**Options:**\n- `--infile <file>`: Specify the input OrthoXML file (required).\n- `--outdir <folder>`: Specify the output folder where the NHX files will be saved (required).\n- `--xref-tag <tag>`: Specify the attribute of the `<gene>` element to use as the label for the leaves. Default is `protId`.\n- `--encode-levels`: If set, encode group levels as NHX comments in the output tree. This is useful for visualizing the hierarchy of orthologous groups.\n  \n**Example:**\n```bash\northoxml-tools to-nhx --infile examples/data/sample-for-nhx.orthoxml --outdir ./tests_output/trees --xref-tag protId --encode-levels\n```\n\n### \ud83d\udee0\ufe0f **Newick Tree (NHX) to OrthoXML**\nConvert Newick (NHX) format to OrthoXML.\n\n```bash\northoxml-tools from-nhx --infile path/to/file.nhx --outfile path/to/file.orthoxml\n```\n\n**Options:**\n- `--infile <file>`: Specify the input nhx file or files. (at least one file is required).\n  - You can specify multiple files by providing them as a space-separated list.\n  - If you provide multiple files, they will be combined into a single OrthoXML output.\n- `--outfile <folder>`: Specify the output OrthoXML file (required).\n\n**Example:**\n```bash\northoxml-tools from-nhx --infile examples/data/sample.nhx --outfile ./tests_output/from_nhx.orthoxml\northoxml-tools from-nhx --infile examples/data/sample2.nhx examples/data/sample.nhx --outfile ./tests_output/from_nhx21.orthoxml \n```\n\n### \ud83d\udee0\ufe0f CSV to OrthoXML (exploratory feature)\nConvert a CSV file to OrthoXML. The CSV file is structured such that each row represents an orthogroup (OG), each column corresponds to a species, and each cell contains a gene name. This format is generated by OrthoFinder e.g. `examples/data/InputOrthogroups.csv`.\n\n> [!WARNING]\n> Note that since the CSV does not contain the full information required to represent the hierarchical structure of HOGs, the output OrthoXML file is reported at the root level. It should not be considered a full-fledged OrthoXML file.\n\n```bash\northoxml-tools from-csv --infile path/to/file.csv --outfile path/to/file.orthoxml\n```\n\n**Options:**\n- `--infile <file>`: Specify the input orthogroups.csv file (required).\n- `--outfile <folder>`: Specify the output OrthoXML file (required).\n\n**Example:**\n```bash\northoxml-tools from-csv --infile examples/data/InputOrthogroups.csv --outfile tests_output/orthofinder.orthoxml\n```\n\n\n### \ud83d\udee0\ufe0f **filter**\nFilter the OrthoXML tree by a completeness score. \n\n- `--score-name <str>`: Name of the field for completeness score annotation (e.g. 'CompletenessScore') \n- `--threshold <float>`: Threshold value for the completeness score\n- `--strategy <bottomup|topdown>`: Filtering strategy. Bottom-up will keep complete subHOGs even if they parents are incomplete.\n- `--outfile <file>`: If provided, write the filtered OrthoXML to this file; otherwise, print to stdout\n\n```bash\northoxml-tools tests/test-data/case_filtering.orthoxml filter --score-name CompletenessScore \\\n                                                        --threshold 0.75 \\\n                                                        --strategy bottomup \\\n                                                        --outfile output-oxml.orthoxml \n```\n\n### **Help**\nTo see help for any command:\n\n```bash\northoxml-tools --help\northoxml-tools -h\northoxml-tools stats --help\northoxml-tools stats -h\n```\n\n## Legacy API\n\nThe `orthoxml-tools` package used to provides a object oriented interface for working with OrthoXML files. This API is deprecated and will be removed in v1.0.0. Please use the new streaming CLI method. The documentation on it can be found [here](LEGACY-README.md).\n\n## Testing\n\n```\nuv install `.[test]`\npytest -vv\n\n# test cli\ntests/test_cli.sh\n```\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "orthoxml-tools", "package_url": "https://pypi.org/project/orthoxml-tools/", "platform": null, "project_url": "https://pypi.org/project/orthoxml-tools/", "project_urls": null, "provides_extra": ["test"], "release_url": "https://pypi.org/project/orthoxml-tools/1.1.1/", "requires_dist": ["dendropy>=5.0.8", "lxml>=5.3.0", "pytest-cov>=3.0.0; extra == \"test\"", "pytest>=7.0.0; extra == \"test\""], "requires_python": ">=3.9", "summary": "Tools for working with OrthoXML files.", "version": "1.1.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388401, "urls": [{"comment_text": null, "digests": {"blake2b_256": "3a0bae9c6fd051ec56adc73140e628aec9e1aab4a9334a805df2336a478e1f7c", "md5": "4027cac3b66f910e822780f2be70b468", "sha256": "867eb2d16deae52a81ac51d7c14035569edb2374dc429f7a1c44d6b10109b947"}, "downloads": -1, "filename": "orthoxml_tools-1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "4027cac3b66f910e822780f2be70b468", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 62900, "upload_time": "2025-07-28T18:07:00", "upload_time_iso_8601": "2025-07-28T18:07:00.235621Z", "url": "https://files.pythonhosted.org/packages/3a/0b/ae9c6fd051ec56adc73140e628aec9e1aab4a9334a805df2336a478e1f7c/orthoxml_tools-1.1.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "7409e607164b931bd31b86ce9d2c3168fed5ca93bc9271d51c402bfb3325cb2a", "md5": "725917b647201c6e002a2c217ff4cae3", "sha256": "56356812b3a716a3d15fa30b69fbe953ac1d3d5b5448cad0b9c3ad7d2e4fb7ed"}, "downloads": -1, "filename": "orthoxml_tools-1.1.1.tar.gz", "has_sig": false, "md5_digest": "725917b647201c6e002a2c217ff4cae3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 15044235, "upload_time": "2025-07-28T18:07:01", "upload_time_iso_8601": "2025-07-28T18:07:01.506393Z", "url": "https://files.pythonhosted.org/packages/74/09/e607164b931bd31b86ce9d2c3168fed5ca93bc9271d51c402bfb3325cb2a/orthoxml_tools-1.1.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:06:36 GMT", "package": "wyrdbound-dice", "version": "0.0.1", "json": {"info": {"author": null, "author_email": "The Wyrd One <wyrdbound@proton.me>", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "Intended Audience :: End Users/Desktop", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Games/Entertainment :: Role-Playing", "Topic :: Scientific/Engineering :: Mathematics", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# Wyrdbound Dice\n\nA comprehensive dice rolling library for tabletop RPGs, designed to handle complex dice expressions with mathematical precision and extensive system support.\n\nThis library is designed for use in [wyrdbound](https://github.com/wyrdbound), a text-based RPG system that emphasizes narrative and player choice.\n\n[![CI](https://github.com/wyrdbound/wyrdbound-dice/actions/workflows/ci.yml/badge.svg)](https://github.com/wyrdbound/wyrdbound-dice/actions/workflows/ci.yml)\n[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n> \ud83d\udce3 This library is experimental and was built with much :heart: and [vibe coding](https://en.wikipedia.org/wiki/Vibe_coding). Please do not launch :rocket: or perform :brain: surgery using it. (Should be :a:-:ok: for your Table-Top application though!)\n\n## Features\n\nWyrdbound Dice supports an extensive range of dice rolling mechanics used across many tabletop RPG systems:\n\n### Basic Dice Rolling\n\n- **Standard polyhedral dice**: `1d4`, `1d6`, `1d8`, `1d10`, `1d12`, `1d20`, `1d100`\n- **Multiple dice**: `3d6`, `4d8`, etc.\n- **Percentile dice**: `1d%` (displays as [tens, ones])\n\n### Mathematical Operations\n\n- **Arithmetic operations**: `2d6 + 3`, `1d20 - 2`, `1d6 \u00d7 4`, `1d10 \u00f7 2`\n- **Complex expressions**: `2d6 + 1d4 \u00d7 2 - 1`\n- **Proper precedence**: Mathematical order of operations (PEMDAS/BODMAS)\n- **Unicode operators**: Support for `\u00d7`, `\u00f7`, `\u2212`, and fullwidth characters\n\n### Keep/Drop Mechanics\n\n- **Keep highest**: `4d6kh3` (ability score generation), `2d20kh1` (advantage)\n- **Keep lowest**: `4d6kl3`, `2d20kl1` (disadvantage)\n- **Drop operations**: `4d6dh1` (drop highest), `4d6dl1` (drop lowest)\n- **Multiple operations**: `5d6kh3kl1` (chain keep/drop operations)\n\n### Reroll Mechanics\n\n- **Unlimited rerolls**: `1d6r<=2` (reroll while \u2264 2)\n- **Limited rerolls**: `1d6r1<=2` (reroll once), `1d6r3<=3` (reroll up to 3 times)\n- **Comparison operators**: `<=`, `<`, `>=`, `>`, `=`\n- **Alternate notation**: `1d6ro<=2` (reroll once)\n\n### Exploding Dice\n\n- **Simple explosion**: `1d6e` (explode on max value)\n- **Explicit threshold**: `1d6e6`, `1d10e>=8`\n- **Custom conditions**: `1d6e>=5` (explode on 5 or 6)\n- **Multiple explosions**: Dice can explode repeatedly\n\n### Fudge Dice (Fate Core/Accelerated)\n\n- **Single Fudge die**: `1dF` (results: -1, 0, +1)\n- **Standard Fate roll**: `4dF`\n- **Symbol display**: Shows as `-, B, +`\n- **Math operations**: Can be combined with other dice and modifiers\n\n### System Shorthands\n\n- **FUDGE**: `4dF` (Fate Core)\n- **BOON**: `3d6kh2` (Traveller advantage)\n- **BANE**: `3d6kl2` (Traveller disadvantage)\n- **FLUX**: `1d6 - 1d6` (Traveller flux)\n- **GOODFLUX**: Always positive flux (highest 1d6 - lowest 1d6)\n- **BADFLUX**: Always negative flux (lowest 1d6 - highest 1d6)\n- **PERC / PERCENTILE**: `1d%`\n\n### Named Modifiers\n\n- **Static modifiers**: `{\"Strength\": 3, \"Proficiency\": 2}`\n- **Dice modifiers**: `{\"Guidance\": \"1d4\", \"Bane\": \"-1d4\"}`\n- **Mixed modifiers**: Combine static numbers and dice expressions\n\n### Advanced Features\n\n- **Zero dice handling**: `0d6` returns 0\n- **Negative dice**: `-1d6` returns negative result\n- **Thread safety**: Safe for concurrent use\n- **Error handling**: Clear exceptions for invalid conditions\n- **Infinite condition detection**: Prevents impossible reroll/explode scenarios\n\n## Installation\n\n### For End Users\n\n```bash\npip install wyrdbound-dice\n```\n\n> **Note**: This package is currently in development and not yet published to PyPI. For now, please use the development installation method below.\n\n### For Development\n\nIf you want to contribute to the project or use the latest development version:\n\n```bash\n# Clone the repository\ngit clone https://github.com/wyrdbound/wyrdbound-dice.git\ncd wyrdbound-dice\n\n# Install in development mode\npip install -e .\n```\n\n### Optional Dependencies\n\nFor visualization features (graph tool):\n\n```bash\npip install \"wyrdbound-dice[visualization]\"\n```\n\nFor development:\n\n```bash\npip install -e \".[dev]\"\n```\n\nFor both visualization and development:\n\n```bash\npip install -e \".[dev,visualization]\"\n```\n\n## Quick Start\n\n```python\nfrom wyrdbound_dice import Dice\n\n# Basic roll\nresult = Dice.roll(\"1d20\")\nprint(result.total) # 20\nprint(result)       # 20 = 20 (1d20: 20)\n\n# Complex expression\nresult = Dice.roll(\"2d6 + 1d4 \u00d7 2 + 3\")\nprint(result) # 17 = 8 (2d6: 6, 2) + 3 (1d4: 3) x 2 + 3\n\n# Advantage roll (D&D 5e)\nresult = Dice.roll(\"2d20kh1\")\nprint(result) # 19 = 19 (2d20kh1: 19, 12)\n\n# Reroll (D&D 5e - Great Weapon Fighting)\nresult = Dice.roll(\"2d6r1<=2\")\nprint(result) # 12 = 12 (2d6r1<=2: 1, 2, 6, 6)\n\n# Exploding dice (Savage Worlds)\nresult = Dice.roll(\"1d6e\")\nprint(result) # 11 = 11 (1d6e6: 6, 5)\n\n# Fate Core\nresult = Dice.roll(\"4dF + 2\")\nprint(result) # 2 = 0 (4dF: +, B, -, B) + 2\n\n# With named modifiers\nmodifiers = {\"Strength\": 3, \"Proficiency\": 2, \"Bless\": \"1d4\"}\nresult = Dice.roll(\"1d20\", modifiers)\nprint(result) # 20 = 12 (1d20: 12) + 3 (Strength) + 2 (Proficiency) + 3 (Bless: 3 = 3 (1d4: 3))\n```\n\n## API Reference\n\n### Main Classes\n\n#### `Dice`\n\nThe main entry point for dice rolling.\n\n**`Dice.roll(expression, modifiers=None)`**\n\n- `expression` (str): Dice expression to evaluate\n- `modifiers` (dict, optional): Named modifiers as `{name: value}` where value can be int or dice expression string\n- Returns: `RollResultSet` object\n\n#### `RollResultSet`\n\nContains the results of a dice roll.\n\n**Properties:**\n\n- `total` (int): Final calculated result\n- `results` (list): List of individual `RollResult` objects\n- `modifiers` (list): List of applied modifiers\n- `__str__()`: Human-readable description of the complete roll\n\n#### `RollResult`\n\nRepresents a single dice expression result.\n\n**Properties:**\n\n- `num` (int): Number of dice rolled\n- `sides` (int/str): Number of sides (or \"F\" for Fudge, \"%\" for percentile)\n- `rolls` (list): Final kept dice values\n- `all_rolls` (list): All dice rolled (including rerolls, explosions)\n- `total` (int): Sum of kept dice\n\n### Exceptions\n\n- **`ParseError`**: Invalid dice expression syntax\n- **`DivisionByZeroError`**: Division by zero in expression\n- **`InfiniteConditionError`**: Impossible reroll/explode condition\n\n## Command Line Tools\n\n### Roll Tool\n\nRoll dice expressions from the command line:\n\n```bash\n# Basic usage\npython tools/roll.py \"1d20 + 5\"\n\n# Multiple rolls\npython tools/roll.py \"2d6\" --count 10\n\n# JSON output (single roll)\npython tools/roll.py \"1d20\" --json\n\n# JSON output (multiple rolls)\npython tools/roll.py \"1d6\" --count 3 --json\n```\n\n**Options:**\n\n- `-v, --verbose`: Show detailed breakdown\n- `-n, --count N`: Roll N times\n- `--json`: Output results as JSON\n\n**JSON Output Format:**\n\nSingle roll returns an object:\n\n```json\n{\n  \"result\": 14,\n  \"description\": \"14 = 14 (1d20: 14)\"\n}\n```\n\nMultiple rolls return an array:\n\n```json\n[\n  {\n    \"result\": 4,\n    \"description\": \"4 = 4 (1d6: 4)\"\n  },\n  {\n    \"result\": 6,\n    \"description\": \"6 = 6 (1d6: 6)\"\n  }\n]\n```\n\n### Visualization Tool\n\nGenerate probability distributions and statistics:\n\n```bash\n# Basic distribution graph\npython tools/graph.py \"2d6\"\n\n# Complex expression with more samples\npython tools/graph.py \"1d20 + 5\" --num-rolls 50000\n\n# Specify output file\npython tools/graph.py \"4d6kh3\" --output ability_scores.html\n```\n\n**Features:**\n\n- Probability distribution histograms\n- Statistical analysis (mean, mode, range)\n- Comparison charts for multiple expressions\n- Export to various image formats\n\n## Supported Systems\n\nWyrdBound Dice has been designed to support mechanics from many popular RPG systems:\n\n- **D&D 5e / Pathfinder**: Advantage/disadvantage (`2d20kh1`/`2d20kl1`), ability scores (`4d6kh3`)\n- **Savage Worlds**: Exploding dice (`1d6e`), wild dice, aces\n- **Fate Core/Accelerated**: Fudge dice (`4dF`, `FUDGE`)\n- **Traveller**: Boon/Bane (`BOON`/`BANE`), Flux dice (`FLUX`)\n- **World of Darkness**: Dice pools with success counting (upcoming)\n- **Shadowrun**: Exploding dice, glitch detection (upcoming)\n\n## Examples\n\n### Character Creation\n\n```python\n# D&D 5e ability scores\nstats = []\nfor _ in range(6):\n    result = Dice.roll(\"4d6kh3\")\n    stats.append(result.total)\n\n# Traveller characteristics with modifiers\ncharacteristics = Dice.roll(\"2d6\", {\"DM\": 1})\n```\n\n### Combat Rolls\n\n```python\n# D&D 5e attack with advantage\nattack = Dice.roll(\"2d20kh1 + 8\")  # +8 attack bonus\n\n# Savage Worlds damage with ace\ndamage = Dice.roll(\"1d6e + 2\")\n\n# Fate Core with aspects\nfate_roll = Dice.roll(\"4dF + 3\", {\"Aspect\": 2})\n```\n\n### Complex Expressions\n\n```python\n# Fireball damage (8d6) with Metamagic (reroll 1s)\nfireball = Dice.roll(\"8d6r1<=1\")\n\n# Sneak attack with multiple damage types\nsneak = Dice.roll(\"1d8 + 3d6\")  # Rapier + sneak attack\n\n# Mathematical complexity\ncomplex_formula = Dice.roll(\"(2d6 + 3) \u00d7 2 + 1d4 - 1\")\n```\n\n## Debug Logging\n\nWyrdBound Dice includes comprehensive debug logging to help troubleshoot dice rolling issues and understand how expressions are parsed and evaluated.\n\n### Enabling Debug Mode\n\n```python\nfrom wyrdbound_dice import Dice\n\n# Enable debug logging for a roll\nresult = Dice.roll(\"2d6 + 3\", debug=True)\n```\n\n### Debug Output Example\n\nWhen debug mode is enabled, you'll see detailed step-by-step information:\n\n```\nDEBUG: [START] Rolling expression: '2d6 + 3'\nDEBUG: [PROCESSING] Starting expression processing\nDEBUG: NORMALIZED: '2d6 + 3'\nDEBUG: [PARSER_SELECTION] Using precedence parser\nDEBUG: [TOKENIZING] Tokenizing expression: '2d6 + 3'\nDEBUG: Tokens: ['DICE(2d6)@0', 'PLUS(+)@3', 'NUMBER(3)@4']\nDEBUG: [PARSING] Parsing tokens with precedence rules\nDEBUG: [EVALUATING] Evaluating parsed expression\nDEBUG: Rolling 1d6: 5\nDEBUG: Rolling 1d6: 4\nDEBUG: [RESULT] Expression evaluated to: 12\nDEBUG: TOTAL 12 modifiers(0) = 12\nDEBUG: [COMPLETE] Final result: 12\n```\n\n### What Debug Mode Shows\n\nDebug logging provides insights into:\n\n- **Expression normalization**: How input expressions are cleaned and processed\n- **Shorthand expansion**: When shortcuts like \"FUDGE\" are expanded to \"4dF\"\n- **Parser selection**: Whether the precedence parser or original parser is used\n- **Tokenization**: How complex expressions are broken into tokens\n- **Individual dice rolls**: Each die roll with specific results\n- **Keep/drop operations**: Parsed keep/drop operations like \"kh2\"\n- **Mathematical evaluation**: Step-by-step calculation of complex expressions\n- **Modifier processing**: How modifiers are applied to results\n- **Error handling**: Debug information even when errors occur\n\n### Debug Examples\n\n```python\n# Simple dice with debug\nresult = Dice.roll(\"1d20\", debug=True)\n\n# Complex expression with debug\nresult = Dice.roll(\"2d6 * 2 + 1d4\", debug=True)\n\n# Keep operations with debug\nresult = Dice.roll(\"4d6kh3\", debug=True)\n\n# Shorthand expansion with debug\nresult = Dice.roll(\"FUDGE\", debug=True)\n\n# With modifiers and debug\nmodifiers = {\"strength\": 3, \"magic_bonus\": 2}\nresult = Dice.roll(\"1d20\", modifiers=modifiers, debug=True)\n```\n\n### Custom Debug Loggers\n\nYou can inject your own logger to capture debug output using Python's standard logging interface:\n\n```python\nimport logging\nfrom wyrdbound_dice import Dice\nfrom wyrdbound_dice.debug_logger import StringLogger\n\n# Method 1: Use the built-in StringLogger for testing/API purposes\nstring_logger = StringLogger()\nresult = Dice.roll(\"2d6 + 3\", debug=True, logger=string_logger)\n\n# Get all the debug output as a string\ndebug_output = string_logger.get_logs()\nprint(debug_output)\n\n# Clear the logger for reuse\nstring_logger.clear()\n\n# Method 2: Use Python's standard logging module\n# Create a custom logger with your preferred configuration\nlogger = logging.getLogger('my_dice_app')\nlogger.setLevel(logging.DEBUG)\n\n# Add your own handler (file, web service, etc.)\nhandler = logging.FileHandler('dice_debug.log')\nhandler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\nlogger.addHandler(handler)\n\n# Use with dice rolling\nresult = Dice.roll(\"1d20\", debug=True, logger=logger)\n\n# Method 3: Create a custom logger class\nclass WebAppLogger:\n    def debug(self, message):\n        # Send to your web app's logging system\n        app.logger.debug(message)\n\n    def info(self, message):\n        app.logger.info(message)\n\n    def warning(self, message):\n        app.logger.warning(message)\n\n    def error(self, message):\n        app.logger.error(message)\n\nweb_logger = WebAppLogger()\nresult = Dice.roll(\"1d20\", debug=True, logger=web_logger)\n```\n\n### Logger Interface\n\nCustom loggers should implement Python's standard logging interface methods:\n\n```python\nclass MyCustomLogger:\n    def debug(self, message: str) -> None:\n        \"\"\"Log a debug message.\"\"\"\n        ...\n\n    def info(self, message: str) -> None:\n        \"\"\"Log an info message.\"\"\"\n        ...\n\n    def warning(self, message: str) -> None:\n        \"\"\"Log a warning message.\"\"\"\n        ...\n\n    def error(self, message: str) -> None:\n        \"\"\"Log an error message.\"\"\"\n        ...\nclass MyLogger:\n    def log(self, message: str) -> None:\n        # Your custom logging implementation\n        pass\n```\n\n### Command Line Debug\n\nThe `tools/roll.py` script also supports debug mode:\n\n```bash\n# Basic roll with debug\npython tools/roll.py \"2d6 + 3\" --debug\n\n# Complex expression with debug\npython tools/roll.py \"4d6kh3\" --debug\n\n# Multiple rolls with debug\npython tools/roll.py \"1d6\" -n 3 --debug\n\n# JSON output with debug information included\npython tools/roll.py \"2d6 + 3\" --json --debug\n\n# Help shows all options including debug\npython tools/roll.py --help\n```\n\nWhen using `--json --debug`, the debug output is captured and included in the JSON response under a \"debug\" key:\n\n```json\n{\n  \"result\": 11,\n  \"description\": \"11 = 8 (2d6: 4, 4) + 3\",\n  \"debug\": \"DEBUG: [START] Rolling expression: '2d6 + 3'\\nDEBUG: [PROCESSING] Starting expression processing\\n...\"\n}\n```\n\n### Debug Output Format\n\nDebug messages are prefixed with `DEBUG:` and use structured labels like `[START]`, `[TOKENIZING]`, `[PARSING]`, etc. This makes it easy to follow the progression through the dice rolling engine and identify where issues might occur.\n\n## Development\n\n### Setting Up Development Environment\n\n```bash\n# Install the package with development dependencies\npip install -e \".[dev]\"\n\n# Install with both development and visualization dependencies\npip install -e \".[dev,visualization]\"\n\n# Or install development dependencies separately\npip install pytest pytest-cov black isort ruff\n```\n\n### Running Tests\n\n```bash\n# Run all tests\npython -m pytest tests/\n\n# Run with coverage\npython -m pytest tests/ --cov=wyrdbound_dice\n\n# Run with coverage and generate HTML report\npython -m pytest tests/ --cov=wyrdbound_dice --cov-report=html\n\n# Run specific test class\npython -m pytest tests/test_dice.py::TestDiceKeepHighestLowest\n```\n\n### Code Quality\n\n```bash\n# Format code\nblack src/ tests/ tools/\n\n# Sort imports\nisort src/ tests/ tools/\n\n# Lint code\nruff check src/ tests/ tools/\n```\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\n\n### Areas for Contribution\n\n- New features for existing RPG systems\n- Performance optimizations\n- Additional CLI tools\n- Documentation improvements\n- Bug fixes and testing\n\n### Continuous Integration\n\nThis project uses GitHub Actions for CI/CD:\n\n- **Testing**: Automated tests across Python 3.8-3.12 on Ubuntu, Windows, and macOS\n- **Code Quality**: Black formatting, isort import sorting, and Ruff linting\n- **Package Validation**: Installation testing and CLI tool verification\n\nAll pull requests are automatically tested and must pass all checks before merging.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Inspired by the diverse mechanics of tabletop RPG systems\n- Thanks to the RPG community for feedback and feature requests\n- Built with mathematical precision and gaming passion\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": "dice, tabletop, rpg, ttrpg, gaming, random, probability", "license": "MIT", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": "The Wyrd One <wyrdbound@proton.me>", "name": "wyrdbound-dice", "package_url": "https://pypi.org/project/wyrdbound-dice/", "platform": null, "project_url": "https://pypi.org/project/wyrdbound-dice/", "project_urls": {"Bug Tracker": "https://github.com/yourusername/wyrdbound-dice/issues", "Documentation": "https://github.com/yourusername/wyrdbound-dice#readme", "Homepage": "https://github.com/yourusername/wyrdbound-dice", "Repository": "https://github.com/yourusername/wyrdbound-dice"}, "provides_extra": ["visualization", "dev"], "release_url": "https://pypi.org/project/wyrdbound-dice/0.0.1/", "requires_dist": ["matplotlib>=3.7.0; extra == \"visualization\"", "pytest>=7.0.0; extra == \"dev\"", "pytest-cov>=4.0.0; extra == \"dev\"", "black>=23.0.0; extra == \"dev\"", "isort>=5.12.0; extra == \"dev\"", "ruff>=0.1.0; extra == \"dev\""], "requires_python": ">=3.8", "summary": "A comprehensive dice rolling library for tabletop RPGs", "version": "0.0.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388397, "urls": [{"comment_text": null, "digests": {"blake2b_256": "ef244653de922203357d0a252bf6917b3b25da69a80df3312384dbf3ea549400", "md5": "a29aaa03ef4fb147ecfcce2f4411a5b6", "sha256": "5ec7258371742c1607c73701262fc7a3b965a1e1147669dcf1089245069f89fc"}, "downloads": -1, "filename": "wyrdbound_dice-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "a29aaa03ef4fb147ecfcce2f4411a5b6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 29035, "upload_time": "2025-07-28T18:06:36", "upload_time_iso_8601": "2025-07-28T18:06:36.368761Z", "url": "https://files.pythonhosted.org/packages/ef/24/4653de922203357d0a252bf6917b3b25da69a80df3312384dbf3ea549400/wyrdbound_dice-0.0.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "90018b2b6ccaee3137e3623ac8b9a79ff3447a55106a34e48f70973139de0d95", "md5": "a7d90693ff399567831dc8a7dcdb97a8", "sha256": "33899eafea13e06479abd341032d1bbe39041cf2ae5ed44c6eb3388d312193de"}, "downloads": -1, "filename": "wyrdbound_dice-0.0.1.tar.gz", "has_sig": false, "md5_digest": "a7d90693ff399567831dc8a7dcdb97a8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 45379, "upload_time": "2025-07-28T18:06:37", "upload_time_iso_8601": "2025-07-28T18:06:37.384085Z", "url": "https://files.pythonhosted.org/packages/90/01/8b2b6ccaee3137e3623ac8b9a79ff3447a55106a34e48f70973139de0d95/wyrdbound_dice-0.0.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:06:29 GMT", "package": "bdtariff", "version": "0.1.0", "json": {"info": {"author": null, "author_email": "\"Md.Shamim-Ul-Islam\" <nbrshamim@gmail.com>", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# BD Tariff\n\nThis project provides a Python interface to access tariff rates and calculate duties based on the Bangladesh Customs Tariff for the fiscal year 2025-2026. The tariff data is sourced from the official document: [Tariff 2025-2026 (02-06-2025).pdf](https://customs.gov.bd/files/Tariff-2025-2026(02-06-2025).pdf).\n\n## Installation\n\n(Add installation instructions here, e.g., `pip install bdtariff` if you plan to publish it to PyPI, or instructions for cloning the repository and setting it up locally.)\n\n## How to Get Total Duty\n\nTo calculate the total duty for a given HSCode and assess value:\n\n```python\nfrom bdtariff import duty\n\nduty()\nWhen prompted, enter the HSCode and then the Assess Value in BDT. The function will return the total duty in BDT.\n\nHow to Know Tariff Rate\nTo retrieve the individual tariff rates for a specific HSCode:\n\nPython\n\nfrom bdtariff import rate\n\nrate()\nWhen prompted, enter the HSCode. The function will display the applicable tariff rates.\n\nHow to Get Tariff Details One by One\nYou can also access the individual tariff components and description for a given HSCode programmatically:\n\nPython\n\nfrom bdtariff import hscode\n\n# Replace \"HSCODE\" with the actual 8-digit HSCode\nresult = hscode(\"HSCODE\")\n\nif result:\n    print(result.cd)            # Get the 'cd' (Customs Duty) field\n    print(result.sd)            # Get the 'sd' (Supplementary Duty) field\n    print(result.rd)            # Get the 'rd' (Regulatory Duty) field\n    print(result.vat)           # Get the 'vat' (Value Added Tax) field\n    print(result.at)            # Get the 'at' (Advance Tax) field\n    print(result.ait)           # Get the 'ait' (Advance Income Tax) field\n    print(result.tti)           # Get the 'tti' (Total Taxable Imports) field - Note: This might be a calculated value, confirm its exact meaning in your context.\n    print(result.tarriff_description) # Get the 'Tariff Description' field\n    print(result.as_dict())     # Get all available fields as a dictionary\nelse:\n    print(\"HSCode not found\")\nSample Program\nHere's a complete example demonstrating how to use the hscode function:\n\nPython\n\nfrom bdtariff import hscode\n\nresult = hscode(\"01012100\") # Example HSCode for live horses, purebred breeding\n\nif result:\n    print(f\"Customs Duty (CD): {result.cd}\")\n    print(f\"Supplementary Duty (SD): {result.sd}\")\n    print(f\"All Tariff Details: {result.as_dict()}\")\nelse:\n    print(\"HSCode not found\")", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "bdtariff", "package_url": "https://pypi.org/project/bdtariff/", "platform": null, "project_url": "https://pypi.org/project/bdtariff/", "project_urls": {"Homepage": "https://github.com/nbrshamim/bdtariff", "Issues": "https://github.com/nbrshamim/bdtariff"}, "provides_extra": null, "release_url": "https://pypi.org/project/bdtariff/0.1.0/", "requires_dist": ["requests"], "requires_python": ">=3.8", "summary": "Tariff rate and duty calculation based on Bangladesh Import and Export related H.S. Code", "version": "0.1.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388392, "urls": [{"comment_text": null, "digests": {"blake2b_256": "b9c059404dd4ec41f1ae64c2ff17d061be1727bf736989c272039cbae73fb031", "md5": "5eeedace2a41f40df0721e239c048336", "sha256": "0af6b23a9e96791a07e965b5d67d4f893902cea534a651df6ba81745376dc631"}, "downloads": -1, "filename": "bdtariff-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "5eeedace2a41f40df0721e239c048336", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 15644, "upload_time": "2025-07-28T18:06:29", "upload_time_iso_8601": "2025-07-28T18:06:29.416122Z", "url": "https://files.pythonhosted.org/packages/b9/c0/59404dd4ec41f1ae64c2ff17d061be1727bf736989c272039cbae73fb031/bdtariff-0.1.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "8ba8b9972ebd3fa8c81dcc8ca7fbf2a4535bf25b626ff873b6bd843f2a08251b", "md5": "6edafc87d3ed7c8479a98d45e707f82b", "sha256": "3d44475653802415d41e16714d2190a0038e99b1c31e9b042c176cf74e433198"}, "downloads": -1, "filename": "bdtariff-0.1.0.tar.gz", "has_sig": false, "md5_digest": "6edafc87d3ed7c8479a98d45e707f82b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 173496, "upload_time": "2025-07-28T18:06:31", "upload_time_iso_8601": "2025-07-28T18:06:31.387749Z", "url": "https://files.pythonhosted.org/packages/8b/a8/b9972ebd3fa8c81dcc8ca7fbf2a4535bf25b626ff873b6bd843f2a08251b/bdtariff-0.1.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:06:23 GMT", "package": "pdex", "version": "0.1.20", "json": {"info": {"author": null, "author_email": "noam teyssier <noam.teyssier@arcinstitute.org>", "bugtrack_url": null, "classifiers": [], "description": "# pdex\n\nparallel differential expression for single-cell perturbation sequencing\n\n## Installation\n\nAdd to your `pyproject.toml` file with [`uv`](https://github.com/astral-sh/uv)\n\n```bash\nuv add pdex\n```\n\n## Summary\n\nThis is a python package for performing parallel differential expression between multiple groups and a control.\n\nIt is optimized for very large datasets and very large numbers of perturbations.\n\nIt makes use of shared memory to parallelize the computation to a high number of threads and minimizes the [IPC](https://en.wikipedia.org/wiki/Inter-process_communication) between processes to reduce overhead.\n\nIt supports the following metrics:\n\n- Wilcoxon Rank Sum\n- Anderson-Darling\n- T-Test\n\n## Usage\n\n```python\nimport anndata as ad\nimport numpy as np\nimport pandas as pd\n\nfrom pdex import parallel_differential_expression\n\nPERT_COL = \"perturbation\"\nCONTROL_VAR = \"control\"\n\nN_CELLS = 1000\nN_GENES = 100\nN_PERTS = 10\nMAX_UMI = 1e6\n\n\ndef build_random_anndata(\n    n_cells: int = N_CELLS,\n    n_genes: int = N_GENES,\n    n_perts: int = N_PERTS,\n    pert_col: str = PERT_COL,\n    control_var: str = CONTROL_VAR,\n) -> ad.AnnData:\n    \"\"\"Sample a random AnnData object.\"\"\"\n    return ad.AnnData(\n        X=np.random.randint(0, MAX_UMI, size=(n_cells, n_genes)),\n        obs=pd.DataFrame(\n            {\n                pert_col: np.random.choice(\n                    [f\"pert_{i}\" for i in range(n_perts)] + [control_var],\n                    size=n_cells,\n                    replace=True,\n                ),\n            }\n        ),\n    )\n\n\ndef main():\n    adata = build_random_anndata()\n\n    # Run pdex with default metric (wilcoxon)\n    results = parallel_differential_expression(\n        adata,\n        reference=CONTROL_VAR,\n        groupby_key=PERT_COL,\n    )\n    assert results.shape[0] == N_GENES * N_PERTS\n\n    # Run pdex with alt metric (anderson)\n    results = parallel_differential_expression(\n        adata,\n        reference=CONTROL_VAR,\n        groupby_key=PERT_COL,\n        metric=\"anderson\"\n    )\n    assert results.shape[0] == N_GENES * N_PERTS\n```\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "pdex", "package_url": "https://pypi.org/project/pdex/", "platform": null, "project_url": "https://pypi.org/project/pdex/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/pdex/0.1.20/", "requires_dist": ["adpbulk>=0.1.4", "anndata>=0.9.0", "numpy>=1.0.0", "pandas>=2.0.0", "polars>=1.30.0", "pyarrow>=18.0.0", "pydeseq2>=0.5.1", "scipy>=1.15.2", "tqdm>=4.67.1"], "requires_python": ">=3.10", "summary": "Parallel differential expression for single-cell perturbation sequencing", "version": "0.1.20", "yanked": false, "yanked_reason": null}, "last_serial": 30388388, "urls": [{"comment_text": null, "digests": {"blake2b_256": "4a0d72fdafb475a6b2c13bb6ff056e5d6b3ec6d58f9046f9091cc778239e36da", "md5": "725d6d86f032ba4c9276b58b67b74c17", "sha256": "f54fc7633aca4f333933420419c80fc6df0808eedf67ff634fbe3f281a634f4c"}, "downloads": -1, "filename": "pdex-0.1.20-py3-none-any.whl", "has_sig": false, "md5_digest": "725d6d86f032ba4c9276b58b67b74c17", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.10", "size": 9548, "upload_time": "2025-07-28T18:06:23", "upload_time_iso_8601": "2025-07-28T18:06:23.400550Z", "url": "https://files.pythonhosted.org/packages/4a/0d/72fdafb475a6b2c13bb6ff056e5d6b3ec6d58f9046f9091cc778239e36da/pdex-0.1.20-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "b064f8c05298342952a2717d9ec47965e05a7435fa94f5fb2ab4b505bc0fb075", "md5": "c38298c2e05dc009ae1e138a4642656a", "sha256": "fa00e8d79720a17b1ed2471a68e87fea6d3f2722e0bd1abf9f7130b16aa7fb08"}, "downloads": -1, "filename": "pdex-0.1.20.tar.gz", "has_sig": false, "md5_digest": "c38298c2e05dc009ae1e138a4642656a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.10", "size": 10790, "upload_time": "2025-07-28T18:06:24", "upload_time_iso_8601": "2025-07-28T18:06:24.346800Z", "url": "https://files.pythonhosted.org/packages/b0/64/f8c05298342952a2717d9ec47965e05a7435fa94f5fb2ab4b505bc0fb075/pdex-0.1.20.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:06:22 GMT", "package": "fractale", "version": "0.0.11", "json": {"info": {"author": "Vanessa Sochat", "author_email": "vsoch@users.noreply.github.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)", "Operating System :: Unix", "Programming Language :: C", "Programming Language :: Python", "Programming Language :: Python :: 3.11", "Topic :: Scientific/Engineering", "Topic :: Software Development"], "description": "# fractale\n\n> Translation layer for a jobspec specification to cluster execution\n\n[![PyPI version](https://badge.fury.io/py/fractale.svg)](https://badge.fury.io/py/fractale)\n[![DOI](https://zenodo.org/badge/773568660.svg)](https://zenodo.org/doi/10.5281/zenodo.13787066)\n\nThis library is primarily being used for development for the descriptive thrust of the Fractale project. It is called fractale, but also not called fractale. You can't be sure of the name until you open the box.\n\n## Design\n\n### Simple\n\nWe provide a simple translation layer between job specifications. We take the assumption that although each manager has many options, the actual options a user would use is a much smaller set, and it's relatively straight forward to translate (and have better accuracy).\n\nSee [examples/transform](examples/transform) for an example.\n\n### Complex\n\nWe want to:\n\n1. Generate software graphs for some cluster (fluxion JGF) (this is done with [compspec](https://github.com/compspec/compspec)\n2. Register N clusters to a tool (should be written as a python module)\n3. Tool would have ability to select clusters from resources known, return\n4. Need graphical representation (json) of each cluster - this will be used with the LLM inference\n\nSee [examples/fractale](examples/fractale) for a detailed walk-through of the above.\n\nFor graph tool:\n\n```bash\nconda install -c conda-forge graph-tool\n```\n\n## Questions\n\n- Should other subsystem types have edges? How used?\n- Should we try to map them to nodes in the graph or use another means (or assume global across cluster nodes as we do now)?\n- Can we simplify spack subsystem graph (it's really big...)\n\n<!-- \u2b50\ufe0f [Documentation](https://compspec.github.io/fractale) \u2b50\ufe0f -->\n\n## License\n\nHPCIC DevTools is distributed under the terms of the MIT license.\nAll new contributions must be made under this license.\n\nSee [LICENSE](https://github.com/converged-computing/cloud-select/blob/main/LICENSE),\n[COPYRIGHT](https://github.com/converged-computing/cloud-select/blob/main/COPYRIGHT), and\n[NOTICE](https://github.com/converged-computing/cloud-select/blob/main/NOTICE) for details.\n\nSPDX-License-Identifier: (MIT)\n\nLLNL-CODE- 842614\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://github.com/compspec/fractale", "keywords": "cluster, orchestration, transformer, jobspec, flux", "license": "LICENSE", "license_expression": null, "license_files": null, "maintainer": "Vanessa Sochat", "maintainer_email": null, "name": "fractale", "package_url": "https://pypi.org/project/fractale/", "platform": null, "project_url": "https://pypi.org/project/fractale/", "project_urls": {"Homepage": "https://github.com/compspec/fractale"}, "provides_extra": ["all"], "release_url": "https://pypi.org/project/fractale/0.0.11/", "requires_dist": ["jsonschema", "compspec", "compspec-spack", "compspec-modules", "rich", "jsonschema; extra == \"all\"", "compspec; extra == \"all\"", "compspec-spack; extra == \"all\"", "compspec-modules; extra == \"all\"", "rich; extra == \"all\"", "pytest>=4.6.2; extra == \"all\""], "requires_python": null, "summary": "Jobspec specification and translation layer for cluster work", "version": "0.0.11", "yanked": false, "yanked_reason": null}, "last_serial": 30388387, "urls": [{"comment_text": "", "digests": {"blake2b_256": "a9a4cd1047185c23bb5f9b514e38d767c2529f3158cbfdec40f527fe0084ffb8", "md5": "9ef91b2d9f783b3300f98d0f35709ba1", "sha256": "3ca8674d5eb728a967042e40210851a8fee095af686f1214d7cd7bbb2d549bf9"}, "downloads": -1, "filename": "fractale-0.0.11-py3-none-any.whl", "has_sig": false, "md5_digest": "9ef91b2d9f783b3300f98d0f35709ba1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 79871, "upload_time": "2025-07-28T18:06:22", "upload_time_iso_8601": "2025-07-28T18:06:22.249261Z", "url": "https://files.pythonhosted.org/packages/a9/a4/cd1047185c23bb5f9b514e38d767c2529f3158cbfdec40f527fe0084ffb8/fractale-0.0.11-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "21f923d8ed75aeb9ed64422634dfcc1fe9343884d8c10cf3f7dcd0d8ef1fc461", "md5": "86891b92bd5ddaef182a74d54074a701", "sha256": "7aaab5f66c8cf74f51265af33a8a6608277a0c9c719d821ab8450df56746e83d"}, "downloads": -1, "filename": "fractale-0.0.11.tar.gz", "has_sig": false, "md5_digest": "86891b92bd5ddaef182a74d54074a701", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60215, "upload_time": "2025-07-28T18:06:24", "upload_time_iso_8601": "2025-07-28T18:06:24.025776Z", "url": "https://files.pythonhosted.org/packages/21/f9/23d8ed75aeb9ed64422634dfcc1fe9343884d8c10cf3f7dcd0d8ef1fc461/fractale-0.0.11.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:06:11 GMT", "package": "dev-cmd", "version": "0.32.1", "json": {"info": {"author": null, "author_email": "John Sirois <john.sirois@gmail.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Intended Audience :: Developers", "Operating System :: MacOS :: MacOS X", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX :: Linux", "Operating System :: Unix", "Programming Language :: Python", "Programming Language :: Python :: 3 :: Only", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Programming Language :: Python :: 3.14", "Programming Language :: Python :: 3.9", "Topic :: Software Development :: Build Tools", "Topic :: Utilities"], "description": "# dev-cmd\n\n[![PyPI Version](https://shields.io/pypi/v/dev-cmd.svg)](https://pypi.org/project/dev-cmd/)\n[![License](https://shields.io/pypi/l/dev-cmd.svg)](LICENSE)\n[![Supported Pythons](https://shields.io/pypi/pyversions/dev-cmd.svg)](pyproject.toml)\n[![CI](https://img.shields.io/github/actions/workflow/status/jsirois/dev-cmd/ci.yml)](https://github.com/jsirois/dev-cmd/actions/workflows/ci.yml)\n\nThe `dev-cmd` tool provides a simple way to define commands you use to develop your project with in\n`pyproject.toml`.\n\n## Configuration\n\nYou define the commands you want `dev-cmd` to run and more under the `[tool.dev-cmd]` table in\n`pyproject.toml`.\n\n### Commands\n\nYou'll need at least one command defined for `dev-cmd` to be able to do anything useful. At a\nminimum a command needs a name and a list of command line arguments that form the command.\nFor example:\n\n```toml\n[tool.dev-cmd.commands]\ngreet = [\"python\", \"-c\", \"import os; print(f'Hello from {os.getcwd()!r}.')\"]\n```\n\nMore on execution in various environments [below](#Execution), but you can run the greet command\nwith, for example `uv run dev-cmd greet`.\n\nThere are two special argv0's you can use in your command arguments list:\n1. \"python\": This will be mapped to the Python interpreter specified for the command (described\n   below) or else the Python interpreter executing `dev-cmd`.\n2. A file name ending in \".py\": This will be assumed to be a python script, and executed with a\n   Python interpreter as described in 1 above.\n\nYou can define as many commands as you want. They will all run from the project root directory (the\ndirectory containing the `pyproject.toml` the commands are defined in) and accept no arguments\nbesides those defined in the command. You can gain further control over the command by defining it\nin a table instead of as a list of command line arguments. For example:\n\n```toml\n[tool.dev-cmd.commands.test]\npython = \"python3.9\"\nargs = [\"pytest\"]\nenv = {\"PYTHONPATH\" = \"../test-support\"}\ncwd = \"tests\"\naccepts-extra-args = true\n```\n\nHere, the test command will be run with `python3.9` regardless of the interpreter running `dev-cmd`.\nThis relies on configuration for custom interpreters described [below](#Custom-Pythons).\n\nAlso, the working directory is set to the `tests/` directory (which must exist) and the `PYTHONPATH`\nis set to its sibling `test-support` directory. This allows for importable shared test code to be\nplaced under the `test-support` directory in a project laid out like so:\n```\nproject-dir/\n    pyproject.toml\n    tests/\n    test-support/\n```\n\n#### Pass Through Args\n\nThe `accepts-extra-args = true` configuration allows for passing extra args to pytest like so:\n```console\nuv run dev-cmd test -- -vvs\n```\nAll arguments after the `--` are passed to `pytest` by appending them to the `test` command `args`\nlist. `dev-cmd` ensures at most one command `accepts-extra-args` per invocation so that they can be\nunambiguously forwarded to the command that needs them. For example, lets expand the set of commands\nwe support:\n```toml\n[tool.dev-cmd.commands]\nfmt = [\"ruff\", \"format\"]\nlint = [\"ruff\", \"check\", \"--fix\"]\n\n[tool.dev-cmd.commands.test]\nargs = [\"pytest\"]\nenv = {\"PYTHONPATH\" = \"../test-support\"}\ncwd = \"tests\"\naccepts-extra-args = true\n```\nYou can now run the following and the extra args (`-vvs`) will be forwarded to `pytest` but not to\n`ruff` in the `fmt` and `lint` commands:\n```console\nuv run dev-cmd fmt lint test -- -vvs\n```\nHere we ran multiple commands in sequence passing extra args to test. We could have also run this\nas:\n```console\nuv run dev-cmd test fmt lint -- -vvs\n```\nThe order commands are run in does not affect where extra args are passed.\n\n#### Platform Selection\n\nYou can condition command availability based on the current platform characteristics as determined\nby a `when` [environment marker][environment marker]. For example, to define a Windows-only command:\n```toml\n[tool.dev-cmd.commands.query-registry]\nwhen = \"sys_platform == 'win32'\"\nargs = [\"scripts/query-windows-registry.py\"]\n```\n\nYou can also use `when` conditions to select from amongst a mutually exclusive set of commands, each\ntailored to a specific platform. For this you can specify a common `name` for the commands forming\nthe mutually exclusive group. For example, to define a \"query\" command that has a different\nimplementation for Windows than for other systems:\n```toml\n[tool.dev-cmd.commands.query-posix]\nwhen = \"sys_platform != 'win32'\"\nname = \"query\"\nargs = [\"scripts/query-posix.py\"]\n\n[tool.dev-cmd.commands.query-windows]\nwhen = \"sys_platform == 'win32'\"\nname = \"query\"\nargs = [\"scripts/query-windows-registry.py\"]\n```\n\n[environment marker]: https://packaging.python.org/en/latest/specifications/dependency-specifiers/#environment-markers\n\n#### Parameterization\n\nA command's python, arguments and env values can be parameterized with values from the execution\nenvironment. Parameters are introduced in between brackets with an optional default value:\n`{<key>(:<default>)?}`. Parameters can draw from four sources:\n1. Environment variables via `{env.<name>}`; e.g.: `{env.HOME}`\n2. The current Python interpreter's marker environment via `{markers.<name>}`; e.g.:\n   `{markers.python_version}`\n3. Factors via `{-<name>}`; e.g.: `{-py:{markers.python_version}}`\n4. A hash seed via `{--hashseed}`. The value comes from `dev-cmd --hashseed` if passed; otherwise a\n   random hash seed suitable for use with `PYTHONHASHSEED` is generated.\n\nIn the first three cases, the parameter name can itself come from a nested parameterization; e.g.:\n`{markers.{-marker:{env.MARKER:python_version}}}` selects the environment marker value for the\nenvironment marker named by the `marker` factor if defined; otherwise the `MARKER` environment\nvariable if defined and finally falling back to `python_version` if none of these are defined.\n\nThe available Python marker environment variables are detailed in [PEP-508](\nhttps://peps.python.org/pep-0508/#environment-markers).\n\nCommand arguments can be elided from the list when their value is parameterized and evaluates to\nempty by wrapping the argument in a single-item `{discard_empty = \"...\"}` table. For example,\ndebug flags could be passed to pytest via the `DEBUG` env var, but only when present, with:\n```toml\n[tool.dev-cmd.commands]\npytest = [\"python\", \"-m\", \"pytest\", {discard_empty = \"{env.DEBUG:\"}]\n```\n\nFactors are introduced as suffixes to command names and are inspired by and similar to those found\nin [tox](https://tox.wiki/) configuration. If a command is named `test` but the command is invoked\nas `test-py3.12`, the `-py3.12` factor will be defined. The value of `3.12` could then be read via\nthe `{-py}` factor parameter placeholder in the command arguments or env values. The factor name\nprefix will be stripped from the factor argument to produce the substituted value. As a consequence,\nyou want to ensure the factor names you use are non-overlapping or else an error will be raised due\nto ambiguity in which factor argument should be applied. An optional leading `:` can proceed the\nfactor argument value, and it will be stripped. So both `test-py:3.12` and `test-py3.12` pass `3.12`\nas the value for the `-py` factor parameter. The colon-prefix helps distinguish factor name from\nfactor value, paralleling the default value syntax that can be used at factor parameter declaration\nsites. If your factor value contains a `-`, just escape it with a `-`; i.e.: `--` will map to a\nsingle `-` in a factor value instead of indicating a new factor starts there.\n\nThere are two special forms of factors to be aware of: flag factors for passing one value or another\nconditionally and the `py` factor when used as the value of a command python.\n\nAn example of a flag factor is `{-color?--color=always:--color=auto}`. Here the factor name is\n`color` and, when present as a command suffix, it evaluates to `--color-always`. When the factor is\nabsent from the command name, it evaluates to `--color=auto`. Re-visiting the `discard_empty`\nexample above, you might more usefully parameterize pytest debugging with:\n```toml\n[tool.dev-cmd.commands]\npytest = [\"python\", \"-m\", \"pytest\", {discard_empty = \"{-debug?--pdb:\"}]\n```\n\nInstead of having to say `DEBUG=--pdb uv run dev-cmd pytest` you can say\n`uv run dev-cmd pytest-debug`. This has the advantage of being discoverable via `--list` and sealing\nin the correct debugger flag for pytest.\n\nThe other special form if the factor named `py` when used as the value for a command python. For\nexample:\n```toml\n[tool.dev-cmd.commands.query]\npython = \"{-py:}\"\nargs = [\"scripts/query.py\"]\n```\n\nHere, executing `uv run dev-cmd query-python3.8` will run the query script with Python 3.8. This\nis just standard factor substitution at work. However, you can also say: `query-py3.8` or even\n`query-py38` with the same result. Namely, for a command python, the `py` factor has special value\nhandling that will add the `python` prefix for you if you just supply the version number or even\njust the version digits. PyPy is also supported. Instead of using the awkward `query-py:pypy` or\n`query-pypypy`, you can use `query-pypy`. The `py` factor value gets expanded to `pypy`. This also\nworks with the version number handling; so you can say `query-pypy310` to pass `pypy3.10` as the\nquery script Python to use.\n\n#### Documentation\n\nYou can document a command by providing a `description`. If the command has factors, you can\ndocument these using a `factors` sub-table whose keys are the factor names and whose values are\nstrings that describe the factor.\n\nFor example:\n```toml\n[tool.dev-cmd.commands.type-check.factors]\npy = \"The Python version to type check in <major>.<minor> form; i.e.: 3.13.\"\n\n[tool.dev-cmd.commands.type-check]\nargs = [\n   \"mypy\",\n   \"--python-version\", \"{-py:{markers.python_version}}\",\n   \"--cache-dir\", \".mypy_cache_{markers.python_version}\",\n   \"setup.py\",\n   \"dev_cmd\",\n   \"tests\",\n]\n```\n\nYou can view this documentation by passing `dev-cmd` either `-l` or `--list`. For example:\n```console\nuv run dev-cmd --list\nCommands:\ntype-check:\n    -py: The Python version to type check in <major>.<minor> form; i.e.: 3.13.\n         [default: {markers.python_version} (currently 3.12)]\n```\n\nIf you'd like to hide a command from being listed, define it as a table and include a\n`hidden = true` entry.\n\n### Tasks\n\nTasks are defined in their own table and compose two or more commands to implement some larger task.\nTask names share the same namespace as command names and so must be unique from those. Continuing\nwith the current example:\n```toml\n[tool.dev-cmd.commands]\nfmt = [\"ruff\", \"format\"]\nlint = [\"ruff\", \"check\", \"--fix\"]\n\n[tool.dev-cmd.commands.test]\nargs = [\"pytest\"]\nenv = {\"PYTHONPATH\" = \"../test-support\"}\ncwd = \"tests\"\naccepts-extra-args = true\n\n[tool.dev-cmd.tasks]\ntidy = [\"fmt\", \"lint\"]\n```\n\nWith that configuration, executing `uv run dev-cmd tidy` will execute the `fmt` command and then\nthe `lint` command in sequence. Each entry in the list is referred to as a step and is the name of\nany command or any task defined earlier in the file. This last restriction naturally avoids cycles.\n\n#### Parallelization\n\nSteps are run in sequence by default and execution halts at the 1st step to fail by default. See\n[Execution](#Execution) for options to control these defaults. To cause two or more steps in a task\nto run in parallel, enclose them in a sub-list. Continuing with the example above, but eliding the\ncommand definitions:\n```toml\n[tool.dev-cmd.tasks]\ntidy = [\"fmt\", \"lint\"]\nunsafe-tidy = [[\"fmt\", \"lint\"]]\nchecks = [[[\"fmt\", \"lint\"], \"test\"]]\n```\nWhen `uv run dev-cmd unsafe-tidy` is run, both `fmt` and `lint` will run in parallel. This is unsafe\nsince both commands can modify the same files. It's up to you to control for this sort of issue when\ndeciding which commands to run in parallel.\n\nWhen `uv run dev-cmd checks` is run, The elements in the 1st nested list are again run in parallel.\nThis time the 1st element is a list: `[\"fmt\", \"lint]`. Each layer of list nesting alternates between\nrunning serially and running in parallel; so `fmt` and `list` will be run serially in that order\nwhile they race `test` as a group in parallel.\n\n#### Platform Selection\n\nYou can define platform-specific tasks using `when` and `name` entries in a task's table similar to\nthe facility described for platform-specific commands above.\n\n#### Expansion\n\nThe `dev-cmd` runner supports expansion of steps via enumerated placeholders like `{a,b,c}` and\nrange placeholders like `{0..3}`. Whether supplied as step names via the command line or in task\nlists, these placeholders will result in the surrounding step name being expanded into two or more\nsteps. For example, the following configuration results in a type-checks task that runs `mypy` in\nparallel checking against Python 3.8 through 3.13:\n```toml\n[tool.dev-cmd.commands]\ntype-check = [\"mypy\", \"--python\", \"{-py:{markers.python_version}}\"]\n\n[tool.dev-cmd.tasks]\ntype-checks = [[\"type-check-py3.{8..13}\"]]\n```\n\nYou could also ad-hoc check against just Python 3.8 and 3.9 in parallel via the following, even if\nyour shell does not do parameter expansion of this sort:\n```console\nuv run dev-cmd -p 'type-check-py3.{8,9}'\n```\n\n#### Documentation\n\nYou can document a task by defining it in a table instead of as a list of steps. To do so, supply\nthe list of steps with the `steps` key and the documentation with the `description` key:\n```toml\n[tool.dev-cmd.commands]\nfmt = [\"ruff\", \"format\"]\nlint = [\"ruff\", \"check\", \"--fix\"]\ntype-check = [\"mypy\", \"--python\", \"{-py:{markers.python_version}}\"]\n\n[tool.dev-cmd.commands.test]\nargs = [\"pytest\"]\ncwd = \"tests\"\naccepts-extra-args = true\n\n[tool.dev-cmd.tasks.checks]\ndescription = \"Runs all development checks, including auto-formatting code.\"\nsteps = [\n    \"fmt\",\n    \"lint\",\n    # Parallelizing the type checks and test is safe (they don't modify files), and it nets a ~3x\n    # speedup over running them all serially.\n    [\"type-check-py3.{8..13}\", \"test\"],\n]\n```\n\nYou can view this documentation by passing `dev-cmd` either `-l` or `--list`. For example:\n```console\nuv run dev-cmd --list\nCommands:\nfmt\nlint\ntype-check:\n    -py: [default: {markers.python_version} (currently 3.12)]\ntest (-- extra pytest args ...)\n\nTasks:\nchecks (-- extra pytest args ...):\n    Runs all development checks, including auto-formatting code.\n```\n\nIf you'd like to hide a task from being listed, define it as a table and include a `hidden = true`\nentry.\n\n### Global Options\n\nYou can set a default command or task to run when `dev-cmd` is passed no positional arguments like\nso:\n```toml\n[tool.dev-cmd]\ndefault = \"checks\"\n```\nThis configuration means the following will run `fmt`, `lint` and `test`:\n```console\nuv run dev-cmd\n```\nYou can also configure when `dev-cmd` exits when it encounters command failures in a run:\n```toml\n[tool.dev-cmd]\nexit-style = \"immediate\"\n```\nThis will cause `dev-cmd` to fail fast as soon as the 1st command fails in a run. By default, the\nexit style is `\"after-step\"` which only exits after the step containing a command (if any)\ncompletes. For the `checks` task defined above, this means a failure in `fmt` would not be\npropagated until after `lint` completed, finishing the step `fmt` found itself in. The final choice\nfor `exit-style` is `end` which causes `dev-cmd` to run everything to completion, only listing\nerrors at the very end.\n\n### Custom Pythons\n\nIf you'd like to use a modern development tool, but you need to run commands against older Pythons\nthan it supports, you may be able to leverage the `--py` / `--python` option as a workaround. There\nare a few preconditions your setup needs to satisfy to be able to use this approach:\n1. Your development tool needs to support locking for older pythons if it uses lock files.\n2. Your development tool needs to be able to export your project development requirements in Pip\n   requirements.txt format.\n\nIf your development tool meets these requirements (for example, `uv` does), then in order to have\naccess to the `--python` option you need to install `dev-cmd` with the `old-pythons` extra;\ne.g.: a requirement string like `\"dev-cmd[old-pythons]\"`.\n\nWith that done, a minimal configuration looks like so:\n```toml\n[[tool.dev-cmd.python]]\n3rdparty-export-command = [\"uv\", \"export\", \"-q\", \"--no-emit-project\", \"-o\", \"{requirements.txt}\"]\n```\n\nHere your export command just needs to be able to output a Pip requirements.txt compatible\nrequirements file to build the venv with. The `{requirements.txt}` placeholder should be inserted in\nthe command line wherever its output path argument lives.\n\nBy default, `dev-cmd` also installs your project in each custom venv in editable mode as an extra\nrequirement. You may wish to adjust which extra requirements are installed, in which case you use\nthe `extra-requirements` key:\n```toml\n[[tool.dev-cmd.python]]\n3rdparty-export-command = [\n   \"uv\", \"export\", \"-q\",\n   \"--no-emit-project\",\n   \"--no-emit-package\", \"subproject\",\n   \"-o\", \"{requirements.txt}\"\n]\nextra-requirements = [\n   \"-e\", \".\",\n   \"subproject @ ./subproject\"\n]\n```\n\nHere we exclude the main project and a local subproject from the requirements export since `uv`\nexports hashes for these which Pip does not support for directories. To work around, we just list\nthese two local projects in `extra-requirements` and they get installed as-is without a hash check\nafter the exported requirements are installed. You can alternatively supply `extra-requirements` as\na single string, in which case the string will be written out to a file and passed to `pip install`\nas a `-r` / `--requirement` file.\n\nYou can also supply a `finalize-command` as a list of command line argument strings for the venv.\nThis command will run last after the 3rdparty requirements and extra requirements are installed and\ncan use `{venv-python}` and `{venv-site-packages}` placeholders to receive these paths for\nmanipulating the venv.\n\nYou may find the need to vary venv setup per Python `--version`. This is supported by specifying\nmultiple `[[tool.dev-cmd.python]]` entries. For example:\n```toml\n[[tool.dev-cmd.python]]\nwhen = \"python_version >= '3.7'\"\n3rdparty-export-command = [\"uv\", \"export\", \"-q\", \"--no-emit-project\", \"-o\", \"{requirements.txt}\"]\n\n[[tool.dev-cmd.python]]\nwhen = \"python_version < '3.7'\"\n\npip-requirement = \"pip<10\"\nextra-requirements = [\".\"]\nextra-requirements-pip-install-opts = [\"--no-use-pep517\"]\n```\n\nYou must ensure just one `[[tool.dev-cmd.python]]` entry is selected per `--python` via a `when`\nenvironment marker. You can then customize the version of Pip selected for the venv via\n`pip-requirement`, the extra `extra-requirements` to install and any custom `pip install` options\nyou need for either the 3rdparty requirements install via `3rdparty-pip-install-opts` or the extra\nrequirements install via `extra-requirements-pip-install-opts`.\n\nNote that when defining multiple `[[tool.dev-cmd.python]]` entries, the 1st is special in setting\ndefaults all subsequent `[[tool.dev-cmd.python]]` entries inherit for keys left unspecified. In the\nexample above, the second entry for Python 3.6 and older could add a `3rdparty-export-command` if\nit needed different export behavior for those older versions.\n\nVenvs are created under a `.dev-cmd` directory and are cached based on the values of the\n\"build-system\", \"project\" and \"project.optional-dependencies\" in `pyproject.toml` by default. To\nchange this default, you can specify a custom `pyproject-cache-keys`. You can also mix the full\ncontents of any other files, directories or environment variables into the venv cache key using\n`extra-cache-keys`. For files or directories, add a string entry denoting their path or else an\nentry like `{path = \"the/path\"}`. For environment variables, add an entry like\n`{env = \"THE_ENV_VAR\"}`. Here, combining both of these options, we turn off pyproject.toml inputs to\nthe venv cache key and just rely on the contents of `uv.lock`, which is what the export command is\npowered by:\n```toml\n[tool.dev-cmd.python.requirements]\n3rdparty-export-command = [\"uv\", \"export\", \"-q\", \"--no-emit-project\", \"-o\", \"{requirements.txt}\"]\npyproject-cache-keys = []\nextra-cache-keys = [\"uv.lock\"]\n```\n\nIf you need to vary the venv contents based on the command being run you can specify which\ndependency-group the command needs and then have your export command respect this value. For\nexample:\n```toml\n[dependency-groups]\ndev = [\n   \"dev-cmd[old-pythons]\",\n   \"mypy\",\n   \"ruff\",\n   {include = \"test\"}\n]\ntest = [\"pytest\"]\n\n[tool.dev-cmd.commands]\nfmt = [\"ruff\", \"format\"]\nlint = [\"ruff\", \"check\", \"--fix\"]\ntype-check = [\"mypy\", \"--python\", \"{-py:{markers.python_version}}\"]\n\n[tool.dev-cmd.commands.test]\nargs = [\"pytest\"]\ncwd = \"tests\"\naccepts-extra-args = true\ndependency-group = \"test\"\n\n[tool.dev-cmd.python.requirements]\n3rdparty-export-command = [\n   \"uv\", \"export\", \"-q\",\n   \"--no-emit-project\",\n   \"--only-group\", \"{dependency-group:dev}\",\n   \"-o\", \"{requirements.txt}\"\n]\npyproject-cache-keys = []\nextra-cache-keys = [\"uv.lock\"]\n```\n\nHere, the export command uses the special `{dependency-group:default}` placeholder to ensure\n`uv run dev-cmd --py 38 fmt lint type-check test` creates a Python 3.8 venv populated by the \"test\"\ndependency group to run pytest in and a default Python 3.8 venv populated with everything in the\n\"dev\" dependency group to run everything else in.\n\n## Execution\n\nThe `dev-cmd` tool supports several command line options to control execution in ad-hoc ways. You\ncan override the configured `exit-style` with `-k` / `--keep-going` (which is equivalent to\n`exit-style = \"end\"`) or `-X` / `--exit-style`. You can also cause all steps named on the command\nline to be run in parallel instead of in order with `-p` / `--parallel`. Finally, you can skip steps\nwith `-s` / `--skip`. This can be useful when running a task like `checks` defined above that\nincludes several commands, but one or more you'd like to skip. This would run all checks except\nthe tests:\n```console\nuv run dev-cmd checks -s test\n```\n\nIn order for `dev-cmd` to run most useful commands, dependencies will need to be installed that\nbring in those commands, like `ruff` or `pytest`. This is done differently in different tools.\nBelow are some commonly used tools and the configuration they require along with the command used to\ninvoke `dev-cmd` using each tool.\n\n### [PDM](https://pdm-project.org/) and [uv](https://docs.astral.sh/uv/)\n\nAdd `dev-cmd` as well as any other needed dependencies to the `dev` dependency group:\n```toml\n[dependency-groups]\ndev = [\"dev-cmd\", \"pytest\", \"ruff\"]\n```\nYou can then execute `dev-cmd` with `uv run dev-cmd [args...]`. For `pdm` you'll have to 1st run\n`pdm install` to make `dev-cmd`, `pytest` and `ruff` available.\n\n### [Poetry](https://python-poetry.org/)\n\nAdd `dev-cmd` as well as any other needed dependencies to the dev dependencies:\n```toml\n[tool.poetry.dev-dependencies]\ndev-cmd = \"*\"\npytest = \"*\"\nruff = \"*\"\n```\n\nRun `poetry install` and then you can run `poetry run dev-cmd [args...]`.\n\n### [Hatch](https://hatch.pypa.io/)\n\nAdd `dev-cmd` as well as any other needed dependencies to an environment's dependencies. Here we use\nthe `default` environment for convenience:\n```toml\n[tool.hatch.envs.default]\ndependencies = [\"dev-cmd\", \"pytest\", \"ruff\"]\n```\n\nYou can then execute `hatch run dev-cmd [args...]`.\n\n## Pre 1.0 Warning\n\nThis is a very new tool that can be expected to change rapidly and in breaking ways until the 1.0\nrelease. The current best documentation is the dogfooding this project uses for its own development\ndescribed below. You can look at the `[tool.dev-cmd]` configuration in [`pyproject.toml`](\npyproject.toml) to get a sense of how definition of commands, tasks and defaults works.\n\n## Development\n\nDevelopment uses [`uv`](https://docs.astral.sh/uv/getting-started/installation/). Install as you\nbest see fit.\n\nWith `uv` installed, running `uv run dev-cmd` is enough to get the tools `dev-cmd` uses installed\nand  run against the codebase. This includes formatting code, linting code, performing type checks\nand then running tests.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": "automation, command, runner, testing", "license": null, "license_expression": "Apache-2.0", "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "dev-cmd", "package_url": "https://pypi.org/project/dev-cmd/", "platform": null, "project_url": "https://pypi.org/project/dev-cmd/", "project_urls": {"Bug Tracker": "https://github.com/jsirois/dev-cmd/issues", "Changelog": "https://github.com/jsirois/dev-cmd/blob/main/CHANGES.md", "Repository": "https://github.com/jsirois/dev-cmd"}, "provides_extra": ["old-pythons"], "release_url": "https://pypi.org/project/dev-cmd/0.32.1/", "requires_dist": ["aioconsole", "ansicolors", "colorama; sys_platform == \"win32\"", "packaging", "tomli; python_version < \"3.11\"", "typing-extensions", "pex; extra == \"old-pythons\"", "filelock; extra == \"old-pythons\""], "requires_python": ">=3.9", "summary": "A simple development command runner for Python projects.", "version": "0.32.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388382, "urls": [{"comment_text": null, "digests": {"blake2b_256": "8bdd4772afe91be92c48ecc05d0d8d83c6b5f9672b99579c0a1681d4b1003f40", "md5": "be3f0fb2d9ea4d2cb855a0b312fc104d", "sha256": "f1f7a0963b1429035aabe2b04283ff3cd8aba7c894d8333dc4f4b8fbbcafbb36"}, "downloads": -1, "filename": "dev_cmd-0.32.1-py3-none-any.whl", "has_sig": false, "md5_digest": "be3f0fb2d9ea4d2cb855a0b312fc104d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 178174, "upload_time": "2025-07-28T18:06:11", "upload_time_iso_8601": "2025-07-28T18:06:11.658374Z", "url": "https://files.pythonhosted.org/packages/8b/dd/4772afe91be92c48ecc05d0d8d83c6b5f9672b99579c0a1681d4b1003f40/dev_cmd-0.32.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "ee2549a25204433c59d380a7d521b793d53dc57bc0c3614c17959fb9f634c56a", "md5": "5dde61d003de72838e7146c7d265c777", "sha256": "1bdfcfac578c39be5fefac9bfc65acc26210d5acb3ffb2b44010a89efbe6a56d"}, "downloads": -1, "filename": "dev_cmd-0.32.1.tar.gz", "has_sig": false, "md5_digest": "5dde61d003de72838e7146c7d265c777", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 66213, "upload_time": "2025-07-28T18:06:13", "upload_time_iso_8601": "2025-07-28T18:06:13.954099Z", "url": "https://files.pythonhosted.org/packages/ee/25/49a25204433c59d380a7d521b793d53dc57bc0c3614c17959fb9f634c56a/dev_cmd-0.32.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:06:05 GMT", "package": "actualpy", "version": "0.13.1", "json": {"info": {"author": null, "author_email": "Brunno Vanelli <brunnovanelli@gmail.com>", "bugtrack_url": null, "classifiers": ["Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Programming Language :: Python :: 3.9"], "description": "[![tests](https://github.com/bvanelli/actualpy/workflows/Tests/badge.svg)](https://github.com/bvanelli/actualpy/actions)\n[![codecov](https://codecov.io/github/bvanelli/actualpy/graph/badge.svg?token=N6V05MY70U)](https://codecov.io/github/bvanelli/actualpy)\n[![version](https://img.shields.io/pypi/v/actualpy.svg?color=52c72b)](https://pypi.org/project/actualpy/)\n[![pyversions](https://img.shields.io/pypi/pyversions/actualpy.svg)](https://pypi.org/project/actualpy/)\n[![docs](https://readthedocs.org/projects/actualpy/badge/?version=latest)](https://actualpy.readthedocs.io/)\n[![codestyle](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\n[![ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/actualpy)](https://pypistats.org/packages/actualpy)\n\n# actualpy\n\nPython API implementation for Actual server.\n\n[Actual Budget](https://actualbudget.org/) is a superfast and privacy-focused app for managing your finances.\n\n> [!WARNING]\n> The [Javascript API](https://actualbudget.org/docs/api/) to interact with Actual server already exists,\n> and is battle-tested as it is the core of the Actual frontend libraries. If you intend to use a reliable and well\n> tested library, that is the way to go.\n\n# Installation\n\nInstall it via Pip:\n\n```bash\npip install actualpy\n```\n\nIf you want to have the latest git version, you can also install using the repository url:\n\n```bash\npip install git+https://github.com/bvanelli/actualpy.git\n```\n\nFor querying basic information, you additionally install the CLI, checkout the\n[basic documentation](https://actualpy.readthedocs.io/en/latest/command-line-interface/)\n\n# Basic usage\n\nThe most common usage would be downloading a budget to more easily build queries. This would you could handle the\nActual database using SQLAlchemy instead of having to retrieve the data via the export. The following script will print\nevery single transaction registered on the Actual budget file:\n\n```python\nfrom actual import Actual\nfrom actual.queries import get_transactions\n\nwith Actual(\n        base_url=\"http://localhost:5006\",  # Url of the Actual Server\n        password=\"<your_password>\",  # Password for authentication\n        encryption_password=None,  # Optional: Password for the file encryption. Will not use it if set to None.\n        # Set the file to work with. Can be either the file id or file name, if name is unique\n        file=\"<file_id_or_name>\",\n        # Optional: Directory to store downloaded files. Will use a temporary if not provided\n        data_dir=\"<path_to_data_directory>\",\n        # Optional: Path to the certificate file to use for the connection, can also be set as False to disable SSL verification\n        cert=\"<path_to_cert_file>\"\n) as actual:\n    transactions = get_transactions(actual.session)\n    for t in transactions:\n        account_name = t.account.name if t.account else None\n        category = t.category.name if t.category else None\n        print(t.date, account_name, t.notes, t.amount, category)\n```\n\nThe `file` will be matched to either one of the following:\n\n- The name of the budget, found top the top left cornet\n- The ID of the budget, a UUID that is only available if you inspect the result of the method `list_user_files`\n- The Sync ID of the budget, a UUID available on the frontend on the \"Advanced options\"\n- If none of those options work for you, you can search for the file manually with `list_user_files` and provide the\n  object directly:\n\n```python\nfrom actual import Actual\n\nwith Actual(\"http://localhost:5006\", password=\"mypass\") as actual:\n    actual.set_file(actual.list_user_files().data[0])\n    actual.download_budget()\n```\n\nCheckout [the full documentation](https://actualpy.readthedocs.io) for more examples.\n\n# Understanding how Actual handles changes\n\nThe Actual budget is stored in a sqlite database hosted on the user's browser. This means all your data is fully local\nand can be encrypted with a local key, so that not even the server can read your statements.\n\nThe Actual Server is a way of only hosting files and changes. Since re-uploading the full database on every single\nchange is too heavy, Actual only stores one state of the \"base database\" and everything added by the user via frontend\nor via the APIs are individual changes applied on top. This means that on every change, done locally, the frontend\ndoes a SYNC request with a list of the following string parameters:\n\n- `dataset`: the name of the table where the change happened.\n- `row`: the row identifier for the entry that was added/update. This would be the primary key of the row (a uuid value)\n- `column`: the column that had the value changed\n- `value`: the new value. Since it's a string, the values are either prefixed by `S:` to denote a string, `N:` to denote\n  a numeric value and `0:` to denote a null value.\n\nAll individual column changes are computed for an insert or update, serialized with protobuf and sent to the server to\nbe stored. Null values and server defaults are not required to be present in the SYNC message, unless a column is\nchanged to null. If the file is encrypted, the protobuf content will also be encrypted, so that the server does not know\nwhat was changed.\n\nNew clients can use this individual changes to then update their local copies. Whenever a SYNC request is done, the\nresponse will also contain changes that might have been done in other browsers, so that the user is informated about\nthe latest information.\n\nBut this also means that new users need to download a long list of changes, possibly making the initialization slow.\nThankfully, the user is also allowed to reset the sync. When doing a reset of the file via frontend, the browser is then\nresetting the file completely and clearing the list of changes. This would make sure all changes are actually stored in\nthe \"base database\". This is done on the frontend under *Settings > Reset sync*, and causes the current file to be\nreset (removed from the server) and re-uploaded again, with all changes already in place.\n\nThis means that, when using this library to operate changes on the database, you have to make sure that either:\n\n- do a sync request is made using the `actual.commit()` method. This only handles pending operations that haven't yet\n  been committed, generates a change list with them and posts them on the sync endpoint.\n- do a full re-upload of the database is done.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": "actual, actualbudget, api, client", "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "actualpy", "package_url": "https://pypi.org/project/actualpy/", "platform": null, "project_url": "https://pypi.org/project/actualpy/", "project_urls": {"Bug Tracker": "https://github.com/bvanelli/actualpy/issues", "Documentation": "https://actualpy.readthedocs.io/", "Homepage": "https://github.com/bvanelli/actualpy", "Repository": "https://github.com/bvanelli/actualpy.git"}, "provides_extra": ["cli"], "release_url": "https://pypi.org/project/actualpy/0.13.1/", "requires_dist": ["requests>=2", "sqlmodel>=0.0.18", "pydantic<3,>=2", "sqlalchemy>=2", "proto-plus>=1", "protobuf>=4", "cryptography>=42", "python-dateutil>=2.9.0", "rich>=13; extra == \"cli\"", "typer>=0.12.0; extra == \"cli\"", "pyyaml>=6; extra == \"cli\""], "requires_python": ">=3.9.0", "summary": "Implementation of the Actual API to interact with Actual over Python.", "version": "0.13.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388379, "urls": [{"comment_text": null, "digests": {"blake2b_256": "0e8b4a632e0fdaea015e5a3e836e23477a9c3e46093fdb23b8afc76c780372aa", "md5": "d7206ebaaf70091c1867a3640ab34f28", "sha256": "1b9c832af075c4246d3f0d667e7d609fd292de3429be0d678440c27b1e20d7ab"}, "downloads": -1, "filename": "actualpy-0.13.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d7206ebaaf70091c1867a3640ab34f28", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9.0", "size": 64916, "upload_time": "2025-07-28T18:06:05", "upload_time_iso_8601": "2025-07-28T18:06:05.935060Z", "url": "https://files.pythonhosted.org/packages/0e/8b/4a632e0fdaea015e5a3e836e23477a9c3e46093fdb23b8afc76c780372aa/actualpy-0.13.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "b1696ec2656f9694033f12db12af339ac2d7de4a282b9da5a245f258329a5a5d", "md5": "af1cfd5320dcb462b833adc64cc797f6", "sha256": "e2042ca8a68b61e0e018d182f328713c8fd7480bc7ff104c57555d6834e56cda"}, "downloads": -1, "filename": "actualpy-0.13.1.tar.gz", "has_sig": false, "md5_digest": "af1cfd5320dcb462b833adc64cc797f6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9.0", "size": 78105, "upload_time": "2025-07-28T18:06:07", "upload_time_iso_8601": "2025-07-28T18:06:07.581502Z", "url": "https://files.pythonhosted.org/packages/b1/69/6ec2656f9694033f12db12af339ac2d7de4a282b9da5a245f258329a5a5d/actualpy-0.13.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:05:29 GMT", "package": "pageai-sdk", "version": "0.6.0.167", "json": {"info": {"author": "OpenAPI Generator community", "author_email": "team@openapitools.org", "bugtrack_url": null, "classifiers": [], "description": "    # Introduction The PageAI (short for Synthetic EPUB) API is capapble of transforming multi page image only PDF files into accessible EPUBs.   # noqa: E501\n    \n", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "OpenAPI, OpenAPI-Generator, PageAI API", "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "pageai-sdk", "package_url": "https://pypi.org/project/pageai-sdk/", "platform": null, "project_url": "https://pypi.org/project/pageai-sdk/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/pageai-sdk/0.6.0.167/", "requires_dist": ["urllib3>=1.15", "six>=1.10", "certifi", "python-dateutil"], "requires_python": null, "summary": "PageAI API", "version": "0.6.0.167", "yanked": false, "yanked_reason": null}, "last_serial": 30388472, "urls": [{"comment_text": null, "digests": {"blake2b_256": "aa28c50887bffda08f9a64e07de41aa17065b81f707e2033df7e016f3355a52b", "md5": "063801744bc5ea17c5d4adee844b8c70", "sha256": "346875d8c500145dd9f0c04165c961144014ad4e9c8ffdca31e6605fec406b7c"}, "downloads": -1, "filename": "pageai_sdk-0.6.0.167-py3-none-any.whl", "has_sig": false, "md5_digest": "063801744bc5ea17c5d4adee844b8c70", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 20198, "upload_time": "2025-07-28T18:05:29", "upload_time_iso_8601": "2025-07-28T18:05:29.099789Z", "url": "https://files.pythonhosted.org/packages/aa/28/c50887bffda08f9a64e07de41aa17065b81f707e2033df7e016f3355a52b/pageai_sdk-0.6.0.167-py3-none-any.whl", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:05:24 GMT", "package": "outerbounds", "version": "0.4.8rc1", "json": {"info": {"author": "Outerbounds, Inc.", "author_email": null, "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "License :: Other/Proprietary License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9"], "description": "# Outerbounds\n\nMain package for the Outerbounds platform.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "data science, machine learning, MLOps", "license": "Proprietary", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "outerbounds", "package_url": "https://pypi.org/project/outerbounds/", "platform": null, "project_url": "https://pypi.org/project/outerbounds/", "project_urls": {"Documentation": "https://docs.metaflow.org"}, "provides_extra": null, "release_url": "https://pypi.org/project/outerbounds/0.4.8rc1/", "requires_dist": ["azure-identity<2.0.0,>=1.15.0; extra == \"azure\"", "azure-keyvault-secrets<5.0.0,>=4.7.0; extra == \"azure\"", "azure-storage-blob<13.0.0,>=12.9.0; extra == \"azure\"", "boto3", "google-api-core<3.0.0,>=2.16.1; extra == \"gcp\"", "google-auth<3.0.0,>=2.27.0; extra == \"gcp\"", "google-cloud-secret-manager<3.0.0,>=2.20.0; extra == \"gcp\"", "google-cloud-storage<3.0.0,>=2.14.0; extra == \"gcp\"", "metaflow_checkpoint==0.2.4", "ob-metaflow==2.16.5.1", "ob-metaflow-extensions==1.2.9rc1", "ob-metaflow-stubs==6.0.4.8rc1", "opentelemetry-distro>=0.41b0; extra == \"otel\"", "opentelemetry-exporter-otlp-proto-http>=1.20.0; extra == \"otel\"", "opentelemetry-instrumentation-requests>=0.41b0; extra == \"otel\""], "requires_python": "<4.0,>=3.7", "summary": "More Data Science, Less Administration", "version": "0.4.8rc1", "yanked": false, "yanked_reason": null}, "last_serial": 30388373, "urls": [{"comment_text": "", "digests": {"blake2b_256": "64da89fc4019894eb0fd833f593a85a570a1fc9018f257c6c97a6332c64aae39", "md5": "2051a0a9b931347940130c7e01538c20", "sha256": "c4261751f277fd53d86862616dc4c7a468e6258ff19bc5b19556b3b7a631f1b4"}, "downloads": -1, "filename": "outerbounds-0.4.8rc1-py3-none-any.whl", "has_sig": false, "md5_digest": "2051a0a9b931347940130c7e01538c20", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<4.0,>=3.7", "size": 303940, "upload_time": "2025-07-28T18:05:24", "upload_time_iso_8601": "2025-07-28T18:05:24.020521Z", "url": "https://files.pythonhosted.org/packages/64/da/89fc4019894eb0fd833f593a85a570a1fc9018f257c6c97a6332c64aae39/outerbounds-0.4.8rc1-py3-none-any.whl", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:05:17 GMT", "package": "nia-mcp-server", "version": "1.0.13", "json": {"info": {"author": null, "author_email": "Nia Team <founders@nozomio.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: GNU Affero General Public License v3", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9"], "description": "# NIA MCP Server\n\nThe NIA MCP Server enables AI assistants like Claude to search and understand your indexed codebases through the Model Context Protocol (MCP).\n\n## Quick Start\n\n### Automatic Setup (Recommended) \u2728\n\nGet your API key from [https://trynia.ai/api-keys](https://trynia.ai/api-keys) and run:\n\n```bash\npipx run nia-mcp-server setup YOUR_API_KEY\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "ai, codebase, mcp, nia, search", "license": null, "license_expression": "AGPL-3.0", "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "nia-mcp-server", "package_url": "https://pypi.org/project/nia-mcp-server/", "platform": null, "project_url": "https://pypi.org/project/nia-mcp-server/", "project_urls": {"Documentation": "https://docs.trynia.ai", "Homepage": "https://trynia.ai"}, "provides_extra": null, "release_url": "https://pypi.org/project/nia-mcp-server/1.0.13/", "requires_dist": ["httpx>=0.24.0", "mcp>=0.1.0", "pydantic>=2.0.0", "python-dotenv>=1.0.0"], "requires_python": ">=3.8", "summary": "Nia Knowledge Agent", "version": "1.0.13", "yanked": false, "yanked_reason": null}, "last_serial": 30388371, "urls": [{"comment_text": null, "digests": {"blake2b_256": "4995e332c85fbee784cdac0489c1f9f57be83bc407566f3c53321603bd107a3a", "md5": "586fc414e49fff85d1caf7e2b35da059", "sha256": "2dd61f476c4c687f1e565c4c41abedc69f3f13835cfcaada35d250a0cccdf22e"}, "downloads": -1, "filename": "nia_mcp_server-1.0.13-py3-none-any.whl", "has_sig": false, "md5_digest": "586fc414e49fff85d1caf7e2b35da059", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 45593, "upload_time": "2025-07-28T18:05:17", "upload_time_iso_8601": "2025-07-28T18:05:17.565937Z", "url": "https://files.pythonhosted.org/packages/49/95/e332c85fbee784cdac0489c1f9f57be83bc407566f3c53321603bd107a3a/nia_mcp_server-1.0.13-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "a3a03523d5755df8942f9f502340d92cd527917eecf4a1f6e00c13043bbc82ef", "md5": "038d164a0de55abc89e081e5dcd6d087", "sha256": "fc4bd23b26a9f8d3a497e05842e8ebf1e3e1a127682b73ce14e011515a64f385"}, "downloads": -1, "filename": "nia_mcp_server-1.0.13.tar.gz", "has_sig": false, "md5_digest": "038d164a0de55abc89e081e5dcd6d087", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 39250, "upload_time": "2025-07-28T18:05:19", "upload_time_iso_8601": "2025-07-28T18:05:19.671959Z", "url": "https://files.pythonhosted.org/packages/a3/a0/3523d5755df8942f9f502340d92cd527917eecf4a1f6e00c13043bbc82ef/nia_mcp_server-1.0.13.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:04:58 GMT", "package": "getignore3", "version": "3.4.1", "json": {"info": {"author": "ashkanfeyzollahi@gmail.com", "author_email": "ashkanfeyzollahi@gmail.com", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9"], "description": "# GetIgnore3\n\n<p align=\"center\">\ud83d\ude0e Get gitignore files without bothering yourself</p>\n\n```\nusage: getignore [-h] [-L] [-l] [-n] [-c] [-o OUTPUT] [-w]\n                 [template_name ...]\n\nGet gitignore files without bothering yourself\n\npositional arguments:\n  template_name         Name(s) of gitignore templates to fetch\n                        (e.g., Python, Node and etc.)\n\noptions:\n  -h, --help            show this help message and exit\n  -L, --list-cached-templates\n                        List cached gitignore templates\n  -l, --list-templates  List available gitignore templates\n  -n, --no-cache        Don't cache the gitignore template file when\n                        downloaded\n  -c, --offline         Get the cached gitignore template instead of\n                        downloading\n  -o, --output OUTPUT   Where to write the gitignore template\n                        content to\n  -w, --override        Override existing gitignore file instead of\n                        appending\n```\n\n## Installation\n\n- You either install it from `pypi` using `pip`:\n\n```bash\npip install getignore3\n```\n\n- Or directly install it from `github` using `pip`:\n\n```bash\npip install git+https://github.com/ashkanfeyzollahi/getignore3.git\n```\n\n- Or even build it from source!\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "getignore3", "package_url": "https://pypi.org/project/getignore3/", "platform": null, "project_url": "https://pypi.org/project/getignore3/", "project_urls": {"Repository": "https://github.com/ashkanfeyzollahi/getignore3"}, "provides_extra": null, "release_url": "https://pypi.org/project/getignore3/3.4.1/", "requires_dist": ["appdirs<2.0.0,>=1.4.4", "requests<3.0.0,>=2.32.4"], "requires_python": ">=3.8", "summary": "Get gitignore files without bothering yourself", "version": "3.4.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388368, "urls": [{"comment_text": null, "digests": {"blake2b_256": "aba57dff5f4ed50c10de95d2e4f787297bc9f953077c5f0d8464aafee52f5d78", "md5": "79b0add5db69260bea6e7a3cdcc8c8bc", "sha256": "34821e72dadc7a9fa8b43de02eee0c9b2d1c8b875ef74dfe41492ac65cf60a4f"}, "downloads": -1, "filename": "getignore3-3.4.1-py3-none-any.whl", "has_sig": false, "md5_digest": "79b0add5db69260bea6e7a3cdcc8c8bc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 3832, "upload_time": "2025-07-28T18:04:58", "upload_time_iso_8601": "2025-07-28T18:04:58.894590Z", "url": "https://files.pythonhosted.org/packages/ab/a5/7dff5f4ed50c10de95d2e4f787297bc9f953077c5f0d8464aafee52f5d78/getignore3-3.4.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "6b96d9ba169bfe0f5ab02af09c8e55f4b0338a9f6bd6289e4a30d1a8734921b7", "md5": "b29ad4120cd49742df528d8935778b45", "sha256": "30b4512266da2b4ddbd801f0af2d90cef5d1506b89f921735d4f8829999cab23"}, "downloads": -1, "filename": "getignore3-3.4.1.tar.gz", "has_sig": false, "md5_digest": "b29ad4120cd49742df528d8935778b45", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 2902, "upload_time": "2025-07-28T18:04:59", "upload_time_iso_8601": "2025-07-28T18:04:59.805726Z", "url": "https://files.pythonhosted.org/packages/6b/96/d9ba169bfe0f5ab02af09c8e55f4b0338a9f6bd6289e4a30d1a8734921b7/getignore3-3.4.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:04:54 GMT", "package": "ob-metaflow-stubs", "version": "6.0.4.8rc1", "json": {"info": {"author": "Netflix, Outerbounds & the Metaflow Community", "author_email": "help@outerbounds.co", "bugtrack_url": null, "classifiers": [], "description": "# Metaflow Stubs\n\nThis package contains stub files for `metaflow` and thus offers type hints for various editors (such as `VSCode`) and language servers (such as `Pylance`).\n\n## Installation\n\nTo install Metaflow Stubs in your local environment, you can install from [PyPi](https://pypi.org/project/metaflow-stubs/):\n\n```sh\npip install metaflow-stubs\n```\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": "Apache License 2.0", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "ob-metaflow-stubs", "package_url": "https://pypi.org/project/ob-metaflow-stubs/", "platform": null, "project_url": "https://pypi.org/project/ob-metaflow-stubs/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/ob-metaflow-stubs/6.0.4.8rc1/", "requires_dist": null, "requires_python": ">=3.7.0", "summary": "Metaflow Stubs: Stubs for the metaflow package", "version": "6.0.4.8rc1", "yanked": false, "yanked_reason": null}, "last_serial": 30388365, "urls": [{"comment_text": "", "digests": {"blake2b_256": "a9f821ada2e6ef8c3037f8b0381eabc33cd72001f686f55f0328cd6d50ca76d0", "md5": "77b1759008fcb1e1ffda4d21e052efa0", "sha256": "dd3367dbfd05bccd2cc790fedb847542e6c4fd6a7069993fd4b906333290106a"}, "downloads": -1, "filename": "ob_metaflow_stubs-6.0.4.8rc1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "77b1759008fcb1e1ffda4d21e052efa0", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.7.0", "size": 296149, "upload_time": "2025-07-28T18:04:54", "upload_time_iso_8601": "2025-07-28T18:04:54.176736Z", "url": "https://files.pythonhosted.org/packages/a9/f8/21ada2e6ef8c3037f8b0381eabc33cd72001f686f55f0328cd6d50ca76d0/ob_metaflow_stubs-6.0.4.8rc1-py2.py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "249d462d326f59dff1847783f8a957c4b9e755a5a5d8879a1922ca1049fcee0c", "md5": "67dc87247ed5a6b5fcec366d8e9359ea", "sha256": "059521b655242505e7f958c2c05418ec99b3ec9125ef880945c90c5a1b73c43b"}, "downloads": -1, "filename": "ob-metaflow-stubs-6.0.4.8rc1.tar.gz", "has_sig": false, "md5_digest": "67dc87247ed5a6b5fcec366d8e9359ea", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7.0", "size": 166685, "upload_time": "2025-07-28T18:04:55", "upload_time_iso_8601": "2025-07-28T18:04:55.568436Z", "url": "https://files.pythonhosted.org/packages/24/9d/462d326f59dff1847783f8a957c4b9e755a5a5d8879a1922ca1049fcee0c/ob-metaflow-stubs-6.0.4.8rc1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:04:46 GMT", "package": "cybrid-api-organization-python", "version": "0.124.18", "json": {"info": {"author": "Cybrid Support", "author_email": "support@cybrid.app", "bugtrack_url": null, "classifiers": [], "description": "View our documentation on [Github](https://github.com/Cybrid-app/cybrid-api-organization-python/)\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Description", "Description-Content-Type", "Home-Page", "Keywords", "License", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/Cybrid-app/cybrid-api-organization-python/", "keywords": "Cybrid Organization API", "license": "Apache-2.0", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "cybrid-api-organization-python", "package_url": "https://pypi.org/project/cybrid-api-organization-python/", "platform": null, "project_url": "https://pypi.org/project/cybrid-api-organization-python/", "project_urls": {"Homepage": "https://github.com/Cybrid-app/cybrid-api-organization-python/"}, "provides_extra": null, "release_url": "https://pypi.org/project/cybrid-api-organization-python/0.124.18/", "requires_dist": ["urllib3>=1.25.3", "python-dateutil"], "requires_python": ">=3.6", "summary": "Cybrid Organization API", "version": "0.124.18", "yanked": false, "yanked_reason": null}, "last_serial": 30388362, "urls": [{"comment_text": null, "digests": {"blake2b_256": "2230c54842fbd3abe0a1b176ef6782abb5097ae05cae57f06bc796727981067c", "md5": "f31f113a59c594ab9eff833e07af3b9c", "sha256": "f70d2aa848865552ee4a715df721860f7080419c00267ba3ff4a094d5c8c371f"}, "downloads": -1, "filename": "cybrid_api_organization_python-0.124.18-py3-none-any.whl", "has_sig": false, "md5_digest": "f31f113a59c594ab9eff833e07af3b9c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 203887, "upload_time": "2025-07-28T18:04:46", "upload_time_iso_8601": "2025-07-28T18:04:46.821434Z", "url": "https://files.pythonhosted.org/packages/22/30/c54842fbd3abe0a1b176ef6782abb5097ae05cae57f06bc796727981067c/cybrid_api_organization_python-0.124.18-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "5dca9be955790c6179ae33db23c9ced6e8929d79acc9bb02a17ce0873d045b67", "md5": "603b56400f8eb0c0fc57aeacee25f828", "sha256": "ce2edeb90f6f27bb199d7a8dc42b7598596c3ece878626b8d09d1c174a5c3308"}, "downloads": -1, "filename": "cybrid_api_organization_python-0.124.18.tar.gz", "has_sig": false, "md5_digest": "603b56400f8eb0c0fc57aeacee25f828", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 105764, "upload_time": "2025-07-28T18:04:51", "upload_time_iso_8601": "2025-07-28T18:04:51.010677Z", "url": "https://files.pythonhosted.org/packages/5d/ca/9be955790c6179ae33db23c9ced6e8929d79acc9bb02a17ce0873d045b67/cybrid_api_organization_python-0.124.18.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:04:42 GMT", "package": "cybrid-api-bank-python", "version": "0.124.18", "json": {"info": {"author": "Cybrid Support", "author_email": "support@cybrid.app", "bugtrack_url": null, "classifiers": [], "description": "View our documentation on [Github](https://github.com/Cybrid-app/cybrid-api-bank-python/)\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Description", "Description-Content-Type", "Home-Page", "Keywords", "License", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/Cybrid-app/cybrid-api-bank-python/", "keywords": "Cybrid Bank API", "license": "Apache-2.0", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "cybrid-api-bank-python", "package_url": "https://pypi.org/project/cybrid-api-bank-python/", "platform": null, "project_url": "https://pypi.org/project/cybrid-api-bank-python/", "project_urls": {"Homepage": "https://github.com/Cybrid-app/cybrid-api-bank-python/"}, "provides_extra": null, "release_url": "https://pypi.org/project/cybrid-api-bank-python/0.124.18/", "requires_dist": ["urllib3>=1.25.3", "python-dateutil"], "requires_python": ">=3.6", "summary": "Cybrid Bank API", "version": "0.124.18", "yanked": false, "yanked_reason": null}, "last_serial": 30388358, "urls": [{"comment_text": null, "digests": {"blake2b_256": "9ef83a8888299eb1aac8101ab6e19e8650874a05c698b9184b2887ac5a416a99", "md5": "1eb767952e9407ffbdfbcb265f4c4362", "sha256": "844826d942375c72ea969ad20973d80a2213e8628ce43368a2d98c87549bda06"}, "downloads": -1, "filename": "cybrid_api_bank_python-0.124.18-py3-none-any.whl", "has_sig": false, "md5_digest": "1eb767952e9407ffbdfbcb265f4c4362", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 1337018, "upload_time": "2025-07-28T18:04:42", "upload_time_iso_8601": "2025-07-28T18:04:42.619620Z", "url": "https://files.pythonhosted.org/packages/9e/f8/3a8888299eb1aac8101ab6e19e8650874a05c698b9184b2887ac5a416a99/cybrid_api_bank_python-0.124.18-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "cdfff10e65dbd8f2df01c52227adcc1dbc337a8b4609b9eab460135892bff598", "md5": "20e926d9dbf772264a90a3a8725c0398", "sha256": "a55866ff0286bb2441a6ab1be91421b4cd72a5544a58cba9af005121c998ff1d"}, "downloads": -1, "filename": "cybrid_api_bank_python-0.124.18.tar.gz", "has_sig": false, "md5_digest": "20e926d9dbf772264a90a3a8725c0398", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 554135, "upload_time": "2025-07-28T18:04:44", "upload_time_iso_8601": "2025-07-28T18:04:44.876495Z", "url": "https://files.pythonhosted.org/packages/cd/ff/f10e65dbd8f2df01c52227adcc1dbc337a8b4609b9eab460135892bff598/cybrid_api_bank_python-0.124.18.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:04:42 GMT", "package": "trackinglog", "version": "0.2.0.1", "json": {"info": {"author": "Yinghao Li", "author_email": "shiyi.yinghao@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# TrackingLog\n\n[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![PyPI](https://img.shields.io/pypi/v/trackinglog)](https://pypi.org/project/trackinglog/)\n[![GitHub](https://img.shields.io/badge/github-trackinglog-green)](https://github.com/shiyi-yinghao/trackinglog)\n\nTrackingLog is a comprehensive Python logging package designed for tracing function calls with advanced features including error handling, profiling, task management, and email notifications. It provides decorators and managers for comprehensive application monitoring and debugging.\n\n## \ud83d\ude80 Features\n\n- **Function & Class Logging**: Decorators for automatic entry/exit logging\n- **Profiling Integration**: Line-level and function-level performance profiling\n- **Task Management**: Structured task lifecycle management with resume capabilities\n- **Error Handling**: Comprehensive error tracking and logging\n- **Email Notifications**: Built-in email notification system\n- **Print Capture**: Redirect print statements to log files\n- **Resource Monitoring**: CPU and memory usage tracking\n- **Cache Management**: Automatic log cleanup and rotation\n- **Flexible Configuration**: Centralized configuration management\n\n## \ud83d\udce6 Installation\n\n```bash\npip install trackinglog\n```\n\n### Dependencies\n\n- `numpy>=2.0.1`\n- `pandas>=2.2.2`  \n- `line_profiler>=4.1.3`\n\n## \ud83d\udd27 Quick Start\n\n### Basic Setup\n\n```python\nimport trackinglog\n\n# Basic setup with default configuration\ntrackinglog.logger.setup()\n\n@trackinglog.logger.get_log('my_logger')\ndef my_function(log=None):\n    log.info(\"This function does something important.\")\n    return \"Function completed\"\n\nresult = my_function()\n```\n\n### Custom Task Setup\n\n```python\nimport trackinglog\n\n# Custom setup with specific task name and path\ntrackinglog.logger.setup(\n    task_name=\"My_Project\",\n    root_folder_path='./my_logs'\n)\n\n@trackinglog.logger.get_log('custom_logger', verbose=1, enable_profiling=\"function\")\ndef compute_data(x, y, log=None):\n    log.info(f\"Computing {x} + {y}\")\n    result = x + y\n    log.info(f\"Result: {result}\")\n    return result\n\nresult = compute_data(5, 3)\n```\n\n## \ud83d\udcda Usage Examples\n\n### Function Decorator\n\n```python\nimport trackinglog\n\ntrackinglog.logger.setup(task_name=\"FunctionExample\")\n\n# Basic function logging\n@trackinglog.logger.get_log('function_logger', verbose=1)\ndef process_data(data, log=None):\n    log.info(f\"Processing {len(data)} items\")\n    processed = [x * 2 for x in data]\n    log.info(\"Processing completed\")\n    return processed\n\n# Function with line-level profiling\n@trackinglog.logger.get_log('profiled_func', verbose=1, enable_profiling=\"line\")\ndef intensive_computation(n, log=None):\n    log.info(f\"Starting computation for {n} iterations\")\n    result = sum(i**2 for i in range(n))\n    log.info(f\"Computation result: {result}\")\n    return result\n\nresult = process_data([1, 2, 3, 4, 5])\nprofiled_result = intensive_computation(1000)\n```\n\n### Class Decorator\n\n```python\nimport trackinglog\n\ntrackinglog.logger.setup(task_name=\"ClassExample\")\n\n@trackinglog.logger.get_log('class_logger', verbose=1, print2log=True)\nclass DataProcessor:\n    def __init__(self, name):\n        self.name = name\n        self.log.info(f\"DataProcessor '{name}' initialized\")\n    \n    def process(self, data):\n        self.log.info(f\"Processing data with {len(data)} items\")\n        print(f\"Processing {self.name}\")  # This will be captured in logs\n        return [x * 2 for x in data]\n    \n    def get_stats(self):\n        self.log.info(\"Generating statistics\")\n        # Access folder structure\n        print(f\"Results folder: {self.log.folder.result}\")\n        return {\"processed\": True}\n\nprocessor = DataProcessor(\"MyProcessor\")\nresult = processor.process([1, 2, 3])\nstats = processor.get_stats()\n```\n\n### Inline Logging\n\n```python\nimport trackinglog\n\ntrackinglog.logger.setup(task_name=\"InlineExample\")\n\ndef regular_function(x, y):\n    # Get logger inside function\n    log = trackinglog.logger.get_logger('inline_logger')\n    log.info(f\"Function called with {x}, {y}\")\n    \n    result = x * y\n    log.info(f\"Calculation result: {result}\")\n    return result\n\nresult = regular_function(6, 7)\n```\n\n## \u2699\ufe0f Configuration\n\n### Comprehensive Setup\n\n```python\nimport trackinglog\n\ntrackinglog.logger.setup(\n    task_name=\"ComprehensiveExample\",\n    root_folder_path='./task_logs',\n    task_config={\n        \"task_num_limit\": 5,\n        \"task_expiration_date\": 7,  # Keep tasks for 7 days\n        \"resume_task\": False,       # Create new task\n        \"new_task\": None\n    },\n    log_config={\n        'root_log_path': \"./logs\",\n        '_cache_log_path': \"./logs/cache\",\n        'cache_log_num_limit': 10,\n        '_cache_log_day_limit': 7\n    },\n    email_credential={\n        'username': \"your_email@example.com\",\n        'password': \"your_password\",\n        'root_emails_folder': \"./logs/emails\"\n    },\n    lock_config={\n        \"lock_folder_path\": \"./logs/locks\"\n    }\n)\n```\n\n### Configuration Parameters\n\n| Parameter | Description | Default |\n|-----------|-------------|---------|\n| `task_name` | Name for the current task | `\"Default_Task\"` |\n| `root_folder_path` | Root directory for all task files | `\"./cache/__trackinglog__\"` |\n| `task_config` | Task management settings | See below |\n| `log_config` | Logging configuration | See below |\n| `email_credential` | Email notification settings | None |\n| `lock_config` | File locking configuration | None |\n\n#### Task Config Options\n\n```python\ntask_config = {\n    \"task_num_limit\": 500,           # Maximum number of tasks to keep\n    \"task_expiration_date\": 30,      # Days to keep tasks\n    \"task_folder_format\": \"%y%m%d_000001\",  # Task folder naming format\n    \"resume_task\": None,             # None: auto, True: resume latest, False: new task\n    \"new_task\": None                 # None: auto, True: force new, False: resume only\n}\n```\n\n#### Log Config Options\n\n```python\nlog_config = {\n    'root_log_path': \"./logs\",           # Main log directory\n    '_cache_log_path': \"./logs/cache\",   # Cache log directory\n    'cache_log_num_limit': 50,           # Max cache log files\n    '_cache_log_day_limit': 30           # Days to keep cache logs\n}\n```\n\n## \ud83c\udfd7\ufe0f Task Management\n\nTrackingLog creates a structured folder hierarchy for each task:\n\n```\ntask_folder/\n\u251c\u2500\u2500 root/           # Main task directory\n\u251c\u2500\u2500 temp/           # Temporary files\n\u251c\u2500\u2500 cache/          # Cache files\n\u251c\u2500\u2500 var/            # Variable data\n\u251c\u2500\u2500 result/         # Task results\n\u2514\u2500\u2500 __config/       # Task configuration and status\n```\n\n### Task Status Management\n\n```python\nimport trackinglog\nfrom datetime import datetime\n\ntrackinglog.logger.setup(task_name=\"StatusExample\")\n\n# Get folder configuration\nfolder_config = trackinglog.logger.config.task_config._folder_path_config\n\n# Set task as in progress\nprogress_config = {\n    \"step\": 1,\n    \"description\": \"Processing data\",\n    \"timestamp\": datetime.now().isoformat()\n}\nfolder_config.inprogress(progress_config)\n\n# Mark as finished\nfinish_config = {\n    \"step\": 2,\n    \"description\": \"Task completed successfully\",\n    \"result\": \"All operations completed\",\n    \"timestamp\": datetime.now().isoformat()\n}\nfolder_config.finish(finish_config)\n\n# Check status\nprint(f\"Task status: {folder_config.status}\")\nprint(f\"Task config: {folder_config.config}\")\n```\n\n### Task Resume\n\n```python\nimport trackinglog\n\n# Create new task\ntrackinglog.logger.setup(\n    task_name=\"ResumeExample\",\n    task_config={\"resume_task\": False}  # Force new task\n)\n\n# Later, resume the latest task\ntrackinglog.logger.setup(\n    task_name=\"ResumeExample\", \n    task_config={\"resume_task\": \"LATEST\"}  # Resume latest task\n)\n```\n\n## \ud83d\udd0d Profiling\n\n### Function-Level Profiling\n\n```python\n@trackinglog.logger.get_log('profiler', enable_profiling=\"function\")\ndef compute_heavy(n, log=None):\n    total = 0\n    for i in range(n):\n        total += i ** 2\n    return total\n```\n\n### Line-Level Profiling\n\n```python\n@trackinglog.logger.get_log('line_profiler', enable_profiling=\"line\")\ndef detailed_analysis(data, log=None):\n    # Each line will be profiled\n    processed = []\n    for item in data:\n        result = item * 2 + 1\n        processed.append(result)\n    return processed\n```\n\n## \ud83d\udce7 Email Notifications\n\n```python\nimport trackinglog\n\n# Setup with email credentials\ntrackinglog.logger.setup(\n    task_name=\"EmailExample\",\n    email_credential={\n        'username': \"your_email@example.com\",\n        'password': \"your_app_password\",\n        'root_emails_folder': \"./emails\"\n    }\n)\n\n# Email notifications will be sent for errors and task completion\n```\n\n## \ud83d\udea8 Error Handling\n\n```python\nimport trackinglog\n\ntrackinglog.logger.setup(task_name=\"ErrorExample\")\n\n@trackinglog.logger.get_log('error_logger', verbose=1)\ndef risky_function(should_fail=False, log=None):\n    log.info(\"Function started\")\n    \n    if should_fail:\n        log.error(\"About to raise an error\")\n        raise ValueError(\"This is a test error\")\n    \n    log.info(\"Function completed successfully\")\n    return \"Success\"\n\n# Successful execution\ntry:\n    result = risky_function(should_fail=False)\n    print(f\"Result: {result}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n# Error case - will be logged automatically\ntry:\n    result = risky_function(should_fail=True)\nexcept Exception as e:\n    print(f\"Caught error: {e}\")\n    \n    # Manually mark task as failed\n    folder_config = trackinglog.logger.config.task_config._folder_path_config\n    folder_config.fail({\"error\": str(e), \"timestamp\": \"2024-01-01T12:00:00\"})\n```\n\n## \ud83c\udfaf Multiple Loggers\n\n```python\nimport trackinglog\n\ntrackinglog.logger.setup(task_name=\"MultiLoggerExample\")\n\n# Create different loggers for different components\nauth_logger = trackinglog.logger.get_logger('auth_system')\ndb_logger = trackinglog.logger.get_logger('database')\napi_logger = trackinglog.logger.get_logger('api_handler')\n\n# Use them independently\nauth_logger.info(\"User authentication started\")\ndb_logger.info(\"Database connection established\")\napi_logger.info(\"API request received\")\n\n# Or use with decorators\n@trackinglog.logger.get_log('auth_decorator', verbose=1)\ndef authenticate_user(username, log=None):\n    log.info(f\"Authenticating user: {username}\")\n    return f\"User {username} authenticated\"\n\n@trackinglog.logger.get_log('db_decorator', verbose=1)  \ndef query_database(query, log=None):\n    log.info(f\"Executing query: {query}\")\n    return \"Query results\"\n```\n\n## \ud83d\udcd6 API Reference\n\n### LogManager Methods\n\n- `setup(**config)`: Configure the logging system\n- `get_log(name, verbose=0, enable_profiling=None, print2log=False)`: Decorator for functions/classes\n- `get_logger(name)`: Get logger instance for inline usage\n\n### Decorator Parameters\n\n- `name`: Logger name (string)\n- `verbose`: Logging verbosity level (0, 1, 2)\n- `enable_profiling`: Profiling mode (\"function\", \"line\", or None)\n- `print2log`: Capture print statements in logs (boolean)\n\n### Task Status Methods\n\n- `folder_config.inprogress(config)`: Mark task as in progress\n- `folder_config.finish(config)`: Mark task as finished\n- `folder_config.fail(config)`: Mark task as failed\n- `folder_config.status`: Get current task status\n- `folder_config.config`: Get current task configuration\n\n## \ud83d\udccb Version History\n\n- **0.1.9.1**: Default task name and root folder path, usage without setup, increased task limit to 500\n- **0.1.9**: Optimized log logic and bug fixes\n- **0.1.8.6**: Enhanced resume task mechanism with True option for latest task\n- **0.1.8.5**: New resume task mechanism with boolean controls\n- **0.1.8.4**: Enhanced task status management (finish, inprogress, fail)\n- **0.1.8.3**: Enhanced task manager with finish tokens\n- **0.1.8**: Embedded task manager and optimized cache logic\n- **0.1.7**: Added error handling message mechanism\n- **0.1.6.2**: Bug fixes\n- **0.1.6**: Updated to Parameter config, added inline log usage\n- **0.1.5**: Added cache log cleaner\n- **0.1.4**: Formatted logging with indentation, print to log feature\n- **0.1.3**: Added profiler, error handling, class name logging, resource tracking\n- **0.1.2**: Added error handling, verbose option, function name logging\n- **0.1.1**: Created decorators and setup check\n- **0.1.0**: Package draft created\n\n## \ud83e\udd1d Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## \ud83d\udcc4 License\n\nThis project is licensed under the MIT License.\n\n## \ud83d\udd17 Links\n\n- **GitHub**: https://github.com/shiyi-yinghao/trackinglog\n- **PyPI**: https://pypi.org/project/trackinglog/\n- **Author**: Yinghao Li (shiyi.yinghao@gmail.com)\n\n## \ud83c\udd98 Support\n\nIf you encounter any issues or have questions, please open an issue on the GitHub repository.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://github.com/shiyi-yinghao/trackinglog", "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "trackinglog", "package_url": "https://pypi.org/project/trackinglog/", "platform": null, "project_url": "https://pypi.org/project/trackinglog/", "project_urls": {"Homepage": "https://github.com/shiyi-yinghao/trackinglog"}, "provides_extra": null, "release_url": "https://pypi.org/project/trackinglog/0.2.0.1/", "requires_dist": ["numpy>=2.0.1", "pandas>=2.2.2", "line-profiler>=4.1.3"], "requires_python": ">=3.9", "summary": "A logging package for tracing function calls with error handling and email notification", "version": "0.2.0.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388357, "urls": [{"comment_text": null, "digests": {"blake2b_256": "3ba4b3962f05bba9f32b5448427875d1be684c13961e3e75e1aa73e4b4a8df12", "md5": "c644a9e741ec1ad0ac7a339f96b834f8", "sha256": "7dab03c0808b25d3d4aa5df138afd1dfb85a87316f01a33d37af5d38a7f8cb73"}, "downloads": -1, "filename": "trackinglog-0.2.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "c644a9e741ec1ad0ac7a339f96b834f8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 25357, "upload_time": "2025-07-28T18:04:42", "upload_time_iso_8601": "2025-07-28T18:04:42.077173Z", "url": "https://files.pythonhosted.org/packages/3b/a4/b3962f05bba9f32b5448427875d1be684c13961e3e75e1aa73e4b4a8df12/trackinglog-0.2.0.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "2f01471bf382227b9407a7aca393a4bb3ac4bb093ed8b0b7b41c502b7786f98a", "md5": "4fdce9fdf1d7adfd4969e5fd1037f61e", "sha256": "07bbdac0449204b5980d95067f56e45e5215ac99bfc40ec4df603b00dc6ad21b"}, "downloads": -1, "filename": "trackinglog-0.2.0.1.tar.gz", "has_sig": false, "md5_digest": "4fdce9fdf1d7adfd4969e5fd1037f61e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 22374, "upload_time": "2025-07-28T18:04:43", "upload_time_iso_8601": "2025-07-28T18:04:43.011135Z", "url": "https://files.pythonhosted.org/packages/2f/01/471bf382227b9407a7aca393a4bb3ac4bb093ed8b0b7b41c502b7786f98a/trackinglog-0.2.0.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:04:37 GMT", "package": "pycarlo", "version": "0.10.48", "json": {"info": {"author": "Monte Carlo Data, Inc", "author_email": "info@montecarlodata.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Software Development :: Build Tools"], "description": "# Pycarlo\n\nMonte Carlo's Alpha Python SDK!\n\n## Installation\n\nRequires Python 3.9 or greater. Normally you can install and update using pip. For instance:\n\n```shell\nvirtualenv venv\n. venv/bin/activate\n\npip install -U pycarlo\n```\n\nDevelopers of the SDK can use:\n\n```shell\nmake install-with-tests\n. venv/bin/activate\npre-commit install\n```\n\n## Overview\n\nPycarlo comprises two components: `core` and `features`.\n\nAll Monte Carlo API queries and mutations that you could execute via the API are supported via the `core` library. Operations can be executed as first class objects, using [sgqlc](https://github.com/profusion/sgqlc), or as raw GQL with variables. In both cases, a consistent object where fields can be referenced by dot notation and the more pythonic snake_case is returned for ease of use.\n\nThe `features` library provides additional convenience for performing common operations like with dbt, circuit breaking, and pii filtering.\n\nNote that an API Key is required to use the SDK. See [here](https://docs.getmontecarlo.com/docs/developer-resources#creating-an-api-key) for details on how to generate one.\n\n## Basic usage\n\n### Core\n\n```python\nfrom pycarlo.core import Client, Query, Mutation\n\n# First create a client. This creates a session using the 'default' profile from\n# '~/.mcd/profiles.ini'. This profile is created automatically via\n# `montecarlo configure` on the CLI. See the session subsection for\n# customizations, options and alternatives (e.g. using the environment, params,\n# named profiles, etc.)\nclient = Client()\n\n# Now you can can execute a query. For instance, getUser (selecting the email field).\n# This would be like executing -\n#     curl --location --request POST 'https://api.getmontecarlo.com/graphql' \\\n#     --header 'x-mcd-id: <ID>' \\\n#     --header 'x-mcd-token: <TOKEN>' \\\n#     --header 'Content-Type: application/json' \\\n#     --data-raw '{\"query\": \"query {getUser {email}}\"}'\n# Notice how the CamelCase from the Graphql query is converted to snake_case in\n# both the request and response.\nquery = Query()\nquery.get_user.__fields__('email')\nprint(client(query).get_user.email)\n\n# You can also execute a query that requires variables. For instance,\n# testTelnetConnection (selecting all fields).\nquery = Query()\nquery.test_telnet_connection(host='montecarlodata.com', port=443)\nprint(client(query))\n\n# If necessary, you can always generate (e.g. print) the raw query that would be executed.\nprint(query)\n# query {\n#   testTelnetConnection(host: \"montecarlodata.com\", port: 443) {\n#     success\n#     validations {\n#       type\n#       message\n#     }\n#     warnings {\n#       type\n#       message\n#     }\n#   }\n# }\n\n# If you are not a fan of sgqlc operations (Query and Mutation) you can also execute any\n# raw query using the client. For instance, if we want the first 10 tables from getTables.\nget_table_query = \"\"\"\nquery getTables{\n  getTables(first: 10) {\n    edges {\n      node {\n        fullTableId\n      }\n    }\n  }\n}\n\"\"\"\nresponse = client(get_table_query)\n# This returns a Box object where fields can be accessed using dot notation.\n# Notice how unlike with the API the response uses the more Pythonic snake_case.\nfor edge in response.get_tables.edges:\n    print(edge.node.full_table_id)\n# The response can still be processed as a standard dictionary.\nprint(response['get_tables']['edges'][0]['node']['full_table_id'])\n\n# You can also execute any mutations too. For instance, generateCollectorTemplate\n# (selecting the templateLaunchUrl).\nmutation = Mutation()\nmutation.generate_collector_template().dc.template_launch_url()\nprint(client(mutation))\n\n# Any errors will raise a GqlError with details. For instance, executing above with an\n# invalid region.\nmutation = Mutation()\nmutation.generate_collector_template(region='artemis')\nprint(client(mutation))\n# pycarlo.common.errors.GqlError: [\n#   {'message': 'Region \"\\'artemis\\'\" not currently active.'...\n# ]\n```\n\nNote that you can find Monte Carlo's API reference [here](https://apidocs.getmontecarlo.com/).\n\nFor details and additional examples on how to map (convert) GraphQL queries to `sgqlc` operations please refer to the docs [here](https://sgqlc.readthedocs.io/en/latest/sgqlc.operation.html).\n\n### Features\n\nYou can use [pydoc](https://docs.python.org/3.8/library/pydoc.html) to retrieve documentation on any feature packages (`pydoc pycarlo.features`).\n\nFor instance for [circuit breakers](https://docs.getmontecarlo.com/docs/circuit-breakers):\n\n```shell\npydoc pycarlo.features.circuit_breakers.service\n```\n\n## Session configuration\n\nBy default, when creating a client the `default` profile from `~/.mcd/profiles.ini` is used. This file created via [montecarlo configure](https://docs.getmontecarlo.com/docs/using-the-cli#setting-up-the-cli) on the CLI. Note that you can find Monte Carlo's CLI reference [here](https://clidocs.getmontecarlo.com/).\n\nYou can override this usage by creating a custom `Session`. For instance, if you want to pass the ID and Token:\n\n```python\nfrom pycarlo.core import Client, Session\n\nclient = Client(session=Session(mcd_id='foo', mcd_token='bar'))\n```\n\nSessions support the following params:\n\n- `mcd_id`: API Key ID.\n- `mcd_token`: API secret.\n- `mcd_profile`: Named profile containing credentials. This is created via the CLI (e.g. `montecarlo configure --profile-name zeus`).\n- `mcd_config_path`: Path to file containing credentials. Defaults to `~/.mcd/`.\n\nYou can also specify the API Key, secret or profile name using the following environment variables:\n\n- `MCD_DEFAULT_API_ID`\n- `MCD_DEFAULT_API_TOKEN`\n- `MCD_DEFAULT_PROFILE`\n\nWhen creating a session any explicitly passed `mcd_id` and `mcd_token` params take precedence, followed by environmental variables and then any config-file options.\n\nEnvironment variables can be mixed with passed credentials, but not the config-file profile.\n\n**We do not recommend passing `mcd_token` as it is a secret and can be accidentally committed.**\n\n## Integration Gateway API\n\nThere are features that require the Integration Gateway API instead of the regular GraphQL Application API, for example Airflow Callbacks invoked by the `airflow-mcd` library.\n\nTo use the Gateway you need to initialize the `Session` object passing a `scope` parameter and then use `make_request` to invoke Gateway endpoints:\n\n```python\nfrom pycarlo.core import Client, Session\n\nclient = Client(session=Session(mcd_id='foo', mcd_token='bar', scope='AirflowCallbacks'))\nresponse = client.make_request(\n  path='/airflow/callbacks', method='POST', body={}, timeout_in_seconds=20\n)\n```\n\n## Advanced configuration\n\nThe following values also be set by the environment:\n\n- `MCD_VERBOSE_ERRORS`: Enable logging. This includes a trace ID for each session and request.\n- `MCD_API_ENDPOINT`: Customize the endpoint where queries and mutations are executed.\n\n## Tests and releases\n\nTo update queries and mutations via introspection, use `make generate`.\n\n`make test` can be used to run all tests locally. CircleCI manages all testing for deployment. When ready for a review, create a PR against `main`.\n\nWhen ready to release, create a new [Github release](https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository) with a tag using semantic versioning (e.g. `v0.42.0`) and CircleCI will test and publish to PyPI. Note that an existing version will not be deployed.\n\n## References\n\n- Dashboard: <https://getmontecarlo.com>\n- Product docs: <https://docs.getmontecarlo.com>\n- Status page: <https://status.getmontecarlo.com>\n- API (and SDK): <https://apidocs.getmontecarlo.com>\n- CLI: <https://clidocs.getmontecarlo.com>\n\n## License\n\nApache 2.0 - See the [LICENSE](http://www.apache.org/licenses/LICENSE-2.0) for more information.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://www.montecarlodata.com/", "keywords": null, "license": "Apache Software License (Apache 2.0)", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "pycarlo", "package_url": "https://pypi.org/project/pycarlo/", "platform": null, "project_url": "https://pypi.org/project/pycarlo/", "project_urls": {"Homepage": "https://www.montecarlodata.com/"}, "provides_extra": null, "release_url": "https://pypi.org/project/pycarlo/0.10.48/", "requires_dist": ["dataclasses-json<6.0.0,>=0.5.7", "python-box>=5.0.0", "requests<3.0.0,>=2.0.0", "responses>=0.20.0", "sgqlc<17.0,>=14.1"], "requires_python": ">=3.8", "summary": "Monte Carlo's Python SDK", "version": "0.10.48", "yanked": false, "yanked_reason": null}, "last_serial": 30388352, "urls": [{"comment_text": "", "digests": {"blake2b_256": "b08a8143da5f916b4c3ce6084e70107ee1cc6d520d78dd1970746971ceb0935b", "md5": "93415e28a5d35cc61196dda7f70ff713", "sha256": "1fedd5fe66c57a11a5378b6003d3f210fae7343fa425449c7ad1347748bacdee"}, "downloads": -1, "filename": "pycarlo-0.10.48-py3-none-any.whl", "has_sig": false, "md5_digest": "93415e28a5d35cc61196dda7f70ff713", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 647910, "upload_time": "2025-07-28T18:04:37", "upload_time_iso_8601": "2025-07-28T18:04:37.120545Z", "url": "https://files.pythonhosted.org/packages/b0/8a/8143da5f916b4c3ce6084e70107ee1cc6d520d78dd1970746971ceb0935b/pycarlo-0.10.48-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "1b75832b82e471787121df704a73d33058d637355f7a4c667d8905d5965495d9", "md5": "04ce0bdf89608b7e436a073d7bb31599", "sha256": "084619cd43a629d4e0cc80f945450df317b72b751383ffadad967561e7539f7c"}, "downloads": -1, "filename": "pycarlo-0.10.48.tar.gz", "has_sig": false, "md5_digest": "04ce0bdf89608b7e436a073d7bb31599", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 691495, "upload_time": "2025-07-28T18:04:38", "upload_time_iso_8601": "2025-07-28T18:04:38.441071Z", "url": "https://files.pythonhosted.org/packages/1b/75/832b82e471787121df704a73d33058d637355f7a4c667d8905d5965495d9/pycarlo-0.10.48.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:04:12 GMT", "package": "cybrid-api-id-python", "version": "0.124.18", "json": {"info": {"author": "Cybrid Support", "author_email": "support@cybrid.app", "bugtrack_url": null, "classifiers": [], "description": "View our documentation on [Github](https://github.com/Cybrid-app/cybrid-api-id-python/)\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Description", "Description-Content-Type", "Home-Page", "Keywords", "License", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/Cybrid-app/cybrid-api-id-python/", "keywords": "Cybrid Identity API", "license": "Apache-2.0", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "cybrid-api-id-python", "package_url": "https://pypi.org/project/cybrid-api-id-python/", "platform": null, "project_url": "https://pypi.org/project/cybrid-api-id-python/", "project_urls": {"Homepage": "https://github.com/Cybrid-app/cybrid-api-id-python/"}, "provides_extra": null, "release_url": "https://pypi.org/project/cybrid-api-id-python/0.124.18/", "requires_dist": ["urllib3>=1.25.3", "python-dateutil"], "requires_python": ">=3.6", "summary": "Cybrid Identity API", "version": "0.124.18", "yanked": false, "yanked_reason": null}, "last_serial": 30388349, "urls": [{"comment_text": null, "digests": {"blake2b_256": "c882c11b6ff04cfea1fd651ca4535b4f2bb264eec005af72ddff8f92226da2b0", "md5": "daa2ae518223419e8b68247419a49da3", "sha256": "b4c5c3c377220de627d5151bc86bd52924549ee3a95ec9a7a4f64cd3c37d21e2"}, "downloads": -1, "filename": "cybrid_api_id_python-0.124.18-py3-none-any.whl", "has_sig": false, "md5_digest": "daa2ae518223419e8b68247419a49da3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 198145, "upload_time": "2025-07-28T18:04:12", "upload_time_iso_8601": "2025-07-28T18:04:12.251258Z", "url": "https://files.pythonhosted.org/packages/c8/82/c11b6ff04cfea1fd651ca4535b4f2bb264eec005af72ddff8f92226da2b0/cybrid_api_id_python-0.124.18-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "30a087f1fa7b1f197ab05dbb6d2cb989e2b4f72e18e8502a669d2116ac44fcca", "md5": "cee9b5eda22df61d4e456d4e8396b491", "sha256": "557c2d36bee03a40ab66e883f7eef6a29c2aabd70c3aa6f70f4a37841387c07a"}, "downloads": -1, "filename": "cybrid_api_id_python-0.124.18.tar.gz", "has_sig": false, "md5_digest": "cee9b5eda22df61d4e456d4e8396b491", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 120029, "upload_time": "2025-07-28T18:04:14", "upload_time_iso_8601": "2025-07-28T18:04:14.129522Z", "url": "https://files.pythonhosted.org/packages/30/a0/87f1fa7b1f197ab05dbb6d2cb989e2b4f72e18e8502a669d2116ac44fcca/cybrid_api_id_python-0.124.18.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:04:11 GMT", "package": "vacancycalculator", "version": "0.3.1.3", "json": {"info": {"author": "E.Bringa-S.Bergamin-SiMaF", "author_email": "santiagobergamin@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Home-Page", "License", "Requires-Dist", "Summary"], "home_page": "https://github.com/TiagoBe0/VFScript-SiMaF", "keywords": null, "license": "MIT", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "vacancycalculator", "package_url": "https://pypi.org/project/vacancycalculator/", "platform": null, "project_url": "https://pypi.org/project/vacancycalculator/", "project_urls": {"Homepage": "https://github.com/TiagoBe0/VFScript-SiMaF"}, "provides_extra": null, "release_url": "https://pypi.org/project/vacancycalculator/0.3.1.3/", "requires_dist": ["scikit-learn", "pandas", "xgboost", "ovito", "numpy"], "requires_python": null, "summary": "Defect analysis and vacancy calculation for materials science", "version": "0.3.1.3", "yanked": false, "yanked_reason": null}, "last_serial": 30388504, "urls": [{"comment_text": null, "digests": {"blake2b_256": "e2a0e9204fd0b9957be9d88ad79d260dc293a8514c82168368f3833a1d3d4f62", "md5": "fb11ba0d37822eed728760b60b944bde", "sha256": "1f20ac080151d42ae3491484b5c65a8f3835a93eb8c28d13def12d1df1152c4c"}, "downloads": -1, "filename": "vacancycalculator-0.3.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "fb11ba0d37822eed728760b60b944bde", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 59273, "upload_time": "2025-07-28T18:04:11", "upload_time_iso_8601": "2025-07-28T18:04:11.233968Z", "url": "https://files.pythonhosted.org/packages/e2/a0/e9204fd0b9957be9d88ad79d260dc293a8514c82168368f3833a1d3d4f62/vacancycalculator-0.3.1.3-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "eda01ce33b29e219c3a3ef099076aa97c10ea4a715ddfec79a5914f284190c7f", "md5": "a7292c616ffe6d4d66c8ec339f082e8d", "sha256": "8c30553eb505ab0a498c6f01be5df23b3af252634ac53654ea5f6c6a249c030f"}, "downloads": -1, "filename": "vacancycalculator-0.3.1.3.tar.gz", "has_sig": false, "md5_digest": "a7292c616ffe6d4d66c8ec339f082e8d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 36401, "upload_time": "2025-07-28T18:04:12", "upload_time_iso_8601": "2025-07-28T18:04:12.656182Z", "url": "https://files.pythonhosted.org/packages/ed/a0/1ce33b29e219c3a3ef099076aa97c10ea4a715ddfec79a5914f284190c7f/vacancycalculator-0.3.1.3.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:04:01 GMT", "package": "pymitsubishi", "version": "0.1.7", "json": {"info": {"author": "Ashleigh Hopkins", "author_email": "ashleigh@example.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Topic :: Home Automation", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# PyMitsubishi\n\n[![PyPI version](https://badge.fury.io/py/pymitsubishi.svg)](https://badge.fury.io/py/pymitsubishi)\n[![Python Versions](https://img.shields.io/pypi/pyversions/pymitsubishi.svg)](https://pypi.org/project/pymitsubishi/)\n[![Downloads](https://static.pepy.tech/badge/pymitsubishi)](https://pepy.tech/project/pymitsubishi)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA Python library for controlling and monitoring Mitsubishi MAC-577IF-2E air conditioners.\n\n## Features\n\n- **Device Control**: Power, temperature, mode, fan speed, and vane direction control\n- **Status Monitoring**: Real-time device status, temperatures, and error states\n- **Capability Detection**: Dynamic detection of device capabilities using ProfileCode analysis\n- **Group Code Analysis**: Advanced protocol analysis for enhanced device understanding\n- **Encryption Support**: Full support for Mitsubishi's encryption protocol\n\n## Installation\n\n```bash\npip install pymitsubishi\n```\n\n## Quick Start\n\n```python\nfrom pymitsubishi import MitsubishiAPI, MitsubishiController\n\n# Initialize the API and controller\napi = MitsubishiAPI(device_ip=\"192.168.1.100\")\ncontroller = MitsubishiController(api=api)\n\n# Fetch device status\nif controller.fetch_status():\n    summary = controller.get_status_summary()\n    print(f\"Power: {summary['power']}\")\n    print(f\"Temperature: {summary['target_temp']}\u00b0C\")\n    print(f\"Mode: {summary['mode']}\")\n\n# Control the device\ncontroller.set_power(True)\ncontroller.set_temperature(24.0)\ncontroller.set_mode(DriveMode.COOLER)\n\n# Clean up\napi.close()\n```\n\n## Advanced Usage\n\n### Capability Detection\n\n```python\nfrom pymitsubishi import CapabilityDetector\n\ndetector = CapabilityDetector(api=api)\ncapabilities = detector.detect_all_capabilities(debug=True)\n\n# Check specific capabilities\nif capabilities.has_capability(CapabilityType.OUTDOOR_TEMPERATURE_SENSOR):\n    print(\"Device has outdoor temperature sensor\")\n\n# Save capabilities for later use\ndetector.save_capabilities(\"device_capabilities.json\")\n```\n\n### ProfileCode Analysis\n\nThe library automatically analyzes ProfileCode data from device responses to detect capabilities and device characteristics:\n\n```python\n# Fetch status with capability detection\ncontroller.fetch_status(detect_capabilities=True)\n\n# Access detected capabilities\nsummary = controller.get_status_summary()\nif 'capabilities' in summary:\n    for cap_name, cap_info in summary['capabilities'].items():\n        print(f\"{cap_name}: {cap_info['supported']}\")\n```\n\n## API Reference\n\n### MitsubishiAPI\n\nCore communication class handling encryption and HTTP requests.\n\n### MitsubishiController\n\nHigh-level control interface for device operations.\n\n### CapabilityDetector\n\nAdvanced capability detection using ProfileCode and group code analysis.\n\n### Data Classes\n\n- `PowerOnOff`: Power state enumeration\n- `DriveMode`: Operating mode enumeration  \n- `WindSpeed`: Fan speed enumeration\n- `VerticalWindDirection`, `HorizontalWindDirection`: Vane direction enumerations\n\n## Requirements\n\n- Python 3.12+\n- requests\n- pycryptodome\n\n## License\n\nMIT License - see LICENSE file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "License-File", "Provides-Extra", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/pymitsubishi/pymitsubishi", "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "pymitsubishi", "package_url": "https://pypi.org/project/pymitsubishi/", "platform": null, "project_url": "https://pypi.org/project/pymitsubishi/", "project_urls": {"Homepage": "https://github.com/pymitsubishi/pymitsubishi"}, "provides_extra": ["dev"], "release_url": "https://pypi.org/project/pymitsubishi/0.1.7/", "requires_dist": ["requests", "pycryptodome", "pytest>=6.0; extra == \"dev\"", "flake8; extra == \"dev\"", "mypy; extra == \"dev\"", "build; extra == \"dev\"", "twine; extra == \"dev\""], "requires_python": ">=3.12", "summary": "Control and monitor Mitsubishi Air Conditioners", "version": "0.1.7", "yanked": false, "yanked_reason": null}, "last_serial": 30388343, "urls": [{"comment_text": null, "digests": {"blake2b_256": "8b0c76c1aaa9c694af468d0b6111e32886a7196633f89c4c5547783df0ff8fb6", "md5": "c9d68ecc79092135a6a0843330e5e879", "sha256": "299010c8a425e3df1dedeef6750d9ed9b99a830428e1b8579f81e9aa396d8b86"}, "downloads": -1, "filename": "pymitsubishi-0.1.7-py3-none-any.whl", "has_sig": false, "md5_digest": "c9d68ecc79092135a6a0843330e5e879", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.12", "size": 34858, "upload_time": "2025-07-28T18:04:01", "upload_time_iso_8601": "2025-07-28T18:04:01.481860Z", "url": "https://files.pythonhosted.org/packages/8b/0c/76c1aaa9c694af468d0b6111e32886a7196633f89c4c5547783df0ff8fb6/pymitsubishi-0.1.7-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "91d28e382cd9c57156872b9dc1cdf8203dc126e449d40e9de4cc269cadd3e1f3", "md5": "d5d3c699fd3f93531457be9af9539f9b", "sha256": "ed85af77e62d9839f4d286b703b37c5bf996ca3f78753b1c2ece75f8746f6390"}, "downloads": -1, "filename": "pymitsubishi-0.1.7.tar.gz", "has_sig": false, "md5_digest": "d5d3c699fd3f93531457be9af9539f9b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.12", "size": 31367, "upload_time": "2025-07-28T18:04:02", "upload_time_iso_8601": "2025-07-28T18:04:02.594468Z", "url": "https://files.pythonhosted.org/packages/91/d2/8e382cd9c57156872b9dc1cdf8203dc126e449d40e9de4cc269cadd3e1f3/pymitsubishi-0.1.7.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:03:52 GMT", "package": "HRM-pytorch", "version": "0.0.8", "json": {"info": {"author": null, "author_email": "Phil Wang <lucidrains@gmail.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.9", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "<img src=\"./fig4.png\" width=\"250px\"></img>\n\n## Hierarchical Reasoning Model (wip)\n\n\n## Citations\n\n```bibtex\n@misc{wang2025hierarchicalreasoningmodel,\n    title   = {Hierarchical Reasoning Model},\n    author  = {Guan Wang and Jin Li and Yuhao Sun and Xing Chen and Changling Liu and Yue Wu and Meng Lu and Sen Song and Yasin Abbasi Yadkori},\n    year    = {2025},\n    eprint  = {2506.21734},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.AI},\n    url     = {https://arxiv.org/abs/2506.21734},\n}\n```\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "adaptive computation time, artificial intelligence, deep learning, fast slow thinking", "license": "MIT License\n        \n        Copyright (c) 2025 Phil Wang\n        \n        Permission is hereby granted, free of charge, to any person obtaining a copy\n        of this software and associated documentation files (the \"Software\"), to deal\n        in the Software without restriction, including without limitation the rights\n        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n        copies of the Software, and to permit persons to whom the Software is\n        furnished to do so, subject to the following conditions:\n        \n        The above copyright notice and this permission notice shall be included in all\n        copies or substantial portions of the Software.\n        \n        THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n        SOFTWARE.", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "HRM-pytorch", "package_url": "https://pypi.org/project/HRM-pytorch/", "platform": null, "project_url": "https://pypi.org/project/HRM-pytorch/", "project_urls": {"Homepage": "https://pypi.org/project/HRM-pytorch/", "Repository": "https://github.com/lucidrains/hrm"}, "provides_extra": ["examples", "test"], "release_url": "https://pypi.org/project/HRM-pytorch/0.0.8/", "requires_dist": ["adam-atan2-pytorch", "einops>=0.8.0", "torch>=2.0", "x-transformers>=2.5.0", "pytest; extra == \"test\""], "requires_python": ">=3.9", "summary": "The proposal from a Singaporean AGI company", "version": "0.0.8", "yanked": false, "yanked_reason": null}, "last_serial": 30388340, "urls": [{"comment_text": null, "digests": {"blake2b_256": "b9e19946b474118b8e83064189ea7947b28f8df5c8ead0a27c584fb03f82d4d8", "md5": "be7c4d41aaec29d9bf8703d6c7dbcb6b", "sha256": "8f6c57c14f977bce18563d862051d8ef50a370aa778aaf12976d6fe3788a3ad0"}, "downloads": -1, "filename": "hrm_pytorch-0.0.8-py3-none-any.whl", "has_sig": false, "md5_digest": "be7c4d41aaec29d9bf8703d6c7dbcb6b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 5954, "upload_time": "2025-07-28T18:03:52", "upload_time_iso_8601": "2025-07-28T18:03:52.854531Z", "url": "https://files.pythonhosted.org/packages/b9/e1/9946b474118b8e83064189ea7947b28f8df5c8ead0a27c584fb03f82d4d8/hrm_pytorch-0.0.8-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "e15011014d7a8f6dc5176d46bfcbcc96cb83a65e4a166d27615a9c6bdd8d2d0c", "md5": "8355e7775d59e2e4c70b790f80ca253e", "sha256": "29bd737da4f11058c452105f8906c4225536888444f1cfeb00bb088235094c65"}, "downloads": -1, "filename": "hrm_pytorch-0.0.8.tar.gz", "has_sig": false, "md5_digest": "8355e7775d59e2e4c70b790f80ca253e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 192348, "upload_time": "2025-07-28T18:03:54", "upload_time_iso_8601": "2025-07-28T18:03:54.032873Z", "url": "https://files.pythonhosted.org/packages/e1/50/11014d7a8f6dc5176d46bfcbcc96cb83a65e4a166d27615a9c6bdd8d2d0c/hrm_pytorch-0.0.8.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:03:40 GMT", "package": "type_infer", "version": "0.0.23", "json": {"info": {"author": "MindsDB Inc.", "author_email": "hello@mindsdb.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12"], "description": "# MindsDB Type Infer\n\n<h1 align=\"center\">\n\t<img width=\"300\" src=\"https://github.com/mindsdb/mindsdb_native/blob/stable/assets/MindsDBColorPurp@3x.png?raw=true\" alt=\"MindsDB\">\n\t<br>\n\n</h1>\n<div align=\"center\">\n\t<a href=\"https://github.com/mindsdb/type_infer/actions/workflows/python-package.yml\"><img src=\"https://github.com/mindsdb/type_infer/actions/workflows/python-package.yml/badge.svg?branch=stable\" alt=\"Type Infer workflow\"></a>\n  <a href=\"https://www.python.org/downloads/\" target=\"_blank\"><img src=\"https://img.shields.io/badge/python-3.8.x|%203.9.x-brightgreen.svg\" alt=\"Python supported\"></a>\n  <a href=\"https://badge.fury.io/py/type-infer\"><img src=\"https://badge.fury.io/py/type-infer.svg\" alt=\"PyPI version\" height=\"18\"></a>\n<img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/dm/type-infer\">  \n    <a href=\"https://join.slack.com/t/mindsdbcommunity/shared_invite/zt-o8mrmx3l-5ai~5H66s6wlxFfBMVI6wQ\" target=\"_blank\"><img src=\"https://img.shields.io/badge/slack-@mindsdbcommunity-brightgreen.svg?logo=slack \" alt=\"MindsDB Community\"></a>\n\t</br>\n\t\n  <h3 align=\"center\">\n    <a href=\"https://www.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo\">Website</a>\n    <span> | </span>\n    <a href=\"https://mindsdb.github.io/type_infer/\">Docs</a>\n    <span> | </span>\n    <a href=\"https://join.slack.com/t/mindsdbcommunity/shared_invite/zt-o8mrmx3l-5ai~5H66s6wlxFfBMVI6wQ\">Community Slack</a>\n    <span> | </span>\n    <a href=\"https://github.com/mindsdb/mindsdb/projects\">Contribute</a>\n    <span> | </span>\n    <a href=\"https://mindsdb.com/hacktoberfest\">Hacktoberfest</a>\n  </h3>\n  \n</div>\n\nAutomated type inference for Machine Learning pipelines.\n\n\nIn the context of tabular data, `type_infer` aims for optimal interpretation of each column\u2019s data type for ML use cases. For example, strings with date or time format would be classified as timestamps, or integers as categorical if there is a sufficiently small set of unique values in the column.\n\n# Installation\n\nInstall the package easily using pip:\n\n```\npip install type_infer\n```\n> Note: We recommend using a Python virtual environment.\n\n## Development Environment Setup\n\nTo set up a development environment:\n\n1. Clone the repository:\n```\ngit clone https://github.com/mindsdb/type_infer.git\n```\n\n2. Navigate to the cloned directory and install in editable mode:\n```\ncd type_infer\npip install --editable .\n```\n\n### Running Tests\n\nTp run unit tests execute:\n```\npython -m unittest discover tests\n```\n\n## Contributing\n\nWe welcome and appreciate contributions from the community! Here's how you can help:\n\n* Report bugs\n* Improve documentation\n* Solve open issues\n* Propose or discuss new features\n* Test with your own datasets and provide feedback\n\n# Documentation\n<a href=\"https://mindsdb.github.io/type_infer\">Documentation link</a>\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": "GPL-3.0", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "type_infer", "package_url": "https://pypi.org/project/type_infer/", "platform": null, "project_url": "https://pypi.org/project/type_infer/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/type_infer/0.0.23/", "requires_dist": ["python-dateutil<3.0,>=2.1", "scipy<2,>=1", "numpy<2.0,>=1.15", "pandas<3,>=2", "dataclasses-json<0.7.0,>=0.6.3", "colorlog<7.0.0,>=6.5.0", "psutil<8.0,>=7.0", "toml<0.11.0,>=0.10.2", "py3langid<0.3,>=0.2.2", "nltk<4.0,>=3.9"], "requires_python": "<3.13,>=3.10", "summary": "Automated type inference for Machine Learning pipelines.", "version": "0.0.23", "yanked": false, "yanked_reason": null}, "last_serial": 30388337, "urls": [{"comment_text": "", "digests": {"blake2b_256": "1a6dac71adce42fb338e6cca044ac7df2bc5e4f4b26f0bedd85bc2b6e9d1eacb", "md5": "95827fe5aa256f6756d8f3624bd2d47c", "sha256": "ab6514faf653b9633a701174b0b3eb4e91243a885e4155b0c3e09c750771a1c3"}, "downloads": -1, "filename": "type_infer-0.0.23-py3-none-any.whl", "has_sig": false, "md5_digest": "95827fe5aa256f6756d8f3624bd2d47c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<3.13,>=3.10", "size": 27259, "upload_time": "2025-07-28T18:03:40", "upload_time_iso_8601": "2025-07-28T18:03:40.110061Z", "url": "https://files.pythonhosted.org/packages/1a/6d/ac71adce42fb338e6cca044ac7df2bc5e4f4b26f0bedd85bc2b6e9d1eacb/type_infer-0.0.23-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "72a565acf34b5994f1f5ed732f00d61c1646f4869e328959f711b6cb5e527833", "md5": "925ace20f437394dc4433b933146d557", "sha256": "46048782ab8a3e843c50b0a2b477d34c0359b2688ad04f85b2f3d3b267d049e2"}, "downloads": -1, "filename": "type_infer-0.0.23.tar.gz", "has_sig": false, "md5_digest": "925ace20f437394dc4433b933146d557", "packagetype": "sdist", "python_version": "source", "requires_python": "<3.13,>=3.10", "size": 25066, "upload_time": "2025-07-28T18:03:40", "upload_time_iso_8601": "2025-07-28T18:03:40.888370Z", "url": "https://files.pythonhosted.org/packages/72/a5/65acf34b5994f1f5ed732f00d61c1646f4869e328959f711b6cb5e527833/type_infer-0.0.23.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:03:27 GMT", "package": "bookspointer", "version": "0.2.1", "json": {"info": {"author": null, "author_email": "Samir Das <samircd4@gmail.com>", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# bookspointer\r\n\r\nA package for scraping and serving book data.\r\n\r\n## Installation\r\n\r\n```bash\r\npip install bookspointer\r\n```\r\n\r\n## Usage\r\n\r\nImport and use the modules in your Python code:\r\n\r\n```python\r\nfrom bookspointer.scraper import BookScraper\r\nfrom bookspointer.server import BookAPI, AuthorAPI, TokenAPI\r\nfrom bookspointer.sheet import AuthorSheetManager\r\nfrom bookspointer.api import BookspointerAPI\r\n\r\nfrom rich import print\r\nimport random\r\n\r\n# Initialize the main classes\r\nscraper = BookScraper([5, 17])  # Categories to scrape by single page\r\nbook_api = BookAPI()\r\nauthor_api = AuthorAPI()\r\n\r\n\r\ndef update_authors_from_bookspointer():\r\n    \"\"\"\r\n    Fetches authors from a Google Sheet using AuthorSheetManager and updates them on the server.\r\n    \"\"\"\r\n    # Fetch and update authors from Google Sheet to your server\r\n    author_sheet = AuthorSheetManager().run()\r\n\r\n\r\ndef update_books_from_authors():\r\n    \"\"\"\r\n    Fetches unscraped authors from the server, scrapes their books, adds the books to the server, and marks authors as scraped.\r\n    \"\"\"\r\n    # Fetch unscraped authors and update their books\r\n    authors = author_api.get_unscraped_authors()\r\n    for author in authors:\r\n        author_url = author.get('author_link')\r\n        author_name = author.get('author_name', 'Unknown')\r\n        author_id = author.get('author_id')\r\n        if not author_url:\r\n            continue\r\n        books = scraper.get_book_list(author_url)\r\n        for book in books:\r\n            book_list = scraper.get_book_details(book, author_id)\r\n            for book in book_list:\r\n                add_book = book_api.create(book)\r\n                author_api.update(author.get('id'), {\"is_scraped\": \"yes\"})\r\n                print(\r\n                    f\"Added book: {book['title']} by {author_name} with ID: {add_book}\")\r\n        print(f\"Finished scraping books for author: {author_name}\")\r\n\r\n\r\ndef post_books_on_bookspointer():\r\n    \"\"\"\r\n    Posts all books that have not yet been posted to Bookspointer using random tokens for authentication.\r\n    \"\"\"\r\n    books = book_api.get_all_books(is_posted=False)\r\n    tokens = TokenAPI().get_all_tokens()\r\n    for book in books:\r\n        token = random.choice(tokens)\r\n        bookspointer_api = BookspointerAPI(token)\r\n        bookspointer_api.post_book(book)\r\n\r\n\r\ndef main():\r\n    \"\"\"\r\n    Orchestrates the process of updating authors, updating books, and posting books by calling the respective functions in sequence.\r\n    \"\"\"\r\n    print(\"Starting to update authors from Bookspointer...\")\r\n    update_authors_from_bookspointer()\r\n    print(\"Authors updated successfully.\")\r\n    print(\"Starting to update books from authors...\")\r\n    update_books_from_authors()\r\n    print(\"Books updated successfully.\")\r\n    print(\"Starting to post books on Bookspointer...\")\r\n    post_books_on_bookspointer()\r\n    print(\"All books posted successfully.\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n## Features\r\n\r\n- Scrape book data\r\n- Serve book data via API\r\n- Sync authors and books with Google Sheets\r\n\r\n## Project Links\r\n\r\n- [Homepage](https://github.com/samircd4)\r\n- [Repository](https://github.com/samircd4/bookspointer)\r\n\r\n## License\r\n\r\nMIT\r\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": "MIT", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "bookspointer", "package_url": "https://pypi.org/project/bookspointer/", "platform": null, "project_url": "https://pypi.org/project/bookspointer/", "project_urls": {"Homepage": "https://github.com/samircd4", "Repository": "https://github.com/samircd4/bookspointer"}, "provides_extra": null, "release_url": "https://pypi.org/project/bookspointer/0.2.1/", "requires_dist": ["requests", "selectolax", "rich", "python-dotenv", "gspread", "google-auth", "google-auth-oauthlib", "pandas"], "requires_python": ">=3.7", "summary": "A package for scraping and serving book data in to bookspointer.", "version": "0.2.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388334, "urls": [{"comment_text": null, "digests": {"blake2b_256": "842d2481933da27f0eacc82e55ce62a997b393f0ff33b985185f73df080481e5", "md5": "affd94eae9921b0654eb30c0c0badcd0", "sha256": "272b6749336e1331cc5d2ab2f3341a3cde9612052df4535c943eac3a64000390"}, "downloads": -1, "filename": "bookspointer-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "affd94eae9921b0654eb30c0c0badcd0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 15867, "upload_time": "2025-07-28T18:03:27", "upload_time_iso_8601": "2025-07-28T18:03:27.941576Z", "url": "https://files.pythonhosted.org/packages/84/2d/2481933da27f0eacc82e55ce62a997b393f0ff33b985185f73df080481e5/bookspointer-0.2.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "ea50d0d64ec6ca2dc2fe25b22a35512f4c433bf017bbf1f9b37cb6a69055bf5b", "md5": "7242537c71b6fd016a90fea3991f161d", "sha256": "e91073b9da25bc8c73613733bfb734665f18bc2f13f83119f123e86e2bf9f1c9"}, "downloads": -1, "filename": "bookspointer-0.2.1.tar.gz", "has_sig": false, "md5_digest": "7242537c71b6fd016a90fea3991f161d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 15653, "upload_time": "2025-07-28T18:03:29", "upload_time_iso_8601": "2025-07-28T18:03:29.058700Z", "url": "https://files.pythonhosted.org/packages/ea/50/d0d64ec6ca2dc2fe25b22a35512f4c433bf017bbf1f9b37cb6a69055bf5b/bookspointer-0.2.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:03:11 GMT", "package": "cuda-kernels", "version": "0.1.1", "json": {"info": {"author": "Sukhman Virk, Shiv Mehta", "author_email": "sukhmanvirk26@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Mathematics"], "description": "# CUDA Kernels\r\n\r\nA Python package providing CUDA-accelerated functions for autocorrelation and sum reduction operations, with automatic CPU fallback when CUDA is not available.\r\n\r\n## Installation\r\n\r\n### From PyPI (Recommended)\r\n```bash\r\npip install cuda-kernels\r\n```\r\n\r\n### From GitHub\r\n```bash\r\npip install git+https://github.com/AstuteFern/cuda-toolkit.git\r\n```\r\n\r\n### From Source\r\n```bash\r\ngit clone https://github.com/AstuteFern/cuda-toolkit.git\r\ncd cuda-toolkit\r\npip install .\r\n```\r\n\r\n## Requirements\r\n\r\n- **Python 3.6+**\r\n- **NumPy**\r\n\r\n### Optional (for CUDA acceleration)\r\n- NVIDIA GPU with CUDA support\r\n- CUDA Toolkit (version 11.0+)\r\n\r\n**Note:** The package works on any system. If CUDA is not available, it automatically uses optimized CPU implementations.\r\n\r\n## Quick Start\r\n\r\n```python\r\nimport numpy as np\r\nfrom cuda_kernels import autocorrelation, reduction_sum\r\n\r\n# Create test data\r\ndata = np.random.randn(1000).astype(np.float32)\r\n\r\n# Compute autocorrelation (automatically uses CUDA if available)\r\nacf = autocorrelation(data, max_lag=50)\r\nprint(f\"Autocorrelation shape: {acf.shape}\")\r\n\r\n# Compute sum reduction\r\ntotal = reduction_sum(data)\r\nprint(f\"Sum: {total}\")\r\n```\r\n\r\n## API Reference\r\n\r\n### `autocorrelation(data, max_lag=None, force_cpu=False)`\r\n\r\nCompute autocorrelation of a time series.\r\n\r\n**Parameters:**\r\n- `data` (numpy.ndarray): Input 1D array (converted to float32)\r\n- `max_lag` (int, optional): Maximum lag to compute. Default: len(data)-1\r\n- `force_cpu` (bool): Force CPU implementation. Default: False\r\n\r\n**Returns:**\r\n- `numpy.ndarray`: Autocorrelation values for lags [0, max_lag)\r\n\r\n### `reduction_sum(data, force_cpu=False)`\r\n\r\nCompute sum of array elements.\r\n\r\n**Parameters:**\r\n- `data` (numpy.ndarray): Input 1D array (converted to float32)\r\n- `force_cpu` (bool): Force CPU implementation. Default: False\r\n\r\n**Returns:**\r\n- `float`: Sum of all elements\r\n\r\n## Examples\r\n\r\n### Basic Usage\r\n```python\r\nimport numpy as np\r\nfrom cuda_kernels import autocorrelation, reduction_sum\r\n\r\n# Example 1: Autocorrelation\r\nsignal = np.sin(np.linspace(0, 4*np.pi, 1000)).astype(np.float32)\r\nacf = autocorrelation(signal, max_lag=100)\r\n\r\n# Example 2: Sum reduction\r\ndata = np.array([1, 2, 3, 4, 5], dtype=np.float32)\r\ntotal = reduction_sum(data)  # Returns 15.0\r\n```\r\n\r\n### Checking CUDA Status\r\n```python\r\nimport sys\r\nautocorr_module = sys.modules['cuda_kernels.autocorrelation']\r\nreduction_module = sys.modules['cuda_kernels.reduction']\r\n\r\nprint(f\"CUDA available: {autocorr_module._cuda_available}\")\r\n```\r\n\r\n### Force CPU Mode\r\n```python\r\n# Useful for testing or when you want consistent behavior\r\ncpu_result = reduction_sum(data, force_cpu=True)\r\n```\r\n\r\n## Performance\r\n\r\n- **With CUDA**: Significant speedup for large arrays (10K+ elements)\r\n- **CPU Fallback**: Optimized NumPy implementations, still efficient for most use cases\r\n- **Automatic Detection**: No configuration needed, works out of the box\r\n\r\n## License\r\n\r\nMIT License - see LICENSE file for details.\r\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "License-File", "Provides-Extra", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/AstuteFern/cuda-toolkit", "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "cuda-kernels", "package_url": "https://pypi.org/project/cuda-kernels/", "platform": null, "project_url": "https://pypi.org/project/cuda-kernels/", "project_urls": {"Homepage": "https://github.com/AstuteFern/cuda-toolkit"}, "provides_extra": ["cuda"], "release_url": "https://pypi.org/project/cuda-kernels/0.1.1/", "requires_dist": ["numpy>=1.16.0", "torch>=1.7.0; extra == \"cuda\""], "requires_python": ">=3.6", "summary": "CUDA accelerated correlation and sum reduction functions", "version": "0.1.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388331, "urls": [{"comment_text": null, "digests": {"blake2b_256": "cda170b9f1065838b0f47a1531ff1c0eb10b979cd1a473d62b680419fe52ba27", "md5": "c14d590be86857dba5e2464a5d893998", "sha256": "d31735fd4994240eaefd627e319b4eb157f234821c2eb431e90043a612e31859"}, "downloads": -1, "filename": "cuda_kernels-0.1.1-cp310-cp310-win_amd64.whl", "has_sig": false, "md5_digest": "c14d590be86857dba5e2464a5d893998", "packagetype": "bdist_wheel", "python_version": "cp310", "requires_python": ">=3.6", "size": 7577, "upload_time": "2025-07-28T18:03:11", "upload_time_iso_8601": "2025-07-28T18:03:11.102027Z", "url": "https://files.pythonhosted.org/packages/cd/a1/70b9f1065838b0f47a1531ff1c0eb10b979cd1a473d62b680419fe52ba27/cuda_kernels-0.1.1-cp310-cp310-win_amd64.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "b7d54d81eeb778c7bf71b8970a79998dbae2cf1c0120db33619632dabe77b87c", "md5": "37b3b7d07bd40fc3a4bc41c75c4cd527", "sha256": "7f699e076d8bd56688b2a6a29ccfba17850e285d2fad0d38d00381a2a403c7fe"}, "downloads": -1, "filename": "cuda_kernels-0.1.1.tar.gz", "has_sig": false, "md5_digest": "37b3b7d07bd40fc3a4bc41c75c4cd527", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 8190, "upload_time": "2025-07-28T18:03:12", "upload_time_iso_8601": "2025-07-28T18:03:12.253877Z", "url": "https://files.pythonhosted.org/packages/b7/d5/4d81eeb778c7bf71b8970a79998dbae2cf1c0120db33619632dabe77b87c/cuda_kernels-0.1.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:02:52 GMT", "package": "HRM-pytorch", "version": "0.0.7", "json": {"info": {"author": null, "author_email": "Phil Wang <lucidrains@gmail.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.9", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "<img src=\"./fig4.png\" width=\"250px\"></img>\n\n## Hierarchical Reasoning Model (wip)\n\n\n## Citations\n\n```bibtex\n@misc{wang2025hierarchicalreasoningmodel,\n    title   = {Hierarchical Reasoning Model},\n    author  = {Guan Wang and Jin Li and Yuhao Sun and Xing Chen and Changling Liu and Yue Wu and Meng Lu and Sen Song and Yasin Abbasi Yadkori},\n    year    = {2025},\n    eprint  = {2506.21734},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.AI},\n    url     = {https://arxiv.org/abs/2506.21734},\n}\n```\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "adaptive computation time, artificial intelligence, deep learning, fast slow thinking", "license": "MIT License\n        \n        Copyright (c) 2025 Phil Wang\n        \n        Permission is hereby granted, free of charge, to any person obtaining a copy\n        of this software and associated documentation files (the \"Software\"), to deal\n        in the Software without restriction, including without limitation the rights\n        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n        copies of the Software, and to permit persons to whom the Software is\n        furnished to do so, subject to the following conditions:\n        \n        The above copyright notice and this permission notice shall be included in all\n        copies or substantial portions of the Software.\n        \n        THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n        SOFTWARE.", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "HRM-pytorch", "package_url": "https://pypi.org/project/HRM-pytorch/", "platform": null, "project_url": "https://pypi.org/project/HRM-pytorch/", "project_urls": {"Homepage": "https://pypi.org/project/HRM-pytorch/", "Repository": "https://github.com/lucidrains/hrm"}, "provides_extra": ["examples", "test"], "release_url": "https://pypi.org/project/HRM-pytorch/0.0.7/", "requires_dist": ["adam-atan2-pytorch", "einops>=0.8.0", "torch>=2.0", "x-transformers>=2.5.0", "pytest; extra == \"test\""], "requires_python": ">=3.9", "summary": "The proposal from a Singaporean AGI company", "version": "0.0.7", "yanked": false, "yanked_reason": null}, "last_serial": 30388340, "urls": [{"comment_text": null, "digests": {"blake2b_256": "ffe7658f7d5f9c17c57bef2b0f9c42401c351779d4fec133415e8041c4cd2733", "md5": "17cc81f05b0d9054d0059a3a0d58ceb0", "sha256": "6a413fd9826bbfe2552d4a0a592ccfc9a1fe16030ba44539bea1df5f28fffcb4"}, "downloads": -1, "filename": "hrm_pytorch-0.0.7-py3-none-any.whl", "has_sig": false, "md5_digest": "17cc81f05b0d9054d0059a3a0d58ceb0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 5948, "upload_time": "2025-07-28T18:02:52", "upload_time_iso_8601": "2025-07-28T18:02:52.267502Z", "url": "https://files.pythonhosted.org/packages/ff/e7/658f7d5f9c17c57bef2b0f9c42401c351779d4fec133415e8041c4cd2733/hrm_pytorch-0.0.7-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "df7055dd2e7a358e9d4aae8deb39b523230272806278c98faabdda6e700eee1a", "md5": "c595e7183f973ba038c848e5e8bc3834", "sha256": "8774303cb606f567f43f23c51d1bb09e369f06ca348c5d4e133b772f8e72db9f"}, "downloads": -1, "filename": "hrm_pytorch-0.0.7.tar.gz", "has_sig": false, "md5_digest": "c595e7183f973ba038c848e5e8bc3834", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 192337, "upload_time": "2025-07-28T18:02:53", "upload_time_iso_8601": "2025-07-28T18:02:53.384256Z", "url": "https://files.pythonhosted.org/packages/df/70/55dd2e7a358e9d4aae8deb39b523230272806278c98faabdda6e700eee1a/hrm_pytorch-0.0.7.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:02:44 GMT", "package": "ob-metaflow-extensions", "version": "1.2.9rc1", "json": {"info": {"author": "Outerbounds, Inc.", "author_email": null, "bugtrack_url": null, "classifiers": [], "description": "# Outerbounds platform package\n\nThis package installs client side packages for Outerbounds platform. See Outerbounds platform documentation and [Metaflow documentation](https://metaflow.org/) for more info on how to use it.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": "Commercial", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "ob-metaflow-extensions", "package_url": "https://pypi.org/project/ob-metaflow-extensions/", "platform": null, "project_url": "https://pypi.org/project/ob-metaflow-extensions/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/ob-metaflow-extensions/1.2.9rc1/", "requires_dist": ["boto3", "kubernetes", "ob-metaflow==2.16.5.1"], "requires_python": null, "summary": "Outerbounds Platform Extensions for Metaflow", "version": "1.2.9rc1", "yanked": false, "yanked_reason": null}, "last_serial": 30388325, "urls": [{"comment_text": null, "digests": {"blake2b_256": "91615d336e16190db83ef3e4e594c1f52b978cd0b9c21d706bcf27fd62b8ee37", "md5": "7fe037c2c4788e4b9121992d804b7eeb", "sha256": "7ae853829e397530a5a23f97eda6124fcc4cdf6f9ab09d639d0fb936a3b2e190"}, "downloads": -1, "filename": "ob_metaflow_extensions-1.2.9rc1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "7fe037c2c4788e4b9121992d804b7eeb", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 240725, "upload_time": "2025-07-28T18:02:44", "upload_time_iso_8601": "2025-07-28T18:02:44.607721Z", "url": "https://files.pythonhosted.org/packages/91/61/5d336e16190db83ef3e4e594c1f52b978cd0b9c21d706bcf27fd62b8ee37/ob_metaflow_extensions-1.2.9rc1-py2.py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "fd3ec9baf1ccbeaf18fb807f2e8c358fea1354929d985abca9db82dc4fcbeb9c", "md5": "05464817fb44f436951eeb948689d56d", "sha256": "11d734c9a205480f92b5a6d618befe49d3c970b315440ebbe37599f473a129e5"}, "downloads": -1, "filename": "ob_metaflow_extensions-1.2.9rc1.tar.gz", "has_sig": false, "md5_digest": "05464817fb44f436951eeb948689d56d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 192381, "upload_time": "2025-07-28T18:02:46", "upload_time_iso_8601": "2025-07-28T18:02:46.652043Z", "url": "https://files.pythonhosted.org/packages/fd/3e/c9baf1ccbeaf18fb807f2e8c358fea1354929d985abca9db82dc4fcbeb9c/ob_metaflow_extensions-1.2.9rc1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:01:47 GMT", "package": "django-wilayah-indonesia", "version": "0.1.0", "json": {"info": {"author": "irfanpule", "author_email": "irfan.pule2@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Web Environment", "Framework :: Django", "Framework :: Django :: 4.2", "Framework :: Django :: 5.0", "Framework :: Django :: 5.2", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3 :: Only", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Internet :: WWW/HTTP"], "description": "# Wilayah Indonesia\n\nAplikasi ini menyediakan data wilayah administratif Indonesia (provinsi, kabupaten/kota, kecamatan, dan desa) yang dapat digunakan untuk kebutuhan input pada Admin site, form custom, REST API.\n\n![admin-site](https://raw.githubusercontent.com/irfanpule/wilayah_indonesia/refs/heads/master/screenshoots/animation-chaining.gif)\n\n## Fitur\n\n- Menyediakan data wilayah Indonesia secara lengkap\n- Mendukung proses seeding ke database\n- Tersedia form chained untuk diimplementasikan pada form Admin site atau form custome\n- Tersedia endpoint REST API\n\n## Instalasi\n\n1. **Clone repository**\n    - Unduh zip dan extrak dalam direktori proyek\n    - Atau masuk dalam direktori proyek kamu lalu clone repositori ini\n    ```bash\n    git clone https://github.com/irfanpule/data-wilayah-indonesia.\n    ```\n\n2. **Install dependencies**\n    ```bash\n    pip install django-select2\n    ```\n\n3. **Migrate**\n    ```bash\n    ./manage.py migrate\n    ```\n\n4. **Register URL**\n    Registrasikan url django-select2 dan wilayah_indonesia\n    ```python\n    path('wilayah-indonesia/', include('wilayah_indonesia.urls')),\n    path('select2/', include('django_select2.urls'))\n    ```\n\n## Seeding Data Wilayah\n\nJalankan perintah berikut untuk melakukan seeding data wilayah ke database:\n\n```bash\n./manage.py region_seeding\n```\nAtau jika hanya ingin menjalankan seeder wilayah:\n```bash\n./manage.py region_seeding --provinsi\n```\n```bash\n./manage.py region_seeding --kabupaten\n```\n```bash\n./manage.py region_seeding --kecamatan\n```\n```bash\n./manage.py region_seeding --desa\n```\n\nUntuk menghapus data gunakan command ini\n\n```bash\n./manage.py region_seeding --delete\n```\n\n## Model\nContoh kode:\n```python\nfrom wilayah_indonesia.models import WilayahDisplayMixin\n\nclass Profile(WilayahDisplayMixin, models.Model):\n    user = models.OneToOneField(settings.AUTH_USER_MODEL, on_delete=models.CASCADE)\n    nik = models.CharField(max_length=16, unique=True)\n    # field lainnya....\n    provinsi = models.ForeignKey(\"wilayah_indonesia.Provinsi\", on_delete=models.SET_NULL, null=True, blank=True)\n    kabupaten = models.ForeignKey(\"wilayah_indonesia.Kabupaten\", on_delete=models.SET_NULL, null=True, blank=True)\n    kecamatan = models.ForeignKey(\"wilayah_indonesia.Kecamatan\", on_delete=models.SET_NULL, null=True, blank=True)\n    desa = models.ForeignKey(\"wilayah_indonesia.Desa\", on_delete=models.SET_NULL, null=True, blank=True)\n    # field lainnya ....\n\n    def __str__(self):\n        return self.nik\n```\nDisediakan class mixin untuk mempermudah akses nama dari masing-masing wilayah. Cukup gunakan `WilayahDisplayMixin` kamu dapat dengan mudah akses nama wilayah seperti ini: `get_provinsi_display()`, `get_kabupaten_display()`, `get_kecamatan_display`, `get_desa_display()`.\n\n*NB: Disarakan nama field tetap menggunakan `provinsi, kabupaten, kecamatan, desa` agar fungsi mixin bekerja*\n\n\n## Form\nSudah tersedia tersedia class mixin untuk form select2 chained dapat dilihat pada `wilayah_indonesia/forms.py`\n- Gunakan fungsi chiined yang sudah disediakan untuk membuat select chained pada form. Contoh\n```python    \n# Form -------\n# Implementasi fungsi chained pada form\nfrom wilayah_indonesia.forms import WilayahChainedFormMixin\n\nclass ProfileAdminForm(WilayahChainedFormMixin, forms.ModelForm):\n    class Meta:\n        model = Profile\n        fields = '__all__'\n\n\n# Admin site -------\n# Implementasi form pada admin site\n@admin.register(Profile)\nclass ProfileAdmin(admin.ModelAdmin):\n    list_display = ('user', 'nik', 'no_ponsel', 'jenis_kelamin')\n    # atribute lainnya ....\n    form = ProfileAdminForm  # tambahkan form disini\n```\n\n## Endpoint\n- Untuk mendapatkan data provinsi \n```\n{{base_url}}/wilayah-indonesia/provinsi/\n```\n- Untuk mendapatkan data kabupaten harus menambahkan id provinsi pada url\n```\n{{base_url}}/wilayah-indonesia/kabupaten/18/\n```\n- Untuk mendapatkan data kecamatan harus menambahkan id kabupaten pada url\n```\n{{base_url}}/wilayah-indonesia/kecamatan/1809/\n```\n- Untuk mendapatkan data desa harus menambahkan id kecamatan pada url\n```\n{{base_url}}/wilayah-indonesia/desa/1809050/\n```\n- Untuk melakukan filter atau search data cukup menambahkan query param pada url `{{uri}}/?search=way`. Berlaku untuk semua endpoint\n\n\n## Lisensi\n\nMIT License.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "License", "License-File", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://github.com/irfanpule/wilayah_indonesia", "keywords": null, "license": "MIT", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "django-wilayah-indonesia", "package_url": "https://pypi.org/project/django-wilayah-indonesia/", "platform": null, "project_url": "https://pypi.org/project/django-wilayah-indonesia/", "project_urls": {"Homepage": "https://github.com/irfanpule/wilayah_indonesia"}, "provides_extra": null, "release_url": "https://pypi.org/project/django-wilayah-indonesia/0.1.0/", "requires_dist": ["django>=4.2", "django-select2>=8.4.0"], "requires_python": ">=3.6", "summary": "A simple Django app to provide Indonesia region", "version": "0.1.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388319, "urls": [{"comment_text": null, "digests": {"blake2b_256": "13e193c541ceba5d7239fc46fe284b970d5e2fc1215c1018a834e0d3e9b4d210", "md5": "1232444dcde103c46af54e181eb30805", "sha256": "13b42f867e8fdfccb9959b78b400e26a95adc82bc6ef2d40391f0cfb413577ea"}, "downloads": -1, "filename": "django_wilayah_indonesia-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "1232444dcde103c46af54e181eb30805", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 10243, "upload_time": "2025-07-28T18:01:47", "upload_time_iso_8601": "2025-07-28T18:01:47.662178Z", "url": "https://files.pythonhosted.org/packages/13/e1/93c541ceba5d7239fc46fe284b970d5e2fc1215c1018a834e0d3e9b4d210/django_wilayah_indonesia-0.1.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "bfb30e469828eaf97e1c2425a9e2f30540d5d8b10e9b9bbd3cf155bd03c7de37", "md5": "089195c3184427dc435d17f3fbf0d4ed", "sha256": "9e2de09cb22e6f4439b7b35adbb6d637e02381c871f14552f6e8855d8e10c6fe"}, "downloads": -1, "filename": "django_wilayah_indonesia-0.1.0.tar.gz", "has_sig": false, "md5_digest": "089195c3184427dc435d17f3fbf0d4ed", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7478, "upload_time": "2025-07-28T18:01:49", "upload_time_iso_8601": "2025-07-28T18:01:49.501540Z", "url": "https://files.pythonhosted.org/packages/bf/b3/0e469828eaf97e1c2425a9e2f30540d5d8b10e9b9bbd3cf155bd03c7de37/django_wilayah_indonesia-0.1.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:01:43 GMT", "package": "bigsur", "version": "0.0.5", "json": {"info": {"author": "Emmanuel Dollinger", "author_email": "Emmanuel Dollinger <edolling@uci.edu>", "bugtrack_url": null, "classifiers": ["Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# BigSur\nBigSur is a package for principled, robust scRNAseq normalization. Currently we can perform feature selection, see [BigSurR](https://github.com/landerlabcode/BigSurR) for correlations.\n\n# What is BigSur?\nBasic Informatics and Gene Statistics from Unnormalized Reads (BigSur) is a principled pipeline allowing for feature selection, correlation and clustering in scRNAseq.\n* The feature selection derivations are detailed in [the BioRxiv preprint Dollinger et al. 2023](https://www.biorxiv.org/content/10.1101/2024.10.11.617709v1).\n* The correlation are detailed in [Silkwood et al. 2023](https://doi.org/10.1186/s12859-024-05926-z).\n\n\n# Installation\nThe only way to install BigSur currently is to clone the GitHub repo. We've included a environment file for [conda environment installation](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#building-identical-conda-environments); the only package we require that isn't installed with scanpy is [mpmath](https://github.com/mpmath/mpmath) and [numexpr](https://github.com/pydata/numexpr). For example:\n\nIn terminal:\n\n    cd bigsur_dir #directory to clone to\n\n    git clone https://github.com/landerlabcode/BigSur.git\n\n    conda create -f environment.yml -n bigsur\n\n## A note about the virtual environment\nThis environment contains all packages that are required to reproduce any result of the paper. If you want a lightweight conda enviroment (or alternatively, if the environment file is causing issues), you can create a sufficient conda environment as follows:\n\nIn terminal:\n\n    conda create -n bigsur -c conda-forge scanpy mpmath numexpr ipykernel python-igraph leidenalg\n\n# Usage\nUsage for feature selection is detailed in the [example notebook](https://github.com/landerlabcode/BigSur/blob/main/feature_selection_example_usage.ipynb). \n\nTL;DR:\n\n    import sys\n    \n    sys.path.append(bigsur_dir) # directory where git repo was cloned\n    \n    from BigSur.feature_selection import mcfano_feature_selection as mcfano\n\nReplace <code>sc.pp.highly_variable_genes(adata)</code> in your pipeline with <code>mcfano(adata, layer='counts')</code>, where the UMI counts are in <code>adata.layers['counts']</code>.\n\nAnd that's it! You can read more about how to use BigSur for feature selection, and in particular how to optimize cutoffs for a given dataset, in the [example notebook](https://github.com/landerlabcode/BigSur/blob/main/feature_selection_example_usage.ipynb). \n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Home-Page", "License-File", "Requires-Python"], "home_page": "https://github.com/landerlabcode/BigSur", "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "bigsur", "package_url": "https://pypi.org/project/bigsur/", "platform": null, "project_url": "https://pypi.org/project/bigsur/", "project_urls": {"Homepage": "https://github.com/landerlabcode/BigSur/", "Issues": "https://github.com/landerlabcode/BigSur/issues"}, "provides_extra": null, "release_url": "https://pypi.org/project/bigsur/0.0.5/", "requires_dist": ["scanpy", "mpmath", "numexpr", "ipykernel", "python-igraph", "leidenalg"], "requires_python": "<4,>=3.9", "summary": "Basic Informatics and Gene Statistics from Unnormalized Reads, a feature selection tool for scRNAseq", "version": "0.0.5", "yanked": false, "yanked_reason": null}, "last_serial": 30388314, "urls": [{"comment_text": "", "digests": {"blake2b_256": "e5e6c1ce674441968e19b8e7d3fae88d430b3b575629d3b7a13ff6d60f2810a0", "md5": "e237f813b7bc1622534b8e986c76a15d", "sha256": "8eb1250e216f3bc21ab7acd4edb972243ab0d7b889197215458ee02552f28206"}, "downloads": -1, "filename": "bigsur-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "e237f813b7bc1622534b8e986c76a15d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<4,>=3.9", "size": 11460, "upload_time": "2025-07-28T18:01:43", "upload_time_iso_8601": "2025-07-28T18:01:43.326949Z", "url": "https://files.pythonhosted.org/packages/e5/e6/c1ce674441968e19b8e7d3fae88d430b3b575629d3b7a13ff6d60f2810a0/bigsur-0.0.5-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "505cfd8cf41fc97e2b01373f122d79f1a30d064df95df6b81582005877fd409a", "md5": "243bb0b08c12149e92766b97f590dbb7", "sha256": "74f8a5bdd409eae2808fdfccf8336dd48d5d9bf7dadab2d28883244078fa8aed"}, "downloads": -1, "filename": "bigsur-0.0.5.tar.gz", "has_sig": false, "md5_digest": "243bb0b08c12149e92766b97f590dbb7", "packagetype": "sdist", "python_version": "source", "requires_python": "<4,>=3.9", "size": 12621, "upload_time": "2025-07-28T18:01:44", "upload_time_iso_8601": "2025-07-28T18:01:44.146786Z", "url": "https://files.pythonhosted.org/packages/50/5c/fd8cf41fc97e2b01373f122d79f1a30d064df95df6b81582005877fd409a/bigsur-0.0.5.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:01:37 GMT", "package": "motila", "version": "1.0.6", "json": {"info": {"author": null, "author_email": "Fabrizio Musacchio <fabrizio.musacchio@posteo.de>", "bugtrack_url": null, "classifiers": [], "description": "![GitHub Release](https://img.shields.io/github/v/release/FabrizioMusacchio/motila) [![GPLv3 License](https://img.shields.io/badge/License-GPL%20v3-yellow.svg)](https://opensource.org/licenses/) ![Tests](https://github.com/FabrizioMusacchio/motila/actions/workflows/python-tests.yml/badge.svg) [![PyPI version](https://img.shields.io/pypi/v/motila.svg)](https://pypi.org/project/motila/)\n\n\n\n# MotilA: A pipeline for microglial fine process motility analysis\n\n*MotilA* is a Python-based image analysis pipeline designed to quantify microglial fine process motility from 4D and 5D time-lapse image stacks acquired through multi-photon in vivo imaging. While developed for microglial analysis, *MotilA* can be applied to other cell types and imaging studies as well. The pipeline supports both single-file and batch processing, making it adaptable for various experimental designs and high-throughput analyses. \n\n## What does MotilA do?\n*MotilA* automates the processing and analysis of fluorescence microscopy data, particularly for microglial process dynamics. It performs:\n\n- **Preprocessing**: Image registration, spectral unmixing, histogram equalization, bleach correction, and projection of z-layers to enhance signal quality.\n- **Segmentation**: Adaptive thresholding and noise filtering to isolate microglial processes.\n- **Motility quantification**: Frame-to-frame analysis of pixel changes in microglial structures.\n- **Batch processing**: Automated handling of multiple datasets with standardized parameter settings.\n\n## How is \"motility\" determined?\n*MotilA* quantifies motility by first extracting a sub-volume from the 3D stack at each imaging time point $t_i$ and performing a maximum intensity z-projection. This sacrifices the z-axis information but enables segmentation and quantification of stable, lost, and gained pixels in a computationally efficient manner, facilitating batch processing with standard image analysis techniques. This approach aligns with methodologies used in prior studies, such as [Nebeling et al. (2023)](https://pubmed.ncbi.nlm.nih.gov/36749020/) or [Fuhrmann et al. (2010)](https://pubmed.ncbi.nlm.nih.gov/20305648/). The temporal variation $\\Delta B(t_i)$ is then computed as:\n\n$$\\Delta B(t_i) = 2 \\times B(t_i) - B(t_{i+1})$$\n\nwhere $B(t)$ represents the binarized image at time point $t$. From this, *MotilA* categorizes pixels as follows:\n\n- **Stable pixels (S)**: Pixels that remain unchanged $\\Delta B = 1$.\n- **Gained pixels (G)**: Newly appearing microglial pixels $\\Delta B = -1$.\n- **Lost pixels (L)**: Pixels that disappear $\\Delta B = 2$.\n\nFrom these, MotilA derives the **turnover rate (TOR)**, a key metric for motility:\n\n$$\nTOR = \\frac{G + L}{S + G + L}\n$$\n\nThis turnover rate represents the fraction of pixels undergoing change, providing a quantitative measure of microglial fine process motility.\n\n\n![MotilA pipeline overview](figures/motila_figure_1_demo.png)\n**Core pipeline steps of *MotilA* illustrated using a representative microglial cell from the included example dataset**. **a)** The pipeline begins with loading and z-projecting 3D image stacks, followed by optional preprocessing steps such as spectral unmixing, registration, and histogram equalization (upper panel). The resulting projections are filtered and binarized for segmentation of microglial fine processes (lower panel). **b)** Motility analysis compares consecutive time points by classifying stable (S), gained (G), and lost (L) pixels, from which the turnover rate (TOR) is computed. **c)** The TOR is plotted across time points, quantifying microglial fine process motility over time.\n\n\n## Installation\n### Installation via PyPI\nThe easiest way to install *MotilA* is via [PyPI](https://pypi.org/project/motila):\n\n```bash\nconda create -n motila python=3.12 -y\nconda activate motila\npip install motila\n```\n\n### Installation from source\nIf you prefer to install *MotilA* from source, you can clone or download the GitHub repository:\n\n```bash\ngit clone https://github.com/fabriziomusacchio/MotilA.git\ncd MotilA\n```\n\nWe recommend setting up a dedicated conda environment for development and reproducibility:\n\n```bash\nconda create -n motila python=3.12 mamba -y\nconda activate motila\nmamba install -y numpy scipy matplotlib scikit-image scikit-learn pandas tifffile zarr numcodecs pystackreg openpyxl xlrd ipywidgets ipykernel ipympl\n```\n\n\u26a0\ufe0f **Avoid mixing install methods**:  \nIf you install *MotilA* via `pip`, make sure you do **not place a local folder named `motila/`** in the same directory where you run your scripts (e.g., a cloned or downloaded source folder). Python may try to import from the local folder instead of the installed package, leading to confusing errors.\n\n### Compatibility\nWe have tested *MotilA* for Python 3.9 to 3.12 on Windows, macOS, and Linux systems. The pipeline should work on all these platforms without any issues. If you encounter any platform-specific issues, feel free to [open an issue](https://github.com/FabrizioMusacchio/MotilA/issues).\n\n## Example data set and tutorials\nTo help you get started with *MotilA*, we provide an example dataset and tutorials to guide you through the pipeline steps. \n\nThe example dataset includes a sample image stack and metadata file for testing the pipeline. Please download the example dataset from [Zenodo](https://zenodo.org/records/15061566) (Gockel &  Nieves-Rivera, 2025, doi: 10.5281/zenodo.15061566) and place it in the [`example project`](https://github.com/FabrizioMusacchio/MotilA/tree/main/example%20project) directory.\n\nThe tutorials cover the core pipeline steps, from loading and preprocessing image data to analyzing microglial motility and visualizing the results. A second tutorial demonstrates batch processing for analyzing multiple datasets in a structured project folder.\n\n[Jupyter notebooks](https://github.com/FabrizioMusacchio/MotilA/tree/main/example%20notebooks):\n\n* [single_file_run.ipynb](https://github.com/FabrizioMusacchio/MotilA/blob/main/example%20notebooks/single_file_run.ipynb)\n* [batch_run.ipynb](https://github.com/FabrizioMusacchio/MotilA/blob/main/example%20notebooks/batch_run.ipynb)\n\n[Python scripts](https://github.com/FabrizioMusacchio/MotilA/tree/main/example%20scripts):\n\n* [single_file_run.py](https://github.com/FabrizioMusacchio/MotilA/blob/main/example%20scripts/single_file_run.py)\n* [batch_run.py](https://github.com/FabrizioMusacchio/MotilA/blob/main/example%20scripts/batch_run.py)\n\n\nWe used the following Python script to generate the figures presented in our submitted manuscript:\n\n* [single_file_run_paper.py](https://github.com/FabrizioMusacchio/MotilA/blob/main/example%20scripts/single_file_run_paper.py)\n\nThis script includes all parameter settings used during analysis and can be employed to reproduce the figures. It was applied to a subset of the example dataset described above. This specific subset is available in the repository under [`example project/Data/ID240103_P17_1_cutout/`](https://github.com/FabrizioMusacchio/MotilA/tree/main/example%20project/Data/ID240103_P17_1_cutout/TP000).\n\n\n## Data prerequisites  \nBefore using *MotilA*, ensure that your imaging data meets the following requirements:  \n\n### 1. TIFF file format and image axis order  \n*MotilA* expects input image stacks in TIFF format with axes structured as either **TZCYX** (for multi-channel data) or **TZYX** (for single-channel data). These axes correspond to:  \n\n- **T**: Time (imaging frames over time)  \n- **Z**: Depth (z-stack layers)  \n- **C**: Channels (fluorescent signals from different markers, e.g., microglia and neurons)  \n- **Y**: Height (spatial dimension)  \n- **X**: Width (spatial dimension)  \n\nThis format follows the standard used in **ImageJ/Fiji**. If your dataset does not conform to this structure, *MotilA* provides the function **`tiff_axes_check_and_correct`**, which helps rearrange the axes into the required order. Here is an example of how to use this function:\n\n```python\nimport sys\nsys.path.append('motila/')\nimport motila as mt\nfrom pathlib import Path\n\ntif_file_path = Path(\"path/to/your/image_stack.tif\")\ncorrected_tif_file_path = mt.tiff_axes_check_and_correct(tif_file)\n```\n\nThe output `corrected_tif_file` is the path to the corrected TIFF file, which is automatically saved in the same directory as the original file.\n\n\n### 2. Image registration  \nFor accurate motility analysis, the 3D stacks at each time point \\( t_i \\) must be **spatially registered** to ensure alignment across frames. This step minimizes drift and motion artifacts that could otherwise bias motility quantification.  \n\nIf your dataset requires registration, ensure it has been preprocessed accordingly before running MotilA.  \n\n\n## Pipeline steps\n\n### Core pipeline steps\n*MotilA* follows a structured sequence of image processing and analysis steps to extract motility metrics from microscopy data:\n\n1. **Load image data**: Supports TIFF in TZCYX and TZYX formats.\n2. **Extract sub-volumes**: Extracts a sub-volume from each 3D stack at every time point to ensure consistent analysis across time frames.\n3. **(Optional) Register sub-volumes**: Performs motion correction by aligning sub-volumes across time points, improving tracking accuracy.\n4. **(Optional) Perform spectral unmixing**: Reduces channel bleed-through, particularly for two-channel imaging setups.\n5. **Z-projection**: Converts the extracted 3D sub-volume into a 2D projection, enabling computationally efficient segmentation and tracking.\n6. **(Optional) Register projections**: Aligns projections across time points to further correct for motion artifacts.\n7. **(Optional) Apply histogram equalization**: Enhances contrast using contrast-limited adaptive histogram equalization (CLAHE), improving feature visibility.\n8. **(Optional) Apply histogram matching**: Aligns image intensities across time points to correct for bleaching artifacts, ensuring consistent brightness.\n9. **(Optional) Apply filtering**: Median filtering and Gaussian smoothing reduce noise while preserving relevant microglial structures.\n10. **Segment microglial processes**: Identifies microglial structures using adaptive thresholding and blob detection to extract relevant morphological features.\n11. **Analyze motility**: Tracks changes in segmented regions, classifying stable, gained, and lost pixels to compute motility metrics.\n\n### Batch processing steps\nFor large-scale experiments, *MotilA* supports automated batch processing across multiple datasets:\n\n1. **Define a project folder**: Organize multiple image stacks within a structured directory.\n2. **Process each image stack**: Executes the core pipeline steps on all image stacks within the project folder.\n3. **Save results**: Stores segmented images, histograms, and motility metrics for each image stack in its respective results directory.\n4. **Batch-collect results**: Aggregates motility metrics from multiple datasets, facilitating cohort-level analysis and statistical comparisons.\n\n*MotilA*'s batch processing capabilities streamline the analysis of large datasets, enabling efficient processing and comparison of motility metrics across experimental conditions. The batch process expects a specific project folder structure to automate the processing of multiple datasets. This folder structure includes subdirectories for each dataset, containing the necessary image stacks, metadata files, and results directories. See the [Parameters Overview](#batch_file_processing) for details on the required folder structure and input parameters for batch processing.\n\n\n### Main functions\nThe three main processing functions in *MotilA* are:\n\n* **`process_stack`**: Processes a single image stack, performing all core pipeline steps from image loading to motility analysis.\n* **`batch_process_stacks`**: Automates the processing of multiple image stacks within a project folder, applying the core pipeline steps to each dataset.\n* **`batch_collect`**: Collects motility metrics from multiple datasets, aggregating the results for cohort-level analysis and visualization.\n\n\n## Parameters overview\nThe following sections provide an overview of the input/output parameters for single file processing, batch processing, and batch collection in *MotilA*. These parameters define the settings for image processing, analysis, and results output, allowing you to customize the pipeline for your specific experimental design and data requirements.\n\n### Input/output parameters for single file processing \n#### Input paths\n| Parameter | Values | Description |\n|------------|----------------------|----------------|\n| `Current_ID`            | string | define the ID of the mouse/animal |\n| `group`                 | string | define the group of the mouse/animal |\n| `fname`                 | string | define the full image file path |\n\n\n#### Results output settings\n| Parameter | Values | Description |\n|------------|----------------------|----------------|\n| `RESULTS_Path`          | string | define the path to the results folder; can be absolute or relative to the location of the currently executed script |\n| `clear_previous_results`  | bool (`True` or `False`) | optional clear previous results in the results folder. |\n\n\n<a name=\"batch_file_processing\"></a>\n\n### Input/output parameters for batch processing\n\n| Parameter              | Values                          | Description |\n|------------------------|---------------|-------------|\n| `PROJECT_Path`         | string | define the path to the project folder; can be absolute or relative to the location of the currently executed script |\n| `ID_list`              | list of strings | define the list of all IDs to be processed in `PROJECT_Path`; names must be exact names of the ID folders within the `PROJECT_Path` |\n| `project_tag`          | string | define the tag of the project (folder) to be analyzed; all folders in the ID-folders containing this tag will be processed; can be just a part of the tag (will be searched for in the folder name) |\n| `reg_tif_file_folder`  | string | name of the folder within the (found) `project_tag` folder containing the registered TIF files; must be exact. |\n| `reg_tif_file_tag`     | string | a TIF file containing this tag will be processed within the `reg_tif_file_folder`; if multiple files contain this tag, the folder will be skipped |\n| `RESULTS_foldername`   | string | define the folder name (not the full path!) where the results will be saved within each `project_tag` folder; can also be relative to the `project_tag` folder (e.g., `../motility_analysis/`); the default destination will be inside the `reg_tif_file_folder` |\n| `metadata_file`        | string | name of the metadata file in the `project_tag` folder; must be exact; use the template provided in the MotilA repository to create the metadata file |\n\n\nThe batch process expects a project folder structure as follows:\n\n```\nPROJECT_Path\n\u2502\n\u2514\u2500\u2500\u2500ID1\n\u2502   \u2514\u2500\u2500\u2500project_tag\n\u2502       \u2514\u2500\u2500\u2500reg_tif_file_folder\n\u2502           \u2514\u2500\u2500\u2500reg_tif_file_tag\n\u2502       \u2514\u2500\u2500\u2500RESULTS_foldername\n\u2502       \u2514\u2500\u2500\u2500metadata_file\n\u2502\n\u2514\u2500\u2500\u2500ID2\n\u2502   \u2514\u2500\u2500\u2500project_tag\n\u2502       \u2514\u2500\u2500\u2500reg_tif_file_folder\n\u2502           \u2514\u2500\u2500\u2500reg_tif_file_tag\n\u2502       \u2514\u2500\u2500\u2500RESULTS_foldername\n\u2502       \u2514\u2500\u2500\u2500metadata_file\n\u2502\n\u2514\u2500\u2500\u2500ID3\n\u2502   \u2514\u2500\u2500\u2500project_tag ...\n```\n\nThe folder hierarchy follows a structured, [BIDS-inspired format](https://bids-specification.readthedocs.io), organized by subject ID and project-specific subfolders. While not fully BIDS-compliant, this layout supports consistent batch processing and metadata association.\n\n\nBy placing an Excel file (e.g., `metadata.xls`) in the `project_tag` folder for each animal ID folder (listed in `ID_list`), the following parameters set in the execution script/notebook will be overwritten by the parameters in the Excel file: \n\n* `two_channel_default`\n* `MG_channel_default`\n* `N_channel_default`\n* `spectral_unmixing`\n* `projection_center_default`\n\nThis allows for individual settings for each dataset. The table below shows an example of the content of the `metadata.xls` file:\n\n| Two Channel | Registration Channel | Registration Co-Channel | Microglia Channel | Neuron Channel | Spectral Unmixing | Projection Center 1 |\n| ----------- | -------------------- | ----------------------- | ----------------- | -------------- | ----------------- | ------------------- |\n| True        | 1                    | 0                       | 0                 | 1              | False             | 28                  |\n\nA template for this excel files is provided in the `[templates](templates/)` folder. In this template, ignore the columns `Registration Channel`  and `Registration Co-Channel` as they are not used in *MotilA*.\n\nYou can add several projection centers (`Projection Center 1`, `Projection Center 2`, etc.) to the excel file. The pipeline will then create a projection for each center along with the corresponding analysis results.\n\n\n\n### General processing settings (single file and batch processing)\n\n#### Projection settings\n| Parameter | Values | Description |\n|------------|----------------------|----------------|\n| `projection_layers_default` | integer   | define the number of z-layers to project for motility analysis. |\n| `projection_center_default` | integer   | define the center slice of the projection |\n\nIn case of image volumes densely packed with microglia, we recommend to subdivided the volume into several subvolumes with different projection centers. This will help to avoid overlapping microglia in the projection and thus ensure a more accurate capturing of the microglial processes' motility.\n\nAvoid including blood vessels in the projection center. Blood vessels can lead to false-positive motility results, as the pipeline cannot distinguish between microglial processes and blood vessels. \n\n*MotilA* performs a sanity check of the desired subvolume defined by the input parameters `projection_center_default` and `projection_layers_default`. If the subvolume exceeds the image dimensions, the pipeline will automatically adjust the subvolume to fit within the image dimensions. However, this may lead to a smaller subvolume than initially defined. To avoid this, ensure that the subvolume fits within the image dimensions. The final chosen parameters will be saved in a log Excel file into the results folder.\n\n#### Thresholding settings\n| Parameter | Values | Description |\n|------------|----------------------|----------------|\n| `threshold_method`            | string | choose from: `otsu`, `li`, `isodata`, `mean`, `triangle`, `yen`, `minimum`. |\n| `blob_pixel_threshold`        | integer | define the minimal pixel area of a blob during segmentation; 100 is a good starting value |\n| `compare_all_threshold_methods` | bool (`True` or `False`) | optional comparison plot all threshold methods |\n\n\n#### Image enhancement settings\n| Parameter | Values | Description |\n|------------|----------------------|----------------|\n| `hist_equalization`     | bool (`True` or `False`)  | enhance histograms WITHIN each 3D stack. |\n| `hist_equalization_clip_limit` | float | clip limit for the histogram equalization (default is 0.05); the higher the value, the more intense the contrast enhancement, but also the more noise is amplified |\n| `hist_equalization_kernel_size` | `None`/int tuple | kernel size for the histogram equalization; `None` (default) for automatic, or use a tuple (x,y) for a fixed size; when using a tuple, you can start increasing the values from multiples of 8, e.g., (8,8), (16,16), (24,24), (32,32), ... (128,128), ... |\n| `hist_match`            | bool (`True` or `False`)  | match histograms across 3D stacks |\n| `histogram_ref_stack`   | integer     | define the reference 3D stack for histogram matching. |\n\nHistogram equalization enhances the contrast of the image by stretching the intensity range. This can be particularly useful for images with low contrast or uneven illumination. The `hist_equalization_clip_limit` parameter controls the intensity clipping limit for the histogram equalization. A higher value increases the intensity range but may also amplify noise. The `hist_equalization_kernel_size` parameter defines the kernel size for the histogram equalization. The default is `None` which let's the function choose the kernel size automatically. In cases of occurring block artifacts, you can set a fixed kernel size (e.g., (8,8), (16,16), (24,24), ...).\n\nHistogram matching aligns the intensity distributions of different image stacks, ensuring consistent brightness and contrast across time points. The `histogram_ref_stack` parameter defines the reference stack for histogram matching. This reference stack serves as the basis for matching the intensity distributions of all other stacks. Both, the output plot `Normalized average brightness drop rel. to t0.pdf` and Excel file `Normalized average brightness of each stack.xlsx` show the average brightness of each stack relative to the reference stack. This can help to assess the quality of each time point stack and which time points might be excluded from further analysis.\n\n\n#### Filter settings\n| Parameter | Values | Description |\n|------------|----------------------|----------------|\n| `median_filter_slices` | string/bool (`square`, `circular`, or `False`) | median filter on slices before projecting |\n| `median_filter_window_slices`    | integer/float      | median filter window size on slices before projecting; for `square` median filter option, insert odd integer values, for `circular` floating point numbers |\n| `median_filter_projections`      | string/bool (`square`, `circular`, or `False`) | median filter on projections |\n| `median_filter_window_projections` | integer/float | median filter window size on projections; for `square` median filter option, insert odd integer values, for `circular` floating point numbers |\n| `gaussian_sigma_proj` | integer | standard deviation of Gaussian blur filter applied on the projected stack, set to 0 to turn it off  |\n\n\nRegarding median filtering, you have the option to filter on the single slices BEFORE the projection (**`median_filter_slices`**) and/or on the projected images (**`median_filter_projections`**). For both options, you can choose from:\n\n* `False` (no filtering)\n* `square` (square kernel): integer numbers (3, 5, 9)\n* `circular` (disk-shaped kernel; analogous to the median filter in ImageJ/Fiji): only values >= 0.5 allowed/have an effect\n\nWhen you apply median filtering, you need to additionally provide the kernel size (**`median_filter_window_slices`** for single slices and **`median_filter_window_projections`** for projections). Depending on the chosen filtering kernel method, you can choose a kernel size as listed above.\n\nGaussian smoothing further  enhances the contrast and reduces noise. Set\n\n* `gaussian_smoothing` to 0: no smoothing, or\n* `gaussian_smoothing` to a value > 0: the standard deviation of the Gaussian kernel.\n\n\n#### Channel settings\n| Parameter | Values | Description |\n|------------|----------------------|----------------|\n| `two_channel_default`  | bool (`True` or `False`) | define if the stack has two channels. |\n| `MG_channel_default`   | integer    | set the channel number of the Microglia. |\n| `N_channel_default`    | integer    | set the channel number of the Neurons/2nd channel. |\n\nIf your stack contains only one channel, set `two_channel_default = False`; any value set in `N_channel_default` will be ignored.\n\nIf `metadata.xls` is present in `project_tag` folder, the above defined values (`two_channel_default`, `MG_channel_default`, `N_channel_default`) are ignored and values from the metadata.xls are used instead  (**in batch processing only!**)\n\n\n#### Registration settings\n| Parameter | Values | Description |\n|------------|----------------------|----------------|\n| `regStack3d` | bool (`True` or `False`)     | perform optional registration of slices within each 3D time-stack. |\n| `regStack2d` | bool (`True` or `False`)    | perform optional registration of projections on each other using phase cross-correlation. |\n| `usepystackreg` | bool (`True` or `False`) | If `True`, use pystackreg (StackReg) for 2D registration instead of phase cross-correlation. |\n| `template_mode` | string    | set the template mode for 3D registration (`mean`, `median`, `max`, `min`, `std`, and `var`).  |\n| `max_xy_shift_correction` | integer     | Set the maximal shift in x/y direction for 2D registration. |\n\n\n*MotilA* provides the option to register the image stacks. Two registration options are available:\n\n* `regStack3d`: register slices WITHIN each 3D time-stack; `True` or `False`\n* `regStack2d`: register projections on each other;  `True` or `False`\n\nWith `template_mode`you can define the template mode for the registration. Choose between `mean` (default), `median`, `max`, `min`, `std`, and `var`.\n\nWith `max_xy_shift_correction`, you can define the maximum allowed shift in x and y (and z) direction for the registration. This is useful to avoid overcorrection.\n\n\n#### Spectral unmixing settings\n| Parameter | Values | Description |\n|------------|----------------------|----------------|\n| `spectral_unmixing` | bool (`True` or `False`) | perform optional spectral unmixing to correct for channel bleed-through. |\n| `spectral_unmixing_amplifyer` | integer  | amplify the MG channel to preserve more signal from this channel; set to 1 for no amplification |\n| `spectral_unmixing_median_filter_window` | integer | Must be an integer; `1=off`, `3=common`, `5=strong`, `7=very strong`.       |\n\n*MotilA* provides the option to perform spectral unmixing on two-channel data. At the moment, only a simple method is implemented, which subtracts the N-channel from the MG-channel. Set `spectral_unmixing` to `True` to enable this feature. \n\nWith `spectral_unmixing_amplifyer_default` you can define the amplification factor for the MG-channel before subtraction. This can be useful to preserve more information in the MG-channel.\n\n`spectral_unmixing_median_filter_window` defines the kernel size for median filtering of N-channel before subtraction. This can be useful to reduce noise in the N-channel and, thus, achieve a better unmixing result. Allowed are odd integer numbers (3, 5, 9, ...).\n\n#### Debug settings\n| Parameter | Values | Description |\n|------------|----------------------|----------------|\n| `debug_output` | bool (`True` or `False`) | enable debug output for intermediate results; at the moment, only memory outputs are given.|\n| `stats_plots` | bool (`True` or `False`) | enable additional statistics plots for the motility analysis (at the moment: histogram plots of the binarized pixels) |\n\n\n### Input/output parameters for batch collection\n\n| Parameter | Values | Description |\n|-------------------|----------------------------------|-------------|\n| `PROJECT_Path`    | string | define the path to the project folder; can be absolute or relative to the location of the currently executed script |\n| `RESULTS_Path`    | string | define the path to the results folder; combined results of the cohort analysis will be saved here; can be absolute or relative to the location of the currently executed script |\n| `ID_list`         | list of strings | define the list of all IDs to be processed in `PROJECT_Path`; names must be exact names of the ID folders |\n| `project_tag`     | string | define the tag of the project (folder) to be analyzed; all folders in the ID-folders containing this tag will be processed |\n| `motility_folder` | string | folder name (not the path!) containing motility analysis results in each ID folder/project_tag folder; must be exact; all projection center folders therein will be processed to collect the results. |\n\nThe batch collection process expects the same project folder structure as the batch processing (see above).\n\n## Example usage\n\n### Single file processing\nHere is an example of how to use *MotilA* for single file processing. First, import the necessary modules:\n\n```python\nimport sys\nsys.path.append('../motila')\nimport motila as mt\nfrom pathlib import Path\n```\n\n**Note**: `sys.path.append('../motila')` is used to add the *MotilA* directory to the system path \u2013 relative to the current working directory. If you execute this notebook from a different location, you may need to adjust the path accordingly.\n\nYou can verify the correct import by running the following cell:\n\n```python\nmt.hello_world()\n```\n\nInit the logger to get a log file for your current run:\n\n```python\n# init logger:\nlog = mt.logger_object()\n```\n\nThen, define the corresponding parameters as described above. When you have set the parameters, you can run the pipeline:\n\n```python\nmt.process_stack(fname=fname,\n                MG_channel=MG_channel, \n                N_channel=N_channel,\n                two_channel=two_channel,\n                projection_layers=projection_layers,\n                projection_center=projection_center,\n                histogram_ref_stack=histogram_ref_stack,\n                log=log,\n                blob_pixel_threshold=blob_pixel_threshold, \n                regStack2d=regStack2d,\n                regStack3d=regStack3d,\n                template_mode=template_mode,\n                spectral_unmixing=spectral_unmixing,\n                hist_equalization=hist_equalization,\n                hist_equalization_clip_limit=hist_equalization_clip_limit,\n                hist_equalization_kernel_size=hist_equalization_kernel_size,\n                hist_match=hist_match,\n                RESULTS_Path=RESULTS_Path,\n                ID=Current_ID,\n                group=group,\n                threshold_method=threshold_method,\n                compare_all_threshold_methods=compare_all_threshold_methods,\n                gaussian_sigma_proj=gaussian_sigma_proj,\n                spectral_unmixing_amplifyer=spectral_unmixing_amplifyer,\n                median_filter_slices=median_filter_slices,\n                median_filter_window_slices=median_filter_window_slices,\n                median_filter_projections=median_filter_projections,\n                median_filter_window_projections=median_filter_window_projections,\n                clear_previous_results=clear_previous_results,\n                spectral_unmixing_median_filter_window=spectral_unmixing_median_filter_window,\n                debug_output=debug_output,\n                stats_plots=stats_plots)\n```\n\n\n### Batch processing\nThe batch processing is similar to the single file processing. You need to define the parameters as described above and then run the batch processing function:\n\n```python\nmt.batch_process_stacks(PROJECT_Path=PROJECT_Path, \n                        ID_list=ID_list, \n                        project_tag=project_tag, \n                        reg_tif_file_folder=reg_tif_file_folder,\n                        reg_tif_file_tag=reg_tif_file_tag,\n                        metadata_file=metadata_file,\n                        RESULTS_foldername=RESULTS_foldername,\n                        MG_channel=MG_channel, \n                        N_channel=N_channel, \n                        two_channel=two_channel,\n                        projection_center=projection_center, \n                        projection_layers=projection_layers,\n                        histogram_ref_stack=histogram_ref_stack, \n                        log=log, \n                        blob_pixel_threshold=blob_pixel_threshold,\n                        regStack2d=regStack2d, \n                        regStack3d=regStack3d, \n                        template_mode=template_mode,\n                        spectral_unmixing=spectral_unmixing, \n                        hist_equalization=hist_equalization, \n                        hist_equalization_clip_limit=hist_equalization_clip_limit,\n                        hist_equalization_kernel_size=hist_equalization_kernel_size,\n                        hist_match=hist_match,\n                        max_xy_shift_correction=max_xy_shift_correction,\n                        threshold_method=threshold_method, \n                        compare_all_threshold_methods=compare_all_threshold_methods,\n                        gaussian_sigma_proj=gaussian_sigma_proj, \n                        spectral_unmixing_amplifyer=spectral_unmixing_amplifyer,\n                        median_filter_slices=median_filter_slices, \n                        median_filter_window_slices=median_filter_window_slices,\n                        median_filter_projections=median_filter_projections, \n                        median_filter_window_projections=median_filter_window_projections,\n                        clear_previous_results=clear_previous_results, \n                        spectral_unmixing_median_filter_window=spectral_unmixing_median_filter_window,\n                        debug_output=debug_output,\n                        stats_plots=stats_plots)\n```\n\nAfter processing all datasets, you can collect the results and save them to a central output folder. This allows you to perform cohort-level analyses and visualize the results across all datasets. To collect the results, use the following function:\n\n```python\nmt.batch_collect(PROJECT_Path=PROJECT_Path, \n                 ID_list=ID_list, \n                 project_tag=project_tag, \n                 motility_folder=motility_folder,\n                 RESULTS_Path=RESULTS_Path,\n                 log=log)\n```\n\n## Assessing your results\n### Single file processing\nAfter running the pipeline, you can assess the results in the specified output folder. The results of each processing step described above are saved in separate tif and PDF files. By carefully investigating these results, you can evaluate the quality of the processing and adjust the parameters if necessary. An example assessment is given in the tutorial notebook [`single_file_run.ipynb`](https://github.com/FabrizioMusacchio/MotilA/blob/main/example%20notebooks/single_file_run.ipynb) including visualizations of the results.\n\nBesides the intermediate results, the motility metrics are saved in an Excel file called `motility.xlsx` in the results folder. This file contains the \n\n* gained pixels (G),\n* lost pixels (L),\n* stable pixels (S), and\n* the turnover rate (TOR) for each time point,\n\nallowing you to analyze the motility dynamics of microglial processes over time.\n\nAdditionally, brightness metrics and pixel counts are saved in separate Excel files for further analysis. The average pixel brightness is an indicator of the overall intensity of the microglial cells in the image. A decreasing brightness over time could indicate bleaching or other issues. Note that the results show are those after applying the histogram matching (if chosen). Thus, if the average pixel brightness still drops even after histogram matching, the shown values may help to assess the quality each time point stack and which time points might be excluded from further analysis.\n\nThe cell pixel area is the number of segmented pixel of all (projected) MG cells per stack. Usually, this number should remain relatively stable over time as the cell motility does not imply a change in cell area/size. A decrease in cell pixel area could indicate a loss of cells over time, e.g., due to cell death or other issues. Bleaching or other issues could also lead to a decrease in cell pixel area. Thus, the same considerations as for the average pixel brightness apply here.\n\n### Batch processing\nThe batch collection function aggregates the motility metrics from all datasets into a single Excel file, allowing you to compare the motility dynamics across different experimental conditions and animals. This cohort-level analysis provides a comprehensive overview of the motility metrics, enabling you to identify trends, differences, or similarities between groups. The following Excel files are generated:\n\n* `all_motility.xlsx`: Contains the motility metrics (G, L, S, TOR) for each project tag and time point across all datasets.\n* `all_brightness.xlsx`: Contains the average pixel brightness for each project tag and time point across all datasets.\n* `all_cell_pixel_area.xlsx`: Contains the cell pixel area for each project tag and time point across all datasets.\n* `average_motility.xlsx`: Contains the average motility metrics (G, L, S, TOR) for each project tag across all datasets (i.e., the motility dynamics averaged over all time points within each dataset/project tag).\n\n\n## How to contribute\n*MotilA* is an open-source software and improves because of contributions from users all over the world. If there is something about *Motila* that you would like to work on, then please reach out.\n\n## License: GPL-3.0 License\nThis software is licensed under the GNU General Public License v3.0 (GPL-3.0). In summary, you are free to:\n\n- **Use** this software for any purpose.\n- **Modify** the source code and adapt it to your needs.\n- **Distribute** copies of the original or modified software.\n\nHowever, you must:\n- **Share modifications under the same license** (copyleft).  \n  If you distribute a modified version, you must also make the source code available under GPL-3.0.  \n- **Include the original copyright notice and license** in any copies or substantial portions of the software.  \n\n You may **not**:\n- Use this software in **proprietary** (closed-source) applications.  \n- Distribute modified versions under a more restrictive license.  \n\nThis software is distributed WITHOUT ANY WARRANTY; without even the implied warranty of   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the LICENSE file or <https://www.gnu.org/licenses/gpl-3.0.html> for full terms.\n\n## Citation\nIf you use this software in your research, we kindly ask you to cite it using the following BibTeX entry:\n\n\n```\n@software{musacchio2025motila,\n  author       = {Fabrizio Musacchio and Sophie Crux and Felix Nebeling and Nala Gockel and Falko Fuhrmann and Martin Fuhrmann},\n  title        = {MotilA: A pipeline for microglial fine process motility analysis},\n  year         = {2025},\n  url          = {https://github.com/FabrizioMusacchio/motila},\n  version      = {1.0.0},\n  note         = {Accessed: YYYY-MM-DD},\n}\n```\n\n**Note**: We are currently in the process of retrieving a DOI for this project. As soon as the DOI becomes available, we will update the citation reference accordingly.  *(Status: March 21, 2025)*\n\n## Acknowledgments\nWe gratefully acknowledge the **Light Microscopy Facility (LMF)** and the **Animal Research Facility (ARF)** at the **German Center for Neurodegenerative Diseases (DZNE)** for their essential support in acquiring the in vivo imaging data upon which this pipeline is built.\n\nWe also thank [Gockel & Nieves-Rivera (2025)](https://zenodo.org/records/15061566) and colleagues for providing the example dataset used in this repository, which allows users to test and explore MotilA.\n\n\n## Contact\nFor questions, suggestions, or feedback regarding *MotilA*, please contact:\n\nFabrizio Musacchio  \nGerman Center for Neurodegenerative Diseases (DZNE)  \nEmail: [fabrizio.musacchio@dzne.de](mailto:fabrizio.musacchio@dzne.de)\nGitHub: @[FabrizioMusacchio](https://github.com/FabrizioMusacchio)\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "motila", "package_url": "https://pypi.org/project/motila/", "platform": null, "project_url": "https://pypi.org/project/motila/", "project_urls": {"Documentation": "https://github.com/fabriziomusacchio/motila#readme", "Homepage": "https://github.com/fabriziomusacchio/motila", "Source": "https://github.com/fabriziomusacchio/motila", "Tracker": "https://github.com/fabriziomusacchio/motila/issues"}, "provides_extra": null, "release_url": "https://pypi.org/project/motila/1.0.6/", "requires_dist": ["numpy", "scipy", "matplotlib", "tifffile", "scikit-image", "scikit-learn", "zarr", "SimpleITK", "pystackreg", "pandas", "numcodecs", "openpyxl", "xlrd", "ipywidgets", "ipykernel", "ipympl"], "requires_python": ">=3.9", "summary": "MotilA: A pipeline for microglial motility analysis", "version": "1.0.6", "yanked": false, "yanked_reason": null}, "last_serial": 30388320, "urls": [{"comment_text": null, "digests": {"blake2b_256": "5694d60254deea3d032585aab06f89cc87186db8007c0d28cb5a1c5f1f18c4d6", "md5": "7fc9bfd34b9ee76ade87935e323af408", "sha256": "8a6a26b4a659f533349114963abd578b01f73a91273f700964d8148162cac346"}, "downloads": -1, "filename": "motila-1.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "7fc9bfd34b9ee76ade87935e323af408", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 60738, "upload_time": "2025-07-28T18:01:37", "upload_time_iso_8601": "2025-07-28T18:01:37.556774Z", "url": "https://files.pythonhosted.org/packages/56/94/d60254deea3d032585aab06f89cc87186db8007c0d28cb5a1c5f1f18c4d6/motila-1.0.6-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "de25a5f96a8216f51d8820b4987e02b0ae4fca814eec9ef4deb9c81b05186c5b", "md5": "c645d3ac4fd38e6c642bf1630bc9986a", "sha256": "0a47a29c0625c73f15da102601bbff71be8b779eab2ad6cd6c6c4b7c0b94187f"}, "downloads": -1, "filename": "motila-1.0.6.tar.gz", "has_sig": false, "md5_digest": "c645d3ac4fd38e6c642bf1630bc9986a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 50038134, "upload_time": "2025-07-28T18:01:56", "upload_time_iso_8601": "2025-07-28T18:01:56.782764Z", "url": "https://files.pythonhosted.org/packages/de/25/a5f96a8216f51d8820b4987e02b0ae4fca814eec9ef4deb9c81b05186c5b/motila-1.0.6.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:01:18 GMT", "package": "lisnTorch", "version": "0.1.0", "json": {"info": {"author": "lishuainan", "author_email": "lishuainan0209@163.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description-Content-Type", "Requires-Dist", "Requires-Python", "Summary"], "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "lisnTorch", "package_url": "https://pypi.org/project/lisnTorch/", "platform": null, "project_url": "https://pypi.org/project/lisnTorch/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/lisnTorch/0.1.0/", "requires_dist": ["matplotlib", "numpy", "torch", "pandas", "numpy", "torchsummary", "thop"], "requires_python": ">=3.6", "summary": "\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8f85\u52a9\u5de5\u5177\u5305", "version": "0.1.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388398, "urls": [{"comment_text": null, "digests": {"blake2b_256": "73fc6d43291ede483315dc9e40711ec5bbf52d08aaa05d994bb8018028eac7da", "md5": "2d00113e5e7881dca1cd55289073fac0", "sha256": "2d8a071f0e4816c49b0adc5ddc155bf2710cf115448f651db077ee2b4606d1d9"}, "downloads": -1, "filename": "lisntorch-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "2d00113e5e7881dca1cd55289073fac0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 6299, "upload_time": "2025-07-28T18:01:18", "upload_time_iso_8601": "2025-07-28T18:01:18.202834Z", "url": "https://files.pythonhosted.org/packages/73/fc/6d43291ede483315dc9e40711ec5bbf52d08aaa05d994bb8018028eac7da/lisntorch-0.1.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "badea840546784a15433bdee76150dee2bd66c280f8107448d9fc3666343f1dd", "md5": "aa1e620bfbdea9a63589cdd4bc27ec52", "sha256": "43c2ac1870051a770f7ea77b81c9e3a6592207a6dee62a7263a0ee802ce755c9"}, "downloads": -1, "filename": "lisntorch-0.1.0.tar.gz", "has_sig": false, "md5_digest": "aa1e620bfbdea9a63589cdd4bc27ec52", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6482, "upload_time": "2025-07-28T18:01:19", "upload_time_iso_8601": "2025-07-28T18:01:19.680591Z", "url": "https://files.pythonhosted.org/packages/ba/de/a840546784a15433bdee76150dee2bd66c280f8107448d9fc3666343f1dd/lisntorch-0.1.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:01:01 GMT", "package": "bloqade", "version": "0.27.0", "json": {"info": {"author": null, "author_email": "Roger-luo <rluo@quera.com>, kaihsin <khwu@quera.com>, weinbe58 <pweinberg@quera.com>, johnzl-777 <jlong@quera.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12"], "description": "# Welcome to Bloqade -- QuEra's Neutral Atom SDK\n\n[![CI](https://github.com/QuEraComputing/bloqade/actions/workflows/ci.yml/badge.svg)](https://github.com/QuEraComputing/bloqade/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/QuEraComputing/bloqade/graph/badge.svg?token=BpHsAYuzdo)](https://codecov.io/gh/QuEraComputing/bloqade)\n[![Supported Python versions](https://img.shields.io/pypi/pyversions/bloqade.svg?color=%2334D058)](https://pypi.org/project/bloqade)\n[![Documentation](https://img.shields.io/badge/Documentation-6437FF)](https://bloqade.quera.com/)\n[![DOI](https://zenodo.org/badge/629628885.svg)](https://zenodo.org/doi/10.5281/zenodo.11114109)\n\n\nBloqade is a Python SDK for neutral atom quantum computing. It provides a set of embedded domain-specific languages (eDSLs) for programming neutral atom quantum computers. Bloqade is designed to be a high-level, user-friendly SDK that abstracts away the complexities of neutral atom quantum computing, allowing users to focus on developing quantum algorithms and compilation strategies for neutral atom quantum computers.\n\n> [!IMPORTANT]\n>\n> This project is in the early stage of development. API and features are subject to change.\n\n## Installation\n\n### Install via `uv` (Recommended)\n\n```py\nuv add bloqade\n```\n\n## Documentation\n\nThe documentation is available at [https://bloqade.quera.com/latest/](https://bloqade.quera.com/latest/). We are at an early stage of completing the documentation with more details and examples, so comments and contributions are most welcome!\n\n## Roadmap\n\nWe use github issues to track the roadmap. There are more feature requests and proposals in the issues. Here are some of the most wanted features we wish to implement by 2025 summer (July):\n\n- [x] QASM2 dialect (dialect, parser, pyqrack backend, ast, codegen)\n- [x] QASM2 extensions (e.g. parallel gates, noise, etc.)\n- [x] STIM dialect (dialect, codegen)\n- [ ] structural gate dialect (language proposal, dialect, passes)\n- [ ] atom-move dialect (language proposal, dialect, passes)\n- [ ] atom move animation backend\n\nProposal for the roadmap and feature requests are welcome!\n\n## License\n\nApache License 2.0 with LLVM Exceptions\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "bloqade", "package_url": "https://pypi.org/project/bloqade/", "platform": null, "project_url": "https://pypi.org/project/bloqade/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/bloqade/0.27.0/", "requires_dist": ["bloqade-analog~=0.16.3", "bloqade-circuit[cirq,qasm2,qbraid,stim,vis]~=0.6.0"], "requires_python": ">=3.10", "summary": "The software development toolkit for neutral atom arrays.", "version": "0.27.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388304, "urls": [{"comment_text": null, "digests": {"blake2b_256": "8601a1e64fbad3417fc4d9662c4d55565988a136de26b238d9bbc2239f4ad7ba", "md5": "8581765a06de5560d6806bdde86a7a79", "sha256": "9b892a3bfd77fc88aa474831a09aa5787bef3bcd8a3a44edc3487e2697b87d16"}, "downloads": -1, "filename": "bloqade-0.27.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8581765a06de5560d6806bdde86a7a79", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.10", "size": 7797, "upload_time": "2025-07-28T18:01:01", "upload_time_iso_8601": "2025-07-28T18:01:01.938161Z", "url": "https://files.pythonhosted.org/packages/86/01/a1e64fbad3417fc4d9662c4d55565988a136de26b238d9bbc2239f4ad7ba/bloqade-0.27.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "a00ecbbfb3a381d56b3f7376267b880332571d760f9cd4b2e7ee7f3866c34cc8", "md5": "057590ec752b90d82bf93c1296a68071", "sha256": "52cca3a908d587e5901d602307db7eb1985625074b2874de301b8f1cdf622757"}, "downloads": -1, "filename": "bloqade-0.27.0.tar.gz", "has_sig": false, "md5_digest": "057590ec752b90d82bf93c1296a68071", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.10", "size": 7636662, "upload_time": "2025-07-28T18:01:03", "upload_time_iso_8601": "2025-07-28T18:01:03.407677Z", "url": "https://files.pythonhosted.org/packages/a0/0e/cbbfb3a381d56b3f7376267b880332571d760f9cd4b2e7ee7f3866c34cc8/bloqade-0.27.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 18:00:59 GMT", "package": "aio-overpass", "version": "0.15.1", "json": {"info": {"author": "Tim Wiechers", "author_email": "Tim Wiechers <mail@timwie.dev>", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Framework :: AsyncIO", "Framework :: aiohttp", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Topic :: Internet :: WWW/HTTP", "Topic :: Scientific/Engineering :: GIS", "Topic :: Software Development :: Libraries :: Python Modules", "Typing :: Typed"], "description": "A client for the [Overpass API], a read-only API that serves up custom selected\nparts of [OpenStreetMap] data.\n\nThe Overpass API is optimized for data consumers that need a few elements within\na glimpse or up to roughly 10 million elements in some minutes, both selected by\nsearch criteria like location, type of objects, tag properties, proximity, or\ncombinations of them. To make use of it, you should familiarize yourself with\n[Overpass QL], the query language used to select the elements that you want.\n\n#### Contents\n- [Features](#features)\n- [Usage](#usage)\n- [Choosing Extras](#choosing-extras)\n\n#### See also\n- An overview of modules, classes and functions can be found in the [API reference](http://www.timwie.dev/aio-overpass/)\n- The version history is available in [RELEASES.md](https://github.com/timwie/aio-overpass/blob/main/RELEASES.md)\n- There are some notebooks to check out in [examples/](https://github.com/timwie/aio-overpass/tree/main/examples)\n- Developers can find some instructions in [CONTRIBUTING.md](https://github.com/timwie/aio-overpass/blob/main/CONTRIBUTING.md)\n- The Overpass API [repository](https://github.com/drolbr/Overpass-API),\n  its [blog](https://dev.overpass-api.de/blog/),\n  its [user's manual](https://dev.overpass-api.de/overpass-doc/en/index.html)\n  and  its [release notes](https://wiki.openstreetmap.org/wiki/Overpass_API/versions)\n- [Overpass Turbo] to prototype your queries in your browser\n\n<br>\n\n## Features\n- Asynchronous requests using [aiohttp]\n- Parallel queries within rate limits\n- Fault tolerance through a (customizable) retry strategy\n- **Extras**\n  - Typed elements that simplify browsing result sets\n  - [Shapely] geometries for manipulation and analysis\n  - [GeoJSON] exports\n  - Simplified querying and processing of public transportation routes\n\n### Design Goals\n- A small and stable set of core functionality.\n- Good defaults for queries and retrying.\n- Sensible and spec-compliant GeoJSON exports for all objects that represent spatial features.\n- Detailed documentation that supplements learning about OSM and the Overpass API.\n- Room for extensions that simplify querying and/or processing of spatial data\n  in specific problem domains.\n\n<br>\n\n## Usage\nThere are three basic steps to fetch the spatial data you need:\n\n1. **Formulate a query**\n    - Either write your own custom query, f.e. `Query(\"node(5369192667); out;\")`,\n    - or use one of the `Query` subclasses, f.e. `SingleRouteQuery(relation_id=1643324)`.\n\n2. **Call the Overpass API**\n    - Prepare your client with `client = Client(user_agent=...)`.\n    - Use `await client.run_query(query)` to fetch the result set.\n\n3. **Collect results**\n    - Either access the raw result dictionaries with `query.result_set`,\n    - or use a collector, f.e. `collect_elements(query)` to get a list of typed `Elements`.\n    - Collectors are often specific to queries - `collect_routes` requires a `RouteQuery`,\n      for instance.\n\n<br>\n\n### Example: looking up a building in Hamburg\n#### a) Results as Dictionaries\nYou may use the `.result_set` property to get a list of all query results\nwithout any extra processing:\n\n```python\nfrom aio_overpass import Client, Query\n\nquery = Query('way[\"addr:housename\"=Elbphilharmonie]; out geom;')\n\nclient = Client()\n\nawait client.run_query(query)\n\nquery.result_set\n```\n\n```python\n[\n      {\n          \"type\": \"way\",\n          \"id\": 24981342,\n          # ...\n          \"tags\": {\n              \"addr:city\": \"Hamburg\",\n              \"addr:country\": \"DE\",\n              \"addr:housename\": \"Elbphilharmonie\",\n              # ...\n          },\n      }\n]\n```\n\n<br>\n\n#### b) Results as Objects\nThis will give you a user-friendly Python interface\nfor [nodes](https://www.timwie.dev/aio-overpass/aio_overpass/element.html#Node),\n[ways](https://www.timwie.dev/aio-overpass/aio_overpass/element.html#Way),\nand [relations](https://www.timwie.dev/aio-overpass/aio_overpass/element.html#Relation).\nHere we use the `.tags` property:\n\n```python\nfrom aio_overpass.element import collect_elements\n\nelems = collect_elements(query)\n\nelems[0].tags\n```\n\n```python\n{\n    \"addr:city\": \"Hamburg\",\n    \"addr:country\": \"DE\",\n    \"addr:housename\": \"Elbphilharmonie\",\n    # ...\n}\n\n```\n\n<br>\n\n#### c) Results as GeoJSON\nThe processed elements can also easily be converted to GeoJSON:\n\n```python\nimport json\n\njson.dumps(elems[0].geojson, indent=4)\n```\n\n```json\n{\n    \"type\": \"Feature\",\n    \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n            [\n                [\n                    9.9832434,\n                    53.5415472\n                ],\n                ...\n            ]\n        ]\n    },\n    \"properties\": {\n        \"id\": 24981342,\n        \"type\": \"way\",\n        \"tags\": {\n            \"addr:city\": \"Hamburg\",\n            \"addr:country\": \"DE\",\n            \"addr:housename\": \"Elbphilharmonie\",\n            ...\n        },\n        ...\n    },\n    \"bbox\": [\n        9.9832434,\n        53.540877,\n        9.9849674\n        53.5416212,\n    ]\n}\n```\n\n<br>\n\n## Choosing Extras\nThis library can be installed with a number of optional extras.\n\n- Install no extras, if you're fine with `dict` result sets.\n\n- Install the `shapely` extra, if you would like the convenience of typed OSM elements.\n  It is also useful if you are interested in elements' geometries,\n  and either already use Shapely, or want a simple way to export [GeoJSON](https://en.wikipedia.org/wiki/GeoJSON).\n\n  - This includes the `pt` module to make it easier to interact with public transportation routes.\n    Something seemingly trivial like listing the stops of a route can have unexpected pitfalls,\n    since stops can have multiple route members, and may have a range of different tags and roles.\n    This submodule will clean up the relation data for you.\n\n- Install the `networkx` extra to enable the `pt_ordered` module, if you want a route's path as a\n  simple line from A to B. It is hard to do this consistently, mainly because ways are not always\n  ordered, and stop positions might be missing. You can benefit from this submodule if you wish to\n  - render a route's path between any two stops\n  - measure the route's travelled distance between any two stops\n  - validate the order of ways in the relation\n  - check if the route relation has gaps\n\n- Install the `joblib` extra to speed up `pt_ordered.collect_ordered_routes()`, which can benefit\n  greatly from parallelization.\n\n[aiohttp]: https://docs.aiohttp.org/en/stable/\n[GeoJSON]: https://en.wikipedia.org/wiki/GeoJSON\n[OpenStreetMap]: https://www.openstreetmap.org\n[Overpass API]: https://wiki.openstreetmap.org/wiki/Overpass_API\n[Overpass QL]: https://wiki.openstreetmap.org/wiki/Overpass_API/Overpass_QL\n[Overpass Turbo]: http://overpass-turbo.eu/\n[Shapely]: https://shapely.readthedocs.io/en/latest/manual.html\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "geojson, geospatial, gis, openstreetmap, osm, overpass-api, public-transport, spatial-analysis, spatial-data, shapely", "license": null, "license_expression": "MIT", "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "aio-overpass", "package_url": "https://pypi.org/project/aio-overpass/", "platform": null, "project_url": "https://pypi.org/project/aio-overpass/", "project_urls": {"Release Notes": "https://github.com/timwie/aio-overpass/blob/main/RELEASES.md", "Test Coverage": "https://codecov.io/gh/timwie/aio-overpass", "documentation": "https://www.timwie.dev/aio-overpass/", "repository": "https://github.com/timwie/aio-overpass"}, "provides_extra": ["joblib", "networkx", "shapely"], "release_url": "https://pypi.org/project/aio-overpass/0.15.1/", "requires_dist": ["aiohttp[speedups]<4,>=3.9", "joblib<2,>=1.3; extra == \"joblib\"", "networkx<4,>=3; extra == \"networkx\"", "shapely<3,>=2; extra == \"shapely\""], "requires_python": "<4,>=3.11", "summary": "Async client for the Overpass API", "version": "0.15.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388303, "urls": [{"comment_text": null, "digests": {"blake2b_256": "2e134443797a1e2578d06c040c856b370cb542837aa274daac36535eccc351d3", "md5": "1c9e049edbfa2b2bff52576757321537", "sha256": "3149887ef629251a378e0c9ca113c09b27a82acee2c67ca9cf86ba1e92142c53"}, "downloads": -1, "filename": "aio_overpass-0.15.1-py3-none-any.whl", "has_sig": false, "md5_digest": "1c9e049edbfa2b2bff52576757321537", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<4,>=3.11", "size": 56944, "upload_time": "2025-07-28T18:00:59", "upload_time_iso_8601": "2025-07-28T18:00:59.406086Z", "url": "https://files.pythonhosted.org/packages/2e/13/4443797a1e2578d06c040c856b370cb542837aa274daac36535eccc351d3/aio_overpass-0.15.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "0feae343f7f314e47bad8bea890951da65fc41eb8ac15f95baa398d3881d15bc", "md5": "d22a737b4614fe244132120c4f0bd8a4", "sha256": "835ca2c0b7d00b011c657e30b2bad6e8a85d7e840740ef6f6e9009e57f9c5220"}, "downloads": -1, "filename": "aio_overpass-0.15.1.tar.gz", "has_sig": false, "md5_digest": "d22a737b4614fe244132120c4f0bd8a4", "packagetype": "sdist", "python_version": "source", "requires_python": "<4,>=3.11", "size": 50938, "upload_time": "2025-07-28T18:01:01", "upload_time_iso_8601": "2025-07-28T18:01:01.117420Z", "url": "https://files.pythonhosted.org/packages/0f/ea/e343f7f314e47bad8bea890951da65fc41eb8ac15f95baa398d3881d15bc/aio_overpass-0.15.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 17:59:01 GMT", "package": "kodexa", "version": "7.4.416576510437", "json": {"info": {"author": "Austin Redenbaugh", "author_email": "austin@kodexa.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Topic :: Software Development :: Libraries"], "description": "# Kodexa Python SDK\n\nLearn more about Kodexa at [kodexa.ai](https://kodexa.ai).\n\n## Installation & Setup\n\nThe project uses Poetry for dependency management. To get started:\n\n1. Make sure you have Poetry installed:\n   ```bash\n   curl -sSL https://install.python-poetry.org | python3 -\n   ```\n\n2. Install dependencies:\n   ```bash\n   poetry install\n   ```\n\n3. Run tests to verify your setup:\n   ```bash\n   poetry run pytest\n   ```\n\n## Documentation\n\nComprehensive documentation, including API references, tutorials, and best practices, is available at the [Kodexa Support Portal](https://support.kodexa.ai).\n\nKey documentation sections include:\n- Getting Started Guide\n- API Reference\n- Pipeline Development\n- Model Creation\n- Platform Integration\n- Best Practices\n\n## Examples\n\nCheck out our documentation for practical examples of:\n- Document processing pipelines\n- Custom model development\n- Content extraction and transformation\n- Platform integration patterns\n- Action implementation\n\n## Contributing\n\nWe welcome contributions to the Kodexa platform! Whether it's:\n- Bug fixes\n- Feature enhancements\n- Documentation improvements\n- Example contributions\n\nPlease see our [contributing guide](CONTRIBUTING.md) for details on how to get involved.\n\n## License\n\nThis project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.\n\n## Support\n\n- Visit our [Developer Portal](https://developer.kodexa.ai) for documentation\n- Contact us directly at support@kodexa.com for enterprise support\n\n---\n\nBuilt with \u2764\ufe0f by the Kodexa team", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "kodexa", "package_url": "https://pypi.org/project/kodexa/", "platform": null, "project_url": "https://pypi.org/project/kodexa/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/kodexa/7.4.416576510437/", "requires_dist": ["requests<3.0.0,>=2.28.1", "msgpack<2.0.0,>=1.0.6", "urllib3<3.0.0,>=2.0.0", "ply<4.0,>=3.11", "pyyaml<7.0,>=6.0", "deepdiff<9.0.0,>=8.0.1", "appdirs<2.0.0,>=1.4.4", "python-dateutil<3.0.0,>=2.8.2", "better-exceptions<0.4.0,>=0.3.3", "pyfunctional==1.5.0", "pydantic<3.0.0,>=2.5.3", "pydantic-yaml<2.0.0,>=1.0.0", "semver<4.0.0,>=3.0.1", "chevron<0.15.0,>=0.14.0", "addict<3.0.0,>=2.4.0", "simpleeval<2.0.0,>=1.0.0", "certifi<2026.0.0,>=2025.0.0"], "requires_python": "<3.14,>=3.11", "summary": "Python SDK for the Kodexa Platform", "version": "7.4.416576510437", "yanked": false, "yanked_reason": null}, "last_serial": 30388294, "urls": [{"comment_text": "", "digests": {"blake2b_256": "77f08b46c98139229b19f0b3b9dc7ec60ff1cc47ca4e04087e5e851464d2de37", "md5": "1f9c9b8f1ff895625ca44ed30d15f69a", "sha256": "bc385de60971bfcb41aa160ae184d1a10e165a182f7e5f6e007690742045998c"}, "downloads": -1, "filename": "kodexa-7.4.416576510437-py3-none-any.whl", "has_sig": false, "md5_digest": "1f9c9b8f1ff895625ca44ed30d15f69a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<3.14,>=3.11", "size": 182234, "upload_time": "2025-07-28T17:59:01", "upload_time_iso_8601": "2025-07-28T17:59:01.417203Z", "url": "https://files.pythonhosted.org/packages/77/f0/8b46c98139229b19f0b3b9dc7ec60ff1cc47ca4e04087e5e851464d2de37/kodexa-7.4.416576510437-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "c01a1c36ee92c4a24d6f46269fef3bbb59a206aa28e9864ca47e5073b077a18a", "md5": "8a188357e9121c71a6ffd3e32b4c237a", "sha256": "06622b20684548d11c9e180c69961980ed3b264c239983edebd127654cb43af6"}, "downloads": -1, "filename": "kodexa-7.4.416576510437.tar.gz", "has_sig": false, "md5_digest": "8a188357e9121c71a6ffd3e32b4c237a", "packagetype": "sdist", "python_version": "source", "requires_python": "<3.14,>=3.11", "size": 163748, "upload_time": "2025-07-28T17:59:05", "upload_time_iso_8601": "2025-07-28T17:59:05.316885Z", "url": "https://files.pythonhosted.org/packages/c0/1a/1c36ee92c4a24d6f46269fef3bbb59a206aa28e9864ca47e5073b077a18a/kodexa-7.4.416576510437.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 17:58:34 GMT", "package": "trustauthx", "version": "1.3.0", "json": {"info": {"author": "moonlightnexus", "author_email": "nexus@trustauthx.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.9"], "description": "It is the official Python SDK for TrustAuthx. \n\n# TrustAuthX Python Connector SDK \ud83d\udc0d\n\nTrustAuthX is a revolutionary AI-powered authentication platform that provides secure and seamless login experiences for your users. TrustAuthX uses a unique neuroform technology that analyzes the biometric and behavioral patterns of your users and verifies their identity in real time.\n\nWith TrustAuthX Python Connector SDK, you can easily integrate TrustAuthX authentication into your Python web applications. This SDK contains two components:\n\n- **AuthLite**: A lightweight and simple interface that allows you to use TrustAuthX as a standalone authentication service. AuthLite handles the communication between your app and TrustAuthX servers, and provides you with a user-friendly UI for login and registration.\n- **Standard**: A powerful and flexible interface that allows you to use TrustAuthX as a complementary authentication layer on top of your existing authentication system. Standard gives you full control over the customization and configuration of TrustAuthX authentication, and supports more than 20 popular Python web frameworks.\n\n## Getting Started \ud83d\ude80\n\nTo use TrustAuthX Python Connector SDK, you need to have a TrustAuthX account and credentials. You can create them for free on the [TrustAuthX app website].\n\nOnce you have your credentials, you can install the SDK using pip:\n\n```bash\npip install trustauthx\n```\n\nYou got it. I will add the following section under the Quick Start heading:\n\n## Quick Start With AI \ud83d\ude80\n\nIf you want to experience the magic of TrustAuthX neuroform technology, you can use our CLI command to create a fully functional web app with TrustAuthX authentication in seconds. Just run:\n\n```bash\ntrustauthx neuroform <framework>\n```\n\nwhere `<framework>` is the name of your preferred Python web framework. For example, if you want to use Flask, you can run:\n\n```bash\ntrustauthx neuroform flask\n```\n\nThis will generate a Flask app with TrustAuthX authentication already integrated. You can then run the app and test it out. TrustAuthX neuroform will automatically analyze your framework and implement the best practices for TrustAuthX authentication. You don't need to write any code or configure any settings. TrustAuthX neuroform does it all for you. It's like having an AI assistant that builds your authentication system for you. How cool is that? \ud83d\ude0e\n\nFor more details on how to use the neuroform CLI command, please refer to the [TrustAuthX documentation].\n\n\n## Quick Start With Advance Usage & Customizations.\n\nThen, you can import the SDK in your Python code:\n\n```python\nimport trustauthx\n```\n\nDepending on your use case, you can choose to use either AuthLite or Standard interface. For more details on how to use them, please refer to the [TrustAuthX documentation].\n\n## Examples \ud83d\udcdd\n\nHere are some examples of how to use TrustAuthX Python Connector SDK with different web frameworks:\n\n- Flask\n\n```python\nfrom flask import Flask, request, redirect, session\nfrom trustauthx.authlite import AuthLiteClient\n\n\napp = Flask(__name__)\napp.secret_key = \"your_secret_key\"\n\nauth_lite_client = AuthLiteClient(api_key=\"f28ffe7f2e4a47d6a796b0c2df073aeeAVVQBFSSCXIQWNQIEPBI\", \n                        secret_key=\"8ad9741c8fd5a8f286fc34eba21e0871e63dff3dd67e3ea3a1b43077db9531f7\", \n                        org_id=\"c3621ed40ccc4fca955779fab8f776c921e8865e439211ee88069dc8f7663e88\")\n\ndef get_auth_():\n    access_token = session.get(\"access_token\")\n    refresh_token = session.get(\"refresh_token\")\n    try:\n        a = auth_lite_client.validate_token_set(access_token=access_token, refresh_token=refresh_token)\n        if not a.state:\n            session[\"access_token\"] = a.access\n            session[\"refresh_token\"] = a.refresh\n            t=\"Token Regenerated refresh Token Valid\" \n        else:t=\"Access Token Valid\"\n        return t\n    except Exception as e:\n        return redirect(auth_lite_client.generate_url())\n\n@app.route(\"/\")\ndef root():\n    return redirect(auth_lite_client.generate_url())\n\n@app.route(\"/user\")\ndef get_user():\n    code = request.args.get('code')\n    try:\n        user = auth_lite_client.get_user(code)\n        session[\"access_token\"] = user['access_token']\n        session[\"refresh_token\"] = user['refresh_token']\n        return {\"user\": user}\n    except:\n        return {\"error\": \"Bad Request\"}, 400\n\n@app.route(\"/user-update\")\ndef update_user():\n    try:\n        access_token = session.get(\"access_token\")\n        return redirect(auth_lite_client.generate_edit_user_url(access_token, url =\"http://127.0.0.1:3535/re-auth\"))\n    except:\n        return {\"error\": \"Bad Request\"}, 400\n\n@app.route(\"/re-auth\")\ndef re_auth():\n    code = request.args.get('code')\n    try:\n        user = auth_lite_client.re_auth(code)\n        session[\"access_token\"] = user['access_token']\n        session[\"refresh_token\"] = user['refresh_token']\n        return {\"user\": user}\n    except:\n        return redirect(\"http://127.0.0.1:3535/validate-token\")\n\n@app.route(\"/validate-token\")\ndef validate_access_token():\n    token_validator = get_auth_()\n    return token_validator\n\ndef revoketokens():\n    try:\n        return auth_lite_client.revoke_token(AccessToken=session.get(\"access_token\"), revoke_all_tokens=True)\n    except:\n        return redirect(auth_lite_client.generate_url())\n\ndef revokeAccesstokens():\n    try:\n        return auth_lite_client.revoke_token(AccessToken=session.get(\"access_token\"))\n    except:\n        return redirect(auth_lite_client.generate_url())\n\n@app.route(\"/sign-out\")\ndef invalidate_all_token():\n    r = revoketokens()\n    return redirect(\"http://127.0.0.1:3535/validate-token\")\n\n@app.route(\"/semi-sign-out\")\ndef invalidate_access_token():\n    r = revokeAccesstokens()\n    return \"revoked access token\"\n\nif __name__ == \"__main__\":\n    app.run(port=3535)\n\n```\n\n- Django\n\n```python\nfrom django.shortcuts import render, redirect\nfrom django.http import JsonResponse, HttpResponseBadRequest\nfrom django.contrib.sessions.backends.db import SessionStore\nfrom trustauthx.authlite import AuthLiteClient\n\nauth_lite_client = AuthLiteClient(api_key=\"f28ffe7f2e4a47d6a796b0c2df073aeeAVVQBFSSCXIQWNQIEPBI\", \n                        secret_key=\"8ad9741c8fd5a8f286fc34eba21e0871e63dff3dd67e3ea3a1b43077db9531f7\", \n                        org_id=\"c3621ed40ccc4fca955779fab8f776c921e8865e439211ee88069dc8f7663e88\")\n\ndef get_auth_(request):\n    access_token = request.session.get(\"access_token\")\n    refresh_token = request.session.get(\"refresh_token\")\n    try:\n        a = auth_lite_client.validate_token_set(access_token=access_token, refresh_token=refresh_token)\n        if not a.state:\n            request.session[\"access_token\"] = a.access\n            request.session[\"refresh_token\"] = a.refresh\n            t=\"Token Regenerated refresh Token Valid\" \n        else:t=\"Access Token Valid\"\n        return t\n    except Exception as e:\n        return redirect(auth_lite_client.generate_url())\n\ndef root(request):\n    return redirect(auth_lite_client.generate_url())\n\ndef get_user(request):\n    code = request.GET.get('code')\n    try:\n        user = auth_lite_client.get_user(code)\n        request.session[\"access_token\"] = user['access_token']\n        request.session[\"refresh_token\"] = user['refresh_token']\n        return JsonResponse({\"user\": user})\n    except:\n        return HttpResponseBadRequest()\n\ndef update_user(request):\n    try:\n        access_token = request.session.get(\"access_token\")\n        return redirect(auth_lite_client.generate_edit_user_url(access_token, url =\"http://127.0.0.1:3535/re-auth\"))\n    except:\n        return HttpResponseBadRequest()\n\ndef re_auth(request):\n    code = request.GET.get('code')\n    try:\n        user = auth_lite_client.re_auth(code)\n        request.session[\"access_token\"] = user['access_token']\n        request.session[\"refresh_token\"] = user['refresh_token']\n        return JsonResponse({\"user\": user})\n    except:\n        return redirect(\"http://127.0.0.1:3535/validate-token\")\n\ndef validate_access_token(request):\n    token_validator = get_auth_(request)\n    return JsonResponse(token_validator)\n\ndef revoketokens(request):\n    try:\n        return auth_lite_client.revoke_token(AccessToken=request.session.get(\"access_token\"), revoke_all_tokens=True)\n    except:\n        return redirect(auth_lite_client.generate_url())\n\ndef revokeAccesstokens(request):\n    try:\n        return auth_lite_client.revoke_token(AccessToken=request.session.get(\"access_token\"))\n    except:\n        return redirect(auth_lite_client.generate_url())\n\ndef invalidate_all_token(request):\n    r = revoketokens(request)\n    return r\n\ndef invalidate_access_token(request):\n    r = revokeAccesstokens(request)\n    return r\n```\n\nFor more examples and tutorials, please visit the [TrustAuthX documentation].\n\n## Support \ud83d\udcac\n\nIf you have any questions, feedback, or issues, please feel free to contact us at support@trustauthx.com. We are always happy to hear from you and help you with your integration.\n\n## License \ud83d\udcc4\n\nTrustAuthX Python Connector SDK is licensed under the MIT License. See the [LICENSE] file for more details.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "License", "License-File", "Requires-Dist", "Summary"], "home_page": "https://github.com/One-Click-Auth/TrustAuthx-Py-SDK.git", "keywords": null, "license": "MIT", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "trustauthx", "package_url": "https://pypi.org/project/trustauthx/", "platform": null, "project_url": "https://pypi.org/project/trustauthx/", "project_urls": {"Homepage": "https://github.com/One-Click-Auth/TrustAuthx-Py-SDK.git"}, "provides_extra": null, "release_url": "https://pypi.org/project/trustauthx/1.3.0/", "requires_dist": ["certifi>=2023.5.7", "cffi>=1.15.1", "cryptography>=3.4.10", "ecdsa>=0.18.0", "idna>=3.4", "pyasn1>=0.5.0", "pycparser>=2.21", "requests>=2.31.0", "rsa>=4.9", "six>=1.16.0", "urllib3<=3.0.0", "charset-normalizer>=3.2.0", "python-jose>=3.3.0", "python-dotenv>=1.0.0", "fissix==24.4.24"], "requires_python": null, "summary": "Official connector SDK for TrustAuthx", "version": "1.3.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388289, "urls": [{"comment_text": null, "digests": {"blake2b_256": "f6414bfffa3b2b94a3d565c9571c382537703d764c318ff07694d4baa26e2350", "md5": "415d90761fc3f9775935bf2d91f74e69", "sha256": "448aedcbcf9b9b4538e5e6be5f7bd95dd440f559744ea80c2e6cfb0726b68877"}, "downloads": -1, "filename": "trustauthx-1.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "415d90761fc3f9775935bf2d91f74e69", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 15669, "upload_time": "2025-07-28T17:58:34", "upload_time_iso_8601": "2025-07-28T17:58:34.496445Z", "url": "https://files.pythonhosted.org/packages/f6/41/4bfffa3b2b94a3d565c9571c382537703d764c318ff07694d4baa26e2350/trustauthx-1.3.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "b7b0e44bcbc10c359b7107ffc2a293563bafa9fd88f1792988d2c40c619c1302", "md5": "1f18d58d6da6c53ef8306ea581925f52", "sha256": "12573cc1d09d4cb2a093522f1d1fee26e6dc05ced27e7df43ad9362aa9452fd5"}, "downloads": -1, "filename": "trustauthx-1.3.0.tar.gz", "has_sig": false, "md5_digest": "1f18d58d6da6c53ef8306ea581925f52", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17307, "upload_time": "2025-07-28T17:58:35", "upload_time_iso_8601": "2025-07-28T17:58:35.848650Z", "url": "https://files.pythonhosted.org/packages/b7/b0/e44bcbc10c359b7107ffc2a293563bafa9fd88f1792988d2c40c619c1302/trustauthx-1.3.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 17:58:14 GMT", "package": "gr4vy", "version": "1.1.17", "json": {"info": {"author": "Gr4vy", "author_email": null, "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13"], "description": "# Gr4vy Python SDK\n\nDeveloper-friendly & type-safe Python SDK specifically catered to leverage *Gr4vy* API.\n\n<div align=\"left\">\n    <a href=\"https://pypi.org/project/gr4vy/\"></a><img alt=\"PyPI - Version\" src=\"https://img.shields.io/pypi/v/gr4vy?style=for-the-badge\"></a>\n    <a href=\"https://www.speakeasy.com/?utm_source=gr4vy&utm_campaign=python\">\n        <img src=\"https://custom-icon-badges.demolab.com/badge/-Built%20By%20Speakeasy-212015?style=for-the-badge&logoColor=FBE331&logo=speakeasy&labelColor=545454\" />\n    </a>\n</div>\n\n## Summary\n\nGr4vy Python SDK\n\nThe official Gr4vy SDK for Python provides a convenient way to interact with the Gr4vy API from your server-side application. This SDK allows you to seamlessly integrate Gr4vy's powerful payment orchestration capabilities, including:\n\n* Creating Transactions: Initiate and process payments with various payment methods and services.\n* Managing Buyers: Store and manage buyer information securely.\n* Storing Payment Methods: Securely store and tokenize payment methods for future use.\n* Handling Webhooks: Easily process and respond to webhook events from Gr4vy.\n* And much more: Access the full suite of Gr4vy API payment features.\n\nThis SDK is designed to simplify development, reduce boilerplate code, and help you get up and running with Gr4vy quickly and efficiently. It handles authentication, request signing, and provides easy-to-use methods for most API endpoints.\n\n<!-- No Summary [summary] -->\n\n<!-- Start Table of Contents [toc] -->\n## Table of Contents\n<!-- $toc-max-depth=2 -->\n* [Gr4vy Python SDK](https://github.com/gr4vy/gr4vy-python/blob/master/#gr4vy-python-sdk)\n  * [SDK Installation](https://github.com/gr4vy/gr4vy-python/blob/master/#sdk-installation)\n  * [IDE Support](https://github.com/gr4vy/gr4vy-python/blob/master/#ide-support)\n  * [SDK Example Usage](https://github.com/gr4vy/gr4vy-python/blob/master/#sdk-example-usage)\n  * [Bearer token generation](https://github.com/gr4vy/gr4vy-python/blob/master/#bearer-token-generation)\n  * [Embed token generation](https://github.com/gr4vy/gr4vy-python/blob/master/#embed-token-generation)\n  * [Merchant account ID selection](https://github.com/gr4vy/gr4vy-python/blob/master/#merchant-account-id-selection)\n  * [Webhooks verification](https://github.com/gr4vy/gr4vy-python/blob/master/#webhooks-verification)\n  * [Available Resources and Operations](https://github.com/gr4vy/gr4vy-python/blob/master/#available-resources-and-operations)\n  * [Global Parameters](https://github.com/gr4vy/gr4vy-python/blob/master/#global-parameters)\n  * [Pagination](https://github.com/gr4vy/gr4vy-python/blob/master/#pagination)\n  * [Retries](https://github.com/gr4vy/gr4vy-python/blob/master/#retries)\n  * [Error Handling](https://github.com/gr4vy/gr4vy-python/blob/master/#error-handling)\n  * [Server Selection](https://github.com/gr4vy/gr4vy-python/blob/master/#server-selection)\n  * [Custom HTTP Client](https://github.com/gr4vy/gr4vy-python/blob/master/#custom-http-client)\n  * [Resource Management](https://github.com/gr4vy/gr4vy-python/blob/master/#resource-management)\n  * [Debugging](https://github.com/gr4vy/gr4vy-python/blob/master/#debugging)\n* [Development](https://github.com/gr4vy/gr4vy-python/blob/master/#development)\n  * [Testing](https://github.com/gr4vy/gr4vy-python/blob/master/#testing)\n  * [Contributions](https://github.com/gr4vy/gr4vy-python/blob/master/#contributions)\n\n<!-- End Table of Contents [toc] -->\n\n<!-- Start SDK Installation [installation] -->\n## SDK Installation\n\n> [!NOTE]\n> **Python version upgrade policy**\n>\n> Once a Python version reaches its [official end of life date](https://devguide.python.org/versions/), a 3-month grace period is provided for users to upgrade. Following this grace period, the minimum python version supported in the SDK will be updated.\n\nThe SDK can be installed with either *pip* or *poetry* package managers.\n\n### PIP\n\n*PIP* is the default package installer for Python, enabling easy installation and management of packages from PyPI via the command line.\n\n```bash\npip install gr4vy\n```\n\n### Poetry\n\n*Poetry* is a modern tool that simplifies dependency management and package publishing by using a single `pyproject.toml` file to handle project metadata and dependencies.\n\n```bash\npoetry add gr4vy\n```\n\n### Shell and script usage with `uv`\n\nYou can use this SDK in a Python shell with [uv](https://docs.astral.sh/uv/) and the `uvx` command that comes with it like so:\n\n```shell\nuvx --from gr4vy python\n```\n\nIt's also possible to write a standalone Python script without needing to set up a whole project like so:\n\n```python\n#!/usr/bin/env -S uv run --script\n# /// script\n# requires-python = \">=3.9\"\n# dependencies = [\n#     \"gr4vy\",\n# ]\n# ///\n\nfrom gr4vy import Gr4vy\n\nsdk = Gr4vy(\n  # SDK arguments\n)\n\n# Rest of script here...\n```\n\nOnce that is saved to a file, you can run it with `uv run script.py` where\n`script.py` can be replaced with the actual file name.\n<!-- End SDK Installation [installation] -->\n\n<!-- Start IDE Support [idesupport] -->\n## IDE Support\n\n### PyCharm\n\nGenerally, the SDK will work well with most IDEs out of the box. However, when using PyCharm, you can enjoy much better integration with Pydantic by installing an additional plugin.\n\n- [PyCharm Pydantic Plugin](https://docs.pydantic.dev/latest/integrations/pycharm/)\n<!-- End IDE Support [idesupport] -->\n\n## SDK Example Usage\n\n### Example\n\n```python\n# Synchronous Example\nfrom gr4vy import Gr4vy, auth\nimport os\n\n\nwith Gr4vy(\n    id=\"example\",\n    server=\"production\",\n    merchant_account_id=\"default\",\n    bearer_auth=auth.with_token(open(\"./private_key.pem\").read())\n) as g_client:\n\n    res = g_client.transactions.list()\n\n    assert res is not None\n\n    # Handle response\n    print(res)\n```\n\n</br>\n\nThe same SDK client can also be used to make asychronous requests by importing asyncio.\n```python\n# Asynchronous Example\nimport asyncio\nfrom gr4vy import Gr4vy, auth\nimport os\n\nasync def main():\n\n    async with Gr4vy(\n        id=\"example\",\n        server=\"production\",\n        merchant_account_id=\"default\",\n        bearer_auth=auth.with_token(open(\"./private_key.pem\").read())\n    ) as g_client:\n        res = await g_client.transactions.list()\n\n        assert res is not None\n\n        # Handle response\n        print(res)\n\nasyncio.run(main())\n```\n\n<br /><br />\n> [!IMPORTANT]\n> Please use the `auth.with_token` where the documentation mentions `os.getenv(\"GR4VY_BEARER_AUTH\", \"\"),`.\n\n<!-- No SDK Example Usage [usage] -->\n\n<!-- No Authentication [security] -->\n\n## Bearer token generation\n\nAlternatively, you can create a token for use with the SDK or with your own client library.\n\n```python\nfrom gr4vy import Gr4vy, auth\n\nauth.get_token(open(\"./private_key.pem\").read()\n```\n\n> **Note:** This will only create a token once. Use `auth.with_token` to dynamically generate a token\n> for every request.\n\n\n## Embed token generation\n\nAlternatively, you can create a token for use with Embed as follows.\n\n```python\nfrom gr4vy import Gr4vy, auth\n\nprivate_key = open(\"./private_key.pem\").read()\n\ng_client = Gr4vy(\n    id=\"example\",\n    server=\"production\",\n    merchant_account_id=\"default\",\n    bearer_auth=auth.with_token(private_key)\n)\n\ncheckout_session = g_client.checkout_sessions.create()\n\nauth.get_embed_token(\n    privatekey,\n    embed_params={\n        \"amount\": 1299,\n        \"currency\": 'USD',\n        \"buyer_external_identifier\": 'user-1234',\n    },\n    checkout_session_id=checkout_session.id\n)\n```\n\n> **Note:** This will only create a token once. Use `with_token` to dynamically generate a token\n> for every request.\n\n## Merchant account ID selection\n\nDepending on the key used, you might need to explicitly define a merchant account ID to use. In our API, \nthis uses the `X-GR4VY-MERCHANT-ACCOUNT-ID` header. When using the SDK, you can set the `merchant_account_id`\non every request.\n\n```py\nres = g_client.transactions.list(merchant_account_id: 'merchant-12345')\n```\n\nAlternatively, the merchant account ID can also be set when initializing the SDK.\n\n```py\nwith Gr4vy(\n    id=\"spider\",\n    merchant_account_id=\"merchant-12345\",\n    bearer_auth=auth.get_token(private_key)\n) as g_client:\n    response = g_client.transactions.list()\n```\n\n## Webhooks verification\n\nThe SDK makes it easy to verify that incoming webhooks were actually sent by Gr4vy. Once you have configured the webhook subscription with its corresponding secret, that can be verified the following way:\n\n```py\nfrom gr4vy.webhooks import verify_webhook\n\n# Webhook payload and headers\npayload = 'your-webhook-payload'\nsecret = 'your-webhook-secret'\nsignature_header = 'signatures-from-header'\ntimestamp_header = 'timestamp-from-header'\ntimestamp_tolerance = 300  # optional, in seconds (default: 0)\n\ntry:\n    # Verify the webhook\n    verify_webhook(\n        payload=payload,\n        secret=secret,\n        signature_header=signature_header,\n        timestamp_header=timestamp_header,\n        timestamp_tolerance=timestamp_tolerance\n    )\n    print('Webhook verified successfully!')\nexcept ValueError as error:\n    print(f'Webhook verification failed: {error}')\n```\n\n### Parameters\n\n- **`payload`**: The raw payload string received in the webhook request.\n- **`secret`**: The secret used to sign the webhook. This is provided in your Gr4vy dashboard.\n- **`signatureHeader`**: The `X-Gr4vy-Signature` header from the webhook request.\n- **`timestampHeader`**: The `X-Gr4vy-Timestamp` header from the webhook request.\n- **`timestampTolerance`**: _(Optional)_ The maximum allowed difference (in seconds) between the current time and the timestamp in the webhook. Defaults to `0` (no tolerance).\n\n\n\n<!-- Start Available Resources and Operations [operations] -->\n## Available Resources and Operations\n\n<details open>\n<summary>Available methods</summary>\n\n### [account_updater](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/accountupdater/README.md)\n\n\n#### [account_updater.jobs](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/jobs/README.md)\n\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/jobs/README.md#create) - Create account updater job\n\n### [audit_logs](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/auditlogs/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/auditlogs/README.md#list) - List audit log entries\n\n### [buyers](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyerssdk/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyerssdk/README.md#list) - List all buyers\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyerssdk/README.md#create) - Add a buyer\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyerssdk/README.md#get) - Get a buyer\n* [update](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyerssdk/README.md#update) - Update a buyer\n* [delete](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyerssdk/README.md#delete) - Delete a buyer\n\n#### [buyers.gift_cards](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyersgiftcards/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyersgiftcards/README.md#list) - List gift cards for a buyer\n\n#### [buyers.payment_methods](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyerspaymentmethods/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyerspaymentmethods/README.md#list) - List payment methods for a buyer\n\n#### [buyers.shipping_details](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyersshippingdetails/README.md)\n\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyersshippingdetails/README.md#create) - Add buyer shipping details\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyersshippingdetails/README.md#list) - List a buyer's shipping details\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyersshippingdetails/README.md#get) - Get buyer shipping details\n* [update](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyersshippingdetails/README.md#update) - Update a buyer's shipping details\n* [delete](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/buyersshippingdetails/README.md#delete) - Delete a buyer's shipping details\n\n### [card_scheme_definitions](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/cardschemedefinitionssdk/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/cardschemedefinitionssdk/README.md#list) - List card scheme definitions\n\n### [checkout_sessions](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/checkoutsessions/README.md)\n\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/checkoutsessions/README.md#create) - Create checkout session\n* [update](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/checkoutsessions/README.md#update) - Update checkout session\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/checkoutsessions/README.md#get) - Get checkout session\n* [delete](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/checkoutsessions/README.md#delete) - Delete checkout session\n\n### [digital_wallets](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/digitalwalletssdk/README.md)\n\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/digitalwalletssdk/README.md#create) - Register digital wallet\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/digitalwalletssdk/README.md#list) - List digital wallets\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/digitalwalletssdk/README.md#get) - Get digital wallet\n* [delete](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/digitalwalletssdk/README.md#delete) - Delete digital wallet\n* [update](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/digitalwalletssdk/README.md#update) - Update digital wallet\n\n#### [digital_wallets.domains](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/domains/README.md)\n\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/domains/README.md#create) - Register a digital wallet domain\n* [delete](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/domains/README.md#delete) - Remove a digital wallet domain\n\n#### [digital_wallets.sessions](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/sessions/README.md)\n\n* [google_pay](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/sessions/README.md#google_pay) - Create a Google Pay session\n* [apple_pay](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/sessions/README.md#apple_pay) - Create a Apple Pay session\n* [click_to_pay](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/sessions/README.md#click_to_pay) - Create a Click to Pay session\n\n### [gift_cards](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/giftcardssdk/README.md)\n\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/giftcardssdk/README.md#get) - Get gift card\n* [delete](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/giftcardssdk/README.md#delete) - Delete a gift card\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/giftcardssdk/README.md#create) - Create gift card\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/giftcardssdk/README.md#list) - List gift cards\n\n#### [gift_cards.balances](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/balances/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/balances/README.md#list) - List gift card balances\n\n\n### [merchant_accounts](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/merchantaccountssdk/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/merchantaccountssdk/README.md#list) - List all merchant accounts\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/merchantaccountssdk/README.md#create) - Create a merchant account\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/merchantaccountssdk/README.md#get) - Get a merchant account\n* [update](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/merchantaccountssdk/README.md#update) - Update a merchant account\n\n### [payment_links](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentlinkssdk/README.md)\n\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentlinkssdk/README.md#create) - Add a payment link\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentlinkssdk/README.md#list) - List all payment links\n* [expire](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentlinkssdk/README.md#expire) - Expire a payment link\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentlinkssdk/README.md#get) - Get payment link\n\n### [payment_methods](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodssdk/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodssdk/README.md#list) - List all payment methods\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodssdk/README.md#create) - Create payment method\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodssdk/README.md#get) - Get payment method\n* [delete](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodssdk/README.md#delete) - Delete payment method\n\n#### [payment_methods.network_tokens](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodsnetworktokens/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodsnetworktokens/README.md#list) - List network tokens\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodsnetworktokens/README.md#create) - Provision network token\n* [suspend](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodsnetworktokens/README.md#suspend) - Suspend network token\n* [resume](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodsnetworktokens/README.md#resume) - Resume network token\n* [delete](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodsnetworktokens/README.md#delete) - Delete network token\n\n#### [payment_methods.network_tokens.cryptogram](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/networktokenscryptogram/README.md)\n\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/networktokenscryptogram/README.md#create) - Provision network token cryptogram\n\n#### [payment_methods.payment_service_tokens](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodspaymentservicetokens/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodspaymentservicetokens/README.md#list) - List payment service tokens\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodspaymentservicetokens/README.md#create) - Create payment service token\n* [delete](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentmethodspaymentservicetokens/README.md#delete) - Delete payment service token\n\n### [payment_options](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentoptionssdk/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentoptionssdk/README.md#list) - List payment options\n\n### [payment_service_definitions](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicedefinitionssdk/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicedefinitionssdk/README.md#list) - List payment service definitions\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicedefinitionssdk/README.md#get) - Get a payment service definition\n* [session](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicedefinitionssdk/README.md#session) - Create a session for a payment service definition\n\n### [payment_services](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicessdk/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicessdk/README.md#list) - List payment services\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicessdk/README.md#create) - Update a configured payment service\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicessdk/README.md#get) - Get payment service\n* [update](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicessdk/README.md#update) - Configure a payment service\n* [delete](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicessdk/README.md#delete) - Delete a configured payment service\n* [verify](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicessdk/README.md#verify) - Verify payment service credentials\n* [session](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/paymentservicessdk/README.md#session) - Create a session for a payment service definition\n\n### [payouts](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/payouts/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/payouts/README.md#list) - List payouts created\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/payouts/README.md#create) - Create a payout\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/payouts/README.md#get) - Get a payout\n\n### [refunds](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/refundssdk/README.md)\n\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/refundssdk/README.md#get) - Get refund\n\n### [report_executions](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/reportexecutionssdk/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/reportexecutionssdk/README.md#list) - List executed reports\n\n### [reports](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/reportssdk/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/reportssdk/README.md#list) - List configured reports\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/reportssdk/README.md#create) - Add a report\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/reportssdk/README.md#get) - Get a report\n* [put](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/reportssdk/README.md#put) - Update a report\n\n#### [reports.executions](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/executions/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/executions/README.md#list) - List executions for report\n* [url](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/executions/README.md#url) - Create URL for executed report\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/executions/README.md#get) - Get executed report\n\n### [transactions](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactions/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactions/README.md#list) - List transactions\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactions/README.md#create) - Create transaction\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactions/README.md#get) - Get transaction\n* [update](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactions/README.md#update) - Manually update a transaction\n* [capture](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactions/README.md#capture) - Capture transaction\n* [void](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactions/README.md#void) - Void transaction\n* [sync](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactions/README.md#sync) - Sync transaction\n\n#### [transactions.events](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/events/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/events/README.md#list) - List transaction events\n\n#### [transactions.refunds](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactionsrefunds/README.md)\n\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactionsrefunds/README.md#list) - List transaction refunds\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactionsrefunds/README.md#create) - Create transaction refund\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactionsrefunds/README.md#get) - Get transaction refund\n\n#### [transactions.refunds.all](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/all/README.md)\n\n* [create](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/all/README.md#create) - Create batch transaction refund\n\n#### [transactions.settlements](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactionssettlements/README.md)\n\n* [get](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactionssettlements/README.md#get) - Get transaction settlement\n* [list](https://github.com/gr4vy/gr4vy-python/blob/master/docs/sdks/transactionssettlements/README.md#list) - List transaction settlements\n\n</details>\n<!-- End Available Resources and Operations [operations] -->\n\n<!-- Start Global Parameters [global-parameters] -->\n## Global Parameters\n\nA parameter is configured globally. This parameter may be set on the SDK client instance itself during initialization. When configured as an option during SDK initialization, This global value will be used as the default on the operations that use it. When such operations are called, there is a place in each to override the global value, if needed.\n\nFor example, you can set `merchant_account_id` to `` at SDK initialization and then you do not have to pass the same value on calls to operations like `get`. But if you want to do so you may, which will locally override the global setting. See the example code below for a demonstration.\n\n\n### Available Globals\n\nThe following global parameter is available.\nGlobal parameters can also be set via environment variable.\n\n| Name                | Type | Description                                             | Environment               |\n| ------------------- | ---- | ------------------------------------------------------- | ------------------------- |\n| merchant_account_id | str  | The ID of the merchant account to use for this request. | GR4VY_MERCHANT_ACCOUNT_ID |\n\n### Example\n\n```python\nfrom gr4vy import Gr4vy\nimport os\n\n\nwith Gr4vy(\n    merchant_account_id=\"default\",\n    bearer_auth=os.getenv(\"GR4VY_BEARER_AUTH\", \"\"),\n) as g_client:\n\n    res = g_client.merchant_accounts.get(merchant_account_id=\"merchant-12345\")\n\n    # Handle response\n    print(res)\n\n```\n<!-- End Global Parameters [global-parameters] -->\n\n<!-- Start Pagination [pagination] -->\n## Pagination\n\nSome of the endpoints in this SDK support pagination. To use pagination, you make your SDK calls as usual, but the\nreturned response object will have a `Next` method that can be called to pull down the next group of results. If the\nreturn value of `Next` is `None`, then there are no more pages to be fetched.\n\nHere's an example of one such pagination call:\n```python\nfrom gr4vy import Gr4vy\nimport os\n\n\nwith Gr4vy(\n    merchant_account_id=\"default\",\n    bearer_auth=os.getenv(\"GR4VY_BEARER_AUTH\", \"\"),\n) as g_client:\n\n    res = g_client.buyers.list(cursor=\"ZXhhbXBsZTE\", limit=20, search=\"John\", external_identifier=\"buyer-12345\")\n\n    while res is not None:\n        # Handle items\n\n        res = res.next()\n\n```\n<!-- End Pagination [pagination] -->\n\n<!-- Start Retries [retries] -->\n## Retries\n\nSome of the endpoints in this SDK support retries. If you use the SDK without any configuration, it will fall back to the default retry strategy provided by the API. However, the default retry strategy can be overridden on a per-operation basis, or across the entire SDK.\n\nTo change the default retry strategy for a single API call, simply provide a `RetryConfig` object to the call:\n```python\nfrom gr4vy import Gr4vy\nfrom gr4vy.utils import BackoffStrategy, RetryConfig\nimport os\n\n\nwith Gr4vy(\n    merchant_account_id=\"default\",\n    bearer_auth=os.getenv(\"GR4VY_BEARER_AUTH\", \"\"),\n) as g_client:\n\n    res = g_client.account_updater.jobs.create(payment_method_ids=[\n        \"ef9496d8-53a5-4aad-8ca2-00eb68334389\",\n        \"f29e886e-93cc-4714-b4a3-12b7a718e595\",\n    ],\n        RetryConfig(\"backoff\", BackoffStrategy(1, 50, 1.1, 100), False))\n\n    assert res is not None\n\n    # Handle response\n    print(res)\n\n```\n\nIf you'd like to override the default retry strategy for all operations that support retries, you can use the `retry_config` optional parameter when initializing the SDK:\n```python\nfrom gr4vy import Gr4vy\nfrom gr4vy.utils import BackoffStrategy, RetryConfig\nimport os\n\n\nwith Gr4vy(\n    retry_config=RetryConfig(\"backoff\", BackoffStrategy(1, 50, 1.1, 100), False),\n    merchant_account_id=\"default\",\n    bearer_auth=os.getenv(\"GR4VY_BEARER_AUTH\", \"\"),\n) as g_client:\n\n    res = g_client.account_updater.jobs.create(payment_method_ids=[\n        \"ef9496d8-53a5-4aad-8ca2-00eb68334389\",\n        \"f29e886e-93cc-4714-b4a3-12b7a718e595\",\n    ])\n\n    assert res is not None\n\n    # Handle response\n    print(res)\n\n```\n<!-- End Retries [retries] -->\n\n<!-- Start Error Handling [errors] -->\n## Error Handling\n\n[`Gr4vyError`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/gr4vyerror.py) is the base class for all HTTP error responses. It has the following properties:\n\n| Property           | Type             | Description                                                                             |\n| ------------------ | ---------------- | --------------------------------------------------------------------------------------- |\n| `err.message`      | `str`            | Error message                                                                           |\n| `err.status_code`  | `int`            | HTTP response status code eg `404`                                                      |\n| `err.headers`      | `httpx.Headers`  | HTTP response headers                                                                   |\n| `err.body`         | `str`            | HTTP body. Can be empty string if no body is returned.                                  |\n| `err.raw_response` | `httpx.Response` | Raw HTTP response                                                                       |\n| `err.data`         |                  | Optional. Some errors may contain structured data. [See Error Classes](https://github.com/gr4vy/gr4vy-python/blob/master/#error-classes). |\n\n### Example\n```python\nfrom gr4vy import Gr4vy, errors\nimport os\nfrom typing import Literal\n\n\nwith Gr4vy(\n    merchant_account_id=\"default\",\n    bearer_auth=os.getenv(\"GR4VY_BEARER_AUTH\", \"\"),\n) as g_client:\n    res = None\n    try:\n\n        res = g_client.account_updater.jobs.create(payment_method_ids=[\n            \"ef9496d8-53a5-4aad-8ca2-00eb68334389\",\n            \"f29e886e-93cc-4714-b4a3-12b7a718e595\",\n        ])\n\n        assert res is not None\n\n        # Handle response\n        print(res)\n\n\n    except errors.Gr4vyError as e:\n        # The base class for HTTP error responses\n        print(e.message)\n        print(e.status_code)\n        print(e.body)\n        print(e.headers)\n        print(e.raw_response)\n\n        # Depending on the method different errors may be thrown\n        if isinstance(e, errors.Error400):\n            print(e.data.type)  # Optional[Literal[\"error\"]]\n            print(e.data.code)  # Optional[str]\n            print(e.data.status)  # Optional[int]\n            print(e.data.message)  # Optional[str]\n            print(e.data.details)  # Optional[List[models.ErrorDetail]]\n```\n\n### Error Classes\n**Primary errors:**\n* [`Gr4vyError`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/gr4vyerror.py): The base class for HTTP error responses.\n  * [`Error400`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/error400.py): The request was invalid. Status code `400`.\n  * [`Error401`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/error401.py): The request was unauthorized. Status code `401`.\n  * [`Error403`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/error403.py): The credentials were invalid or the caller did not have permission to act on the resource. Status code `403`.\n  * [`Error404`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/error404.py): The resource was not found. Status code `404`.\n  * [`Error405`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/error405.py): The request method was not allowed. Status code `405`.\n  * [`Error409`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/error409.py): A duplicate record was found. Status code `409`.\n  * [`Error425`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/error425.py): The request was too early. Status code `425`.\n  * [`Error429`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/error429.py): Too many requests were made. Status code `429`.\n  * [`Error500`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/error500.py): The server encountered an error. Status code `500`.\n  * [`Error502`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/error502.py): The server encountered an error. Status code `502`.\n  * [`Error504`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/error504.py): The server encountered an error. Status code `504`.\n  * [`HTTPValidationError`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/httpvalidationerror.py): Validation Error. Status code `422`. *\n\n<details><summary>Less common errors (5)</summary>\n\n<br />\n\n**Network errors:**\n* [`httpx.RequestError`](https://www.python-httpx.org/exceptions/#httpx.RequestError): Base class for request errors.\n    * [`httpx.ConnectError`](https://www.python-httpx.org/exceptions/#httpx.ConnectError): HTTP client was unable to make a request to a server.\n    * [`httpx.TimeoutException`](https://www.python-httpx.org/exceptions/#httpx.TimeoutException): HTTP request timed out.\n\n\n**Inherit from [`Gr4vyError`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/gr4vyerror.py)**:\n* [`ResponseValidationError`](https://github.com/gr4vy/gr4vy-python/blob/master/./src/gr4vy/errors/responsevalidationerror.py): Type mismatch between the response data and the expected Pydantic model. Provides access to the Pydantic validation error via the `cause` attribute.\n\n</details>\n\n\\* Check [the method documentation](https://github.com/gr4vy/gr4vy-python/blob/master/#available-resources-and-operations) to see if the error is applicable.\n<!-- End Error Handling [errors] -->\n\n<!-- Start Server Selection [server] -->\n## Server Selection\n\n### Select Server by Name\n\nYou can override the default server globally by passing a server name to the `server: str` optional parameter when initializing the SDK client instance. The selected server will then be used as the default on the operations that use it. This table lists the names associated with the available servers:\n\n| Name         | Server                               | Variables | Description |\n| ------------ | ------------------------------------ | --------- | ----------- |\n| `sandbox`    | `https://api.sandbox.{id}.gr4vy.app` | `id`      |             |\n| `production` | `https://api.{id}.gr4vy.app`         | `id`      |             |\n\nIf the selected server has variables, you may override its default values through the additional parameters made available in the SDK constructor:\n\n| Variable | Parameter | Default     | Description                            |\n| -------- | --------- | ----------- | -------------------------------------- |\n| `id`     | `id: str` | `\"example\"` | The subdomain for your Gr4vy instance. |\n\n#### Example\n\n```python\nfrom gr4vy import Gr4vy\nimport os\n\n\nwith Gr4vy(\n    server=\"production\",\n    id=\"<id>\"\n    merchant_account_id=\"default\",\n    bearer_auth=os.getenv(\"GR4VY_BEARER_AUTH\", \"\"),\n) as g_client:\n\n    res = g_client.account_updater.jobs.create(payment_method_ids=[\n        \"ef9496d8-53a5-4aad-8ca2-00eb68334389\",\n        \"f29e886e-93cc-4714-b4a3-12b7a718e595\",\n    ])\n\n    assert res is not None\n\n    # Handle response\n    print(res)\n\n```\n\n### Override Server URL Per-Client\n\nThe default server can also be overridden globally by passing a URL to the `server_url: str` optional parameter when initializing the SDK client instance. For example:\n```python\nfrom gr4vy import Gr4vy\nimport os\n\n\nwith Gr4vy(\n    server_url=\"https://api.sandbox.example.gr4vy.app\",\n    merchant_account_id=\"default\",\n    bearer_auth=os.getenv(\"GR4VY_BEARER_AUTH\", \"\"),\n) as g_client:\n\n    res = g_client.account_updater.jobs.create(payment_method_ids=[\n        \"ef9496d8-53a5-4aad-8ca2-00eb68334389\",\n        \"f29e886e-93cc-4714-b4a3-12b7a718e595\",\n    ])\n\n    assert res is not None\n\n    # Handle response\n    print(res)\n\n```\n<!-- End Server Selection [server] -->\n\n<!-- Start Custom HTTP Client [http-client] -->\n## Custom HTTP Client\n\nThe Python SDK makes API calls using the [httpx](https://www.python-httpx.org/) HTTP library.  In order to provide a convenient way to configure timeouts, cookies, proxies, custom headers, and other low-level configuration, you can initialize the SDK client with your own HTTP client instance.\nDepending on whether you are using the sync or async version of the SDK, you can pass an instance of `HttpClient` or `AsyncHttpClient` respectively, which are Protocol's ensuring that the client has the necessary methods to make API calls.\nThis allows you to wrap the client with your own custom logic, such as adding custom headers, logging, or error handling, or you can just pass an instance of `httpx.Client` or `httpx.AsyncClient` directly.\n\nFor example, you could specify a header for every request that this sdk makes as follows:\n```python\nfrom gr4vy import Gr4vy\nimport httpx\n\nhttp_client = httpx.Client(headers={\"x-custom-header\": \"someValue\"})\ns = Gr4vy(client=http_client)\n```\n\nor you could wrap the client with your own custom logic:\n```python\nfrom gr4vy import Gr4vy\nfrom gr4vy.httpclient import AsyncHttpClient\nimport httpx\n\nclass CustomClient(AsyncHttpClient):\n    client: AsyncHttpClient\n\n    def __init__(self, client: AsyncHttpClient):\n        self.client = client\n\n    async def send(\n        self,\n        request: httpx.Request,\n        *,\n        stream: bool = False,\n        auth: Union[\n            httpx._types.AuthTypes, httpx._client.UseClientDefault, None\n        ] = httpx.USE_CLIENT_DEFAULT,\n        follow_redirects: Union[\n            bool, httpx._client.UseClientDefault\n        ] = httpx.USE_CLIENT_DEFAULT,\n    ) -> httpx.Response:\n        request.headers[\"Client-Level-Header\"] = \"added by client\"\n\n        return await self.client.send(\n            request, stream=stream, auth=auth, follow_redirects=follow_redirects\n        )\n\n    def build_request(\n        self,\n        method: str,\n        url: httpx._types.URLTypes,\n        *,\n        content: Optional[httpx._types.RequestContent] = None,\n        data: Optional[httpx._types.RequestData] = None,\n        files: Optional[httpx._types.RequestFiles] = None,\n        json: Optional[Any] = None,\n        params: Optional[httpx._types.QueryParamTypes] = None,\n        headers: Optional[httpx._types.HeaderTypes] = None,\n        cookies: Optional[httpx._types.CookieTypes] = None,\n        timeout: Union[\n            httpx._types.TimeoutTypes, httpx._client.UseClientDefault\n        ] = httpx.USE_CLIENT_DEFAULT,\n        extensions: Optional[httpx._types.RequestExtensions] = None,\n    ) -> httpx.Request:\n        return self.client.build_request(\n            method,\n            url,\n            content=content,\n            data=data,\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            timeout=timeout,\n            extensions=extensions,\n        )\n\ns = Gr4vy(async_client=CustomClient(httpx.AsyncClient()))\n```\n<!-- End Custom HTTP Client [http-client] -->\n\n<!-- Start Resource Management [resource-management] -->\n## Resource Management\n\nThe `Gr4vy` class implements the context manager protocol and registers a finalizer function to close the underlying sync and async HTTPX clients it uses under the hood. This will close HTTP connections, release memory and free up other resources held by the SDK. In short-lived Python programs and notebooks that make a few SDK method calls, resource management may not be a concern. However, in longer-lived programs, it is beneficial to create a single SDK instance via a [context manager][context-manager] and reuse it across the application.\n\n[context-manager]: https://docs.python.org/3/reference/datamodel.html#context-managers\n\n```python\nfrom gr4vy import Gr4vy\nimport os\ndef main():\n\n    with Gr4vy(\n        merchant_account_id=\"default\",\n        bearer_auth=os.getenv(\"GR4VY_BEARER_AUTH\", \"\"),\n    ) as g_client:\n        # Rest of application here...\n\n\n# Or when using async:\nasync def amain():\n\n    async with Gr4vy(\n        merchant_account_id=\"default\",\n        bearer_auth=os.getenv(\"GR4VY_BEARER_AUTH\", \"\"),\n    ) as g_client:\n        # Rest of application here...\n```\n<!-- End Resource Management [resource-management] -->\n\n<!-- Start Debugging [debug] -->\n## Debugging\n\nYou can setup your SDK to emit debug logs for SDK requests and responses.\n\nYou can pass your own logger class directly into your SDK.\n```python\nfrom gr4vy import Gr4vy\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\ns = Gr4vy(debug_logger=logging.getLogger(\"gr4vy\"))\n```\n\nYou can also enable a default debug logger by setting an environment variable `GR4VY_DEBUG` to true.\n<!-- End Debugging [debug] -->\n\n<!-- Placeholder for Future Speakeasy SDK Sections -->\n\n# Development\n\n## Testing\n\nTo run the tests, install Python and Poetry, ensure to download the `private_key.pem` for the test environment, and run the following.\n\n```sh\npoetry install\npoetry run pytest\n```\n\n## Contributions\n\nWhile we value open-source contributions to this SDK, this library is generated programmatically. Any manual changes added to internal files will be overwritten on the next generation. \nWe look forward to hearing your feedback. Feel free to open a PR or an issue with a proof of concept and we'll do our best to include it in a future release. \n\n### SDK Created by [Speakeasy](https://www.speakeasy.com/?utm_source=gr4vy&utm_campaign=python)\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": "https://github.com/gr4vy/gr4vy-python.git", "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "gr4vy", "package_url": "https://pypi.org/project/gr4vy/", "platform": null, "project_url": "https://pypi.org/project/gr4vy/", "project_urls": {"Homepage": "https://github.com/gr4vy/gr4vy-python.git", "Repository": "https://github.com/gr4vy/gr4vy-python.git"}, "provides_extra": null, "release_url": "https://pypi.org/project/gr4vy/1.1.17/", "requires_dist": ["PyJWT<3.0.0,>=2.10.1", "cryptography<44.0.0,>=43.0.0", "httpcore>=1.0.9", "httpx>=0.28.1", "jsonpath-python>=1.0.6", "pydantic>=2.11.2", "python-jose<4.0.0,>=3.4.0"], "requires_python": ">=3.9.2", "summary": "Python Client SDK Generated by Speakeasy.", "version": "1.1.17", "yanked": false, "yanked_reason": null}, "last_serial": 30388281, "urls": [{"comment_text": "", "digests": {"blake2b_256": "f0039bf243e1ee3208cd859c8b19df2c2b30263e8f83c215c778c9a4d0759514", "md5": "79945c75d88d0447f44d5aa8f52bf5a4", "sha256": "3c7d06fb86bde52fd205ecff25e8bb87998af4138c33672d7517fc7b8caabe1d"}, "downloads": -1, "filename": "gr4vy-1.1.17-py3-none-any.whl", "has_sig": false, "md5_digest": "79945c75d88d0447f44d5aa8f52bf5a4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9.2", "size": 468782, "upload_time": "2025-07-28T17:58:14", "upload_time_iso_8601": "2025-07-28T17:58:14.490399Z", "url": "https://files.pythonhosted.org/packages/f0/03/9bf243e1ee3208cd859c8b19df2c2b30263e8f83c215c778c9a4d0759514/gr4vy-1.1.17-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "7d0cba9e59e6d0a269b4baba627eae7186c839eb093a4cc9e456778d345dd991", "md5": "21a5e29187f2a9f1b9b8552a99c74cec", "sha256": "bb4b960bea3f1ed33e9e182b31c52e07049b519248c558eb3731f9966449497f"}, "downloads": -1, "filename": "gr4vy-1.1.17.tar.gz", "has_sig": false, "md5_digest": "21a5e29187f2a9f1b9b8552a99c74cec", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9.2", "size": 190975, "upload_time": "2025-07-28T17:58:15", "upload_time_iso_8601": "2025-07-28T17:58:15.960536Z", "url": "https://files.pythonhosted.org/packages/7d/0c/ba9e59e6d0a269b4baba627eae7186c839eb093a4cc9e456778d345dd991/gr4vy-1.1.17.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 17:58:12 GMT", "package": "browser-core", "version": "2.5.0", "json": {"info": {"author": "gabigolo", "author_email": null, "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Software Development :: Testing"], "description": "# Browser-Core\n\n[![Vers\u00e3o PyPI](https://badge.fury.io/py/browser-core.svg)](https://badge.fury.io/py/browser-core)\n[![Licen\u00e7a: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n**Browser-Core** \u00e9 uma plataforma completa para orquestra\u00e7\u00e3o de navegadores. O projeto nasceu para simplificar\nautoma\u00e7\u00f5es em larga escala, garantindo isolamento total dos ambientes e reprodutibilidade dos estados de cada sess\u00e3o.\n\nNeste documento voc\u00ea encontrar\u00e1 uma vis\u00e3o detalhada de como funciona o framework, dicas de utiliza\u00e7\u00e3o e todos os\nrecursos dispon\u00edveis.\n\n## Sum\u00e1rio\n\n1. [Introdu\u00e7\u00e3o](#introdu\u00e7\u00e3o)\n2. [Instala\u00e7\u00e3o](#instala\u00e7\u00e3o)\n3. [Conceitos Fundamentais](#conceitos-fundamentais)\n4. [Fluxo de Trabalho](#fluxo-de-trabalho)\n5. [Exemplos de Uso](#exemplos-de-uso)\n6. [Comandos da CLI](#comandos-da-cli)\n7. [Dicas Avan\u00e7adas](#dicas-avan\u00e7adas)\n8. [Contribuindo](#contribuindo)\n9. [Licen\u00e7a](#licen\u00e7a)\n\n---\n\n## Introdu\u00e7\u00e3o\n\nAutomatizar tarefas de navegador exige controle refinado sobre perfis, vers\u00f5es de drivers e paralelismo. O *\n*Browser-Core** abstrai essa complexidade oferecendo:\n\n- Camadas de snapshots reutiliz\u00e1veis para capturar o estado exato do navegador (cookies, localStorage, extens\u00f5es, etc.).\n- Workers isolados que partem desses snapshots e executam tarefas independentes em paralelo.\n- Integra\u00e7\u00e3o transparente com Selenium e Playwright, bastando escolher o motor desejado.\n- Uma CLI poderosa para manipular snapshots e gerenciar o armazenamento local.\n\nCom esses componentes \u00e9 poss\u00edvel escalar automa\u00e7\u00f5es para centenas de execu\u00e7\u00f5es mantendo total rastreabilidade.\n\n---\n\n## Instala\u00e7\u00e3o\n\nA instala\u00e7\u00e3o mais simples \u00e9 via [PyPI](https://pypi.org/project/browser-core/):\n\n```bash\npip install browser-core\n```\n\nIsso disponibiliza a biblioteca para uso em scripts Python e tamb\u00e9m instala a ferramenta de linha de comando\n`browser-core`.\n\n---\n\n## Conceitos Fundamentais\n\n**Snapshots em Camadas**\n: Permitem criar \"imagens\" do navegador e derivar novos estados a partir delas. Assim voc\u00ea registra um login ou\nconfigura\u00e7\u00e3o apenas uma vez e reutiliza em milhares de execu\u00e7\u00f5es.\n\n**Workers Isolados**\n: Cada worker \u00e9 iniciado a partir de um snapshot espec\u00edfico e executa a tarefa em um perfil totalmente separado dos\ndemais.\n\n**Drivers Gerenciados**\n: O projeto baixa e armazena a vers\u00e3o exata do WebDriver para o navegador escolhido, evitando incompatibilidades em\ndiferentes m\u00e1quinas.\n\n**Arquitetura Multi\u2011engine**\n: Tanto Selenium quanto Playwright podem ser utilizados com a mesma API de alto n\u00edvel.\n\n**CLI Integrada**\n: Inclui comandos para criar snapshots base, listar estados existentes, inspecionar metadados e limpar todos os\nartefatos.\n\n---\n\n## Fluxo de Trabalho\n\nO uso t\u00edpico divide-se em duas fases: cria\u00e7\u00e3o dos snapshots e execu\u00e7\u00e3o das tarefas.\n\n### 1. Preparar os Snapshots\n\n1. **Criar o Snapshot Base** \u2013 Perfil limpo com a vers\u00e3o de navegador desejada:\n\n   ```bash\n   browser-core snapshots create-base chrome-base\n   ```\n\n2. **Derivar um Snapshot com Estado** \u2013 Por exemplo, realizar login em um site e salvar esse estado:\n\n   ```python\n   # scripts/create_login_snapshot.py\n   import os\n   from browser_core import Orchestrator, Worker, create_selector\n   from browser_core.types import SelectorType\n   \n   APP_USER = os.getenv(\"APP_USER\")\n   APP_PASSWORD = os.getenv(\"APP_PASSWORD\")\n\n   def perform_login(worker: Worker):\n       \"\"\"Fun\u00e7\u00e3o que executa a l\u00f3gica de login.\"\"\"\n\n       worker.navigate_to(\"https://app.exemplo.com/login\")\n       worker.get(create_selector(\"input[name='email']\", SelectorType.CSS)).send_keys(APP_USER)\n       worker.get(create_selector(\"input[name='password']\", SelectorType.CSS)).send_keys(APP_PASSWORD)\n       worker.get(create_selector(\"button[type='submit']\", SelectorType.CSS)).click()\n       worker.get(create_selector(\"#dashboard\", SelectorType.CSS)) # Aguarda o carregamento\n\n   def main():\n       \"\"\"Fun\u00e7\u00e3o principal para orquestrar a cria\u00e7\u00e3o do snapshot.\"\"\"\n       Orchestrator().create_snapshot_from_task(\n           base_snapshot_id=\"chrome-base\",\n           new_snapshot_id=\"app_logged_in\",\n           setup_function=perform_login,\n           metadata={\"description\": \"Sess\u00e3o autenticada\"}\n       )\n       print(\"Snapshot 'app_logged_in' criado com sucesso!\")\n\n   if __name__ == \"__main__\":\n       main()\n   ```\n\n### 2. Executar Tarefas em Paralelo\n\nCom o snapshot `app_logged_in` pronto, processe uma s\u00e9rie de itens utilizando v\u00e1rios workers:\n\n```python\n# scripts/run_tasks.py\nfrom browser_core import Orchestrator, Worker, create_selector, default_settings\nfrom browser_core.types import SelectorType\n\n\ndef fetch_report(worker: Worker, report_id: str):\n    \"\"\"Fun\u00e7\u00e3o que cada worker executar\u00e1 para buscar um relat\u00f3rio.\"\"\"\n    worker.navigate_to(f\"https://app.exemplo.com/reports/{report_id}\")\n    table = worker.get(create_selector(\"#report-data-table\", SelectorType.CSS)).text\n    return {\"report_id\": report_id, \"length\": len(table)}\n\ndef main():\n    \"\"\"Fun\u00e7\u00e3o principal para executar as tarefas em paralelo.\"\"\"\n    REPORTS = [\"Q1-2024\", \"Q2-2024\", \"Q3-2024\", \"Q4-2024\"]\n    settings = default_settings()\n    settings[\"browser\"][\"headless\"] = True\n\n    results = Orchestrator(settings).run_tasks_in_squad(\n        squad_size=2,\n        base_snapshot_id=\"app_logged_in\",\n        task_items=REPORTS,\n        worker_setup_function=lambda w: True,  # Fun\u00e7\u00e3o de setup simples\n        item_processing_function=fetch_report,\n    )\n    print(results)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n---\n\n## Exemplos de Uso\n\nO exemplo acima demonstra a execu\u00e7\u00e3o de tarefas em paralelo utilizando snapshots para reaproveitar o estado de login.\n\n---\n\n## Comandos da CLI\n\nA ferramenta `browser-core` auxilia na cria\u00e7\u00e3o e manuten\u00e7\u00e3o dos snapshots e do armazenamento:\n\n- **Criar snapshot base**\n    ```bash\n    browser-core snapshots create-base <snapshot-id>\n    ```\n\n- **Listar snapshots existentes**\n    ```bash\n    browser-core snapshots list\n    ```\n\n- **Inspecionar um snapshot**\n    ```bash\n    browser-core snapshots inspect <snapshot-id>\n    ```\n- **Criar snapshot a partir de uma tarefa**\n    ```bash\n    browser-core snapshots create-from-task --base <id-base> --new <id-novo> \\\n        --setup-script path/setup.py --setup-function func\n    ```\n- **Depurar um snapshot**\n    ```bash\n    browser-core snapshots debug <snapshot-id>\n    ```\n- **Executar tarefas em esquadr\u00e3o**\n    ```bash\n    browser-core run --snapshot <id> --tasks-file dados.csv \\\n        --worker-script worker.py --worker-function processa\n    ```\n\n- **Limpar armazenamento**\n    ```bash\n    browser-core storage clean --force\n    ```\n\nTodas as op\u00e7\u00f5es est\u00e3o dispon\u00edveis com `browser-core --help`.\n\n---\n\n## Dicas Avan\u00e7adas\n\n- **Modo headless ou gr\u00e1fico**: Defina `settings[\"browser\"][\"headless\"]` para alternar entre execu\u00e7\u00e3o invis\u00edvel ou com\n  janela aberta.\n- **Uso de proxies e vari\u00e1veis de ambiente**: \u00c9 poss\u00edvel configurar proxies ou outras op\u00e7\u00f5es de driver diretamente nas\n  defini\u00e7\u00f5es de `Settings`.\n- **Persist\u00eancia de logs**: Cada execu\u00e7\u00e3o cria uma pasta com registros detalhados em `tasks_logs_dir`, auxiliando\n  depura\u00e7\u00e3o e auditoria.\n- **API de Elementos mais rica**: `ElementProxy` agora possui m\u00e9todos como `hover()`, `scroll_to_view()` e\n  utilidades de espera (`wait_for_visible`, `wait_for_clickable`). Tamb\u00e9m \u00e9 poss\u00edvel obter m\u00faltiplos elementos com\n  `worker.get_all()` e coletar seus textos com `get_texts()`.\n- **Pr\u00e9-aquecimento do WebDriver**: O Orchestrator garante que o driver necess\u00e1rio seja baixado uma \u00fanica vez antes\n  de iniciar os workers, evitando conflitos em execu\u00e7\u00f5es paralelas.\n- **Extensibilidade**: A estrutura de `Worker` e `Orchestrator` permite implementar tarefas complexas com facilidade,\n  reutilizando fun\u00e7\u00f5es comuns de manipula\u00e7\u00e3o de p\u00e1gina.\n\n---\n\n## Contribuindo\n\nContribui\u00e7\u00f5es s\u00e3o bem-vindas! Para configurar o ambiente de desenvolvimento:\n\n1. Clone o reposit\u00f3rio:\n   ```bash\n   git clone https://github.com/gabrielbarbosel/browser-core.git\n   cd browser-core\n   ```\n\n2. Crie um ambiente virtual:\n   ```bash\n   python -m venv .venv\n   source .venv/bin/activate\n   ```\n\n3. Instale as depend\u00eancias de desenvolvimento:\n   ```bash\n   pip install -e \".[dev]\"\n   ```\n\n4. Execute as verifica\u00e7\u00f5es locais:\n   ```bash\n   black -q src\n   pytest -q\n   ```\n\n---\n\n## Licen\u00e7a\n\nDistribu\u00eddo sob a licen\u00e7a MIT. Consulte o arquivo [LICENSE](LICENSE) para mais detalhes.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": "automation, selenium, web, browser, framework", "license": "MIT", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "browser-core", "package_url": "https://pypi.org/project/browser-core/", "platform": null, "project_url": "https://pypi.org/project/browser-core/", "project_urls": {"Bug Tracker": "https://github.com/gabrielbarbosel/browser-core/issues", "Homepage": "https://github.com/gabrielbarbosel/browser-core", "Repository": "https://github.com/gabrielbarbosel/browser-core"}, "provides_extra": null, "release_url": "https://pypi.org/project/browser-core/2.5.0/", "requires_dist": ["selenium>=4.15.0", "webdriver-manager>=4.0.1", "click>=8.0.0", "typing-extensions>=4.0.0", "playwright>=1.30.0"], "requires_python": ">=3.8", "summary": "Um framework robusto e configur\u00e1vel para automa\u00e7\u00e3o de navegadores, com gest\u00e3o de perfis, sess\u00f5es e uma CLI.", "version": "2.5.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388278, "urls": [{"comment_text": null, "digests": {"blake2b_256": "42bb1ed38edc6384e3cb10067e349f97833671d176c5fd087d9bea02228e5734", "md5": "c4402e0b3479fecbf9ce07cf5989058d", "sha256": "f177bc5136e031b2ec7e855346b415354a33b68fc75f091ff5874752727925f6"}, "downloads": -1, "filename": "browser_core-2.5.0-py3-none-any.whl", "has_sig": false, "md5_digest": "c4402e0b3479fecbf9ce07cf5989058d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 54775, "upload_time": "2025-07-28T17:58:12", "upload_time_iso_8601": "2025-07-28T17:58:12.414879Z", "url": "https://files.pythonhosted.org/packages/42/bb/1ed38edc6384e3cb10067e349f97833671d176c5fd087d9bea02228e5734/browser_core-2.5.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "666549dc7de472475d9f9a4d74124ed4e798467109bec7f40a5704f76c5997fe", "md5": "d23fe6470c6fd4f603933c25b21c2bf1", "sha256": "7e1d622c132ebb94bc8650066b94f7c3b95830400cd0394e8c2d3e6154c40c6f"}, "downloads": -1, "filename": "browser_core-2.5.0.tar.gz", "has_sig": false, "md5_digest": "d23fe6470c6fd4f603933c25b21c2bf1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 45704, "upload_time": "2025-07-28T17:58:13", "upload_time_iso_8601": "2025-07-28T17:58:13.721986Z", "url": "https://files.pythonhosted.org/packages/66/65/49dc7de472475d9f9a4d74124ed4e798467109bec7f40a5704f76c5997fe/browser_core-2.5.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 17:56:53 GMT", "package": "methodwebscan", "version": "0.0.133", "json": {"info": {"author": null, "author_email": null, "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Operating System :: MacOS", "Operating System :: Microsoft :: Windows", "Operating System :: OS Independent", "Operating System :: POSIX", "Operating System :: POSIX :: Linux", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Software Development :: Libraries :: Python Modules", "Typing :: Typed"], "description": "", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "methodwebscan", "package_url": "https://pypi.org/project/methodwebscan/", "platform": null, "project_url": "https://pypi.org/project/methodwebscan/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/methodwebscan/0.0.133/", "requires_dist": ["pydantic>=1.9.2", "pydantic-core<3.0.0,>=2.18.2"], "requires_python": "<4.0,>=3.8", "summary": null, "version": "0.0.133", "yanked": false, "yanked_reason": null}, "last_serial": 30388274, "urls": [{"comment_text": "", "digests": {"blake2b_256": "023fa8282456eb47f26f0e64acece61738d43fbdc9c67b6823b585efd384ee24", "md5": "efbc899a432f3edafa4aad2f7b69bd3a", "sha256": "01aa5412f2054a6aea7f14f68e45896004719fb59a6d0b28f8c74aab84f7233c"}, "downloads": -1, "filename": "methodwebscan-0.0.133-py3-none-any.whl", "has_sig": false, "md5_digest": "efbc899a432f3edafa4aad2f7b69bd3a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<4.0,>=3.8", "size": 127309, "upload_time": "2025-07-28T17:56:53", "upload_time_iso_8601": "2025-07-28T17:56:53.103108Z", "url": "https://files.pythonhosted.org/packages/02/3f/a8282456eb47f26f0e64acece61738d43fbdc9c67b6823b585efd384ee24/methodwebscan-0.0.133-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "f28f80fa02f57a2b86dd880b68002976173d4bf9952666f8000d32bfca6b9edc", "md5": "a1f2b43059d93dcad4477240f3fbbbf1", "sha256": "a129a92c708bb6732baf27010d9bc219fe925a78ed0cb65558f93ef7db8d9fd4"}, "downloads": -1, "filename": "methodwebscan-0.0.133.tar.gz", "has_sig": false, "md5_digest": "a1f2b43059d93dcad4477240f3fbbbf1", "packagetype": "sdist", "python_version": "source", "requires_python": "<4.0,>=3.8", "size": 26357, "upload_time": "2025-07-28T17:56:54", "upload_time_iso_8601": "2025-07-28T17:56:54.427210Z", "url": "https://files.pythonhosted.org/packages/f2/8f/80fa02f57a2b86dd880b68002976173d4bf9952666f8000d32bfca6b9edc/methodwebscan-0.0.133.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 17:56:50 GMT", "package": "data-analysis-framework", "version": "1.0.0", "json": {"info": {"author": "Wes Jackson", "author_email": "AI Building Blocks <wjackson@redhat.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Database", "Topic :: Office/Business", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# Data Analysis Framework\n\n## \ud83d\udcc8 Purpose\n\nSpecialized framework for analyzing structured data files with AI-powered pattern detection and insights.\n\n## \ud83d\udce6 Supported Formats\n\n### Spreadsheets & Tables\n- **Excel**: XLSX, XLS with multiple sheets\n- **CSV/TSV**: Delimiter detection and parsing\n- **Apache Parquet**: Columnar data analysis\n- **JSON**: Nested and flat structure analysis\n- **JSONL**: Line-delimited JSON streams\n\n### Configuration Data\n- **YAML**: Configuration files and data serialization\n- **TOML**: Configuration file analysis\n- **INI**: Legacy configuration parsing\n- **Environment Files**: .env variable analysis\n\n### Database Exports\n- **SQL Dumps**: Schema and data analysis\n- **SQLite**: Database file inspection\n- **Database Connection**: Live data analysis\n\n## \ud83e\udd16 AI Integration Features\n\n- **Schema Detection**: Automatic column type inference\n- **Pattern Analysis**: Anomaly and trend detection\n- **Data Quality Assessment**: Missing values, duplicates, outliers\n- **Relationship Discovery**: Cross-table dependencies\n- **Business Logic Extraction**: Rules and constraints\n- **Predictive Insights**: Forecasting and recommendations\n\n## \ud83d\ude80 Quick Start\n\n```python\nfrom data_analysis_framework import DataAnalyzer\n\nanalyzer = DataAnalyzer()\nresult = analyzer.analyze(\"sales_data.xlsx\")\n\nprint(f\"Data Type: {result.document_type.type_name}\")\nprint(f\"Schema: {result.analysis.schema_info}\")\nprint(f\"Quality Score: {result.analysis.quality_metrics['overall_score']}\")\nprint(f\"AI Insights: {result.analysis.ai_insights}\")\n```\n\n## \ud83c\udfd7\ufe0f Status\n\n**\ud83d\udea7 Planned** - Architecture designed, implementation pending\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Home-Page", "License-File", "Requires-Python"], "home_page": "https://github.com/rdwj/data-analysis-framework", "keywords": "data-analysis, ai, ml, structured-data, database, excel, csv, json, semantic-search, business-intelligence", "license": "MIT", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "data-analysis-framework", "package_url": "https://pypi.org/project/data-analysis-framework/", "platform": null, "project_url": "https://pypi.org/project/data-analysis-framework/", "project_urls": {"Documentation": "https://github.com/rdwj/data-analysis-framework/blob/main/README.md", "Homepage": "https://github.com/rdwj/data-analysis-framework", "Issues": "https://github.com/rdwj/data-analysis-framework/issues", "Repository": "https://github.com/rdwj/data-analysis-framework"}, "provides_extra": ["dev", "docs", "database", "advanced", "visualization"], "release_url": "https://pypi.org/project/data-analysis-framework/1.0.0/", "requires_dist": ["pandas>=1.5.0", "numpy>=1.21.0", "openpyxl>=3.0.0", "pyarrow>=8.0.0", "sqlalchemy>=1.4.0", "pyyaml>=6.0", "toml>=0.10.2", "pytest>=6.0; extra == \"dev\"", "black>=21.0; extra == \"dev\"", "flake8>=3.8; extra == \"dev\"", "mypy>=0.800; extra == \"dev\"", "sphinx>=3.0; extra == \"docs\"", "sphinx_rtd_theme>=0.5; extra == \"docs\"", "psycopg2-binary>=2.9.0; extra == \"database\"", "pymongo>=4.0.0; extra == \"database\"", "scikit-learn>=1.1.0; extra == \"advanced\"", "scipy>=1.8.0; extra == \"advanced\"", "matplotlib>=3.5.0; extra == \"visualization\"", "seaborn>=0.11.0; extra == \"visualization\""], "requires_python": ">=3.8", "summary": "AI-powered analysis framework for structured data files and databases", "version": "1.0.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388269, "urls": [{"comment_text": null, "digests": {"blake2b_256": "1c3f6f1aab429c2128e41a48215e3a851346b7e2194495914c15c6c7c1f13f34", "md5": "37009e2a8ef4f22c2c65053d7807c152", "sha256": "784004698231d663023a37df20adc2e113eda7586b697236dba3293805dca852"}, "downloads": -1, "filename": "data_analysis_framework-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "37009e2a8ef4f22c2c65053d7807c152", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.8", "size": 25873, "upload_time": "2025-07-28T17:56:50", "upload_time_iso_8601": "2025-07-28T17:56:50.400069Z", "url": "https://files.pythonhosted.org/packages/1c/3f/6f1aab429c2128e41a48215e3a851346b7e2194495914c15c6c7c1f13f34/data_analysis_framework-1.0.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "30bc5b676ac5be96a680cbff297effc7b04a231183515c737c030bb10ecdca09", "md5": "87276ad3f90d48184f3dbfe3b0ecd4d5", "sha256": "0c3b49b472cf5e8ea2d9fe6d2ffd1c71209319ea408ca281f8c5375825d8605c"}, "downloads": -1, "filename": "data_analysis_framework-1.0.0.tar.gz", "has_sig": false, "md5_digest": "87276ad3f90d48184f3dbfe3b0ecd4d5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.8", "size": 221144, "upload_time": "2025-07-28T17:56:52", "upload_time_iso_8601": "2025-07-28T17:56:52.021626Z", "url": "https://files.pythonhosted.org/packages/30/bc/5b676ac5be96a680cbff297effc7b04a231183515c737c030bb10ecdca09/data_analysis_framework-1.0.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 17:56:25 GMT", "package": "functions-framework", "version": "3.9.2", "json": {"info": {"author": "Google LLC", "author_email": "Google LLC <googleapis-packages@google.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "Programming Language :: Python :: 3.10", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9"], "description": "# Functions Framework for Python\n\n[![PyPI version](https://badge.fury.io/py/functions-framework.svg)](https://badge.fury.io/py/functions-framework)\n\n[![Python unit CI][ff_python_unit_img]][ff_python_unit_link] [![Python lint CI][ff_python_lint_img]][ff_python_lint_link] [![Python conformace CI][ff_python_conformance_img]][ff_python_conformance_link] ![Security Scorecard](https://api.securityscorecards.dev/projects/github.com/GoogleCloudPlatform/functions-framework-python/badge)\n\nAn open source FaaS (Function as a service) framework for writing portable\nPython functions -- brought to you by the Google Cloud Functions team.\n\nThe Functions Framework lets you write lightweight functions that run in many\ndifferent environments, including:\n\n*   [Google Cloud Run Functions](https://cloud.google.com/functions/)\n*   Your local development machine\n*   [Knative](https://github.com/knative/)-based environments\n\nThe framework allows you to go from:\n\n```python\ndef hello(request):\n    return \"Hello world!\"\n```\n\nTo:\n\n```sh\ncurl http://my-url\n# Output: Hello world!\n```\n\nAll without needing to worry about writing an HTTP server or complicated request handling logic.\n\n## Features\n\n*   Spin up a local development server for quick testing\n*   Invoke a function in response to a request\n*   Automatically unmarshal events conforming to the [CloudEvents](https://cloudevents.io/) spec\n*   Portable between serverless platforms\n\n## Installation\n\nInstall the Functions Framework via `pip`:\n\n```sh\npip install functions-framework\n```\n\nOr, for deployment, add the Functions Framework to your `requirements.txt` file:\n\n```\nfunctions-framework==3.*\n```\n\n## Quickstarts\n\n### Quickstart: HTTP Function (Hello World)\n\nCreate an `main.py` file with the following contents:\n\n```python\nimport flask\nimport functions_framework\n\n@functions_framework.http\ndef hello(request: flask.Request) -> flask.typing.ResponseReturnValue:\n    return \"Hello world!\"\n```\n\n> Your function is passed a single parameter, `(request)`, which is a Flask [`Request`](https://flask.palletsprojects.com/en/3.0.x/api/#flask.Request) object.\n\nRun the following command:\n\n```sh\nfunctions-framework --target hello --debug\n * Serving Flask app \"hello\" (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: on\n * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n```\n\n(You can also use `functions-framework-python` if you have multiple\nlanguage frameworks installed).\n\nOpen http://localhost:8080/ in your browser and see *Hello world!*.\n\nOr send requests to this function using `curl` from another terminal window:\n\n```sh\ncurl localhost:8080\n# Output: Hello world!\n```\n\n### Quickstart: CloudEvent Function\n\nCreate an `main.py` file with the following contents:\n\n```python\nimport functions_framework\nfrom cloudevents.http.event import CloudEvent\n\n@functions_framework.cloud_event\ndef hello_cloud_event(cloud_event: CloudEvent) -> None:\n   print(f\"Received event with ID: {cloud_event['id']} and data {cloud_event.data}\")\n```\n\n> Your function is passed a single [CloudEvent](https://github.com/cloudevents/sdk-python/blob/main/cloudevents/sdk/event/v1.py) parameter.\n\nRun the following command to run `hello_cloud_event` target locally:\n\n```sh\nfunctions-framework --target=hello_cloud_event\n```\n\nIn a different terminal, `curl` the Functions Framework server:\n\n```sh\ncurl -X POST localhost:8080 \\\n   -H \"Content-Type: application/cloudevents+json\" \\\n   -d '{\n\t\"specversion\" : \"1.0\",\n\t\"type\" : \"example.com.cloud.event\",\n\t\"source\" : \"https://example.com/cloudevents/pull\",\n\t\"subject\" : \"123\",\n\t\"id\" : \"A234-1234-1234\",\n\t\"time\" : \"2018-04-05T17:31:00Z\",\n\t\"data\" : \"hello world\"\n}'\n```\n\nOutput from the terminal running `functions-framework`:\n```\nReceived event with ID: A234-1234-1234 and data hello world\n``` \n\nMore info on sending [CloudEvents](http://cloudevents.io) payloads, see [`examples/cloud_run_cloud_events`](examples/cloud_run_cloud_events/) instruction.\n\n\n### Quickstart: Error handling\n\nThe framework includes an error handler that is similar to the\n[`flask.Flask.errorhandler`](https://flask.palletsprojects.com/en/1.1.x/api/#flask.Flask.errorhandler)\nfunction, which allows you to handle specific error types with a decorator:\n\n```python\nimport functions_framework\n\n\n@functions_framework.errorhandler(ZeroDivisionError)\ndef handle_zero_division(e):\n    return \"I'm a teapot\", 418\n\n\ndef function(request):\n    1 / 0\n    return \"Success\", 200\n```\n\nThis function will catch the `ZeroDivisionError` and return a different\nresponse instead.\n\n### Quickstart: Pub/Sub emulator\n1. Create a `main.py` file with the following contents:\n\n   ```python\n   def hello(event, context):\n        print(\"Received\", context.event_id)\n   ```\n\n1. Start the Functions Framework on port 8080:\n\n   ```sh\n   functions-framework --target=hello --signature-type=event --debug --port=8080\n   ```\n\n1. In a second terminal, start the Pub/Sub emulator on port 8085.\n\n   ```sh\n   export PUBSUB_PROJECT_ID=my-project\n   gcloud beta emulators pubsub start \\\n       --project=$PUBSUB_PROJECT_ID \\\n       --host-port=localhost:8085\n   ```\n\n   You should see the following after the Pub/Sub emulator has started successfully:\n\n   ```none\n   [pubsub] INFO: Server started, listening on 8085\n   ```\n\n1. In a third terminal, create a Pub/Sub topic and attach a push subscription to the topic, using `http://localhost:8080` as its push endpoint. [Publish](https://cloud.google.com/pubsub/docs/quickstart-client-libraries#publish_messages) some messages to the topic. Observe your function getting triggered by the Pub/Sub messages.\n\n   ```sh\n   export PUBSUB_PROJECT_ID=my-project\n   export TOPIC_ID=my-topic\n   export PUSH_SUBSCRIPTION_ID=my-subscription\n   $(gcloud beta emulators pubsub env-init)\n\n   git clone https://github.com/googleapis/python-pubsub.git\n   cd python-pubsub/samples/snippets/\n   pip install -r requirements.txt\n\n   python publisher.py $PUBSUB_PROJECT_ID create $TOPIC_ID\n   python subscriber.py $PUBSUB_PROJECT_ID create-push $TOPIC_ID $PUSH_SUBSCRIPTION_ID http://localhost:8080\n   python publisher.py $PUBSUB_PROJECT_ID publish $TOPIC_ID\n   ```\n\n   You should see the following after the commands have run successfully:\n\n   ```none\n   Created topic: projects/my-project/topics/my-topic\n\n   topic: \"projects/my-project/topics/my-topic\"\n   push_config {\n     push_endpoint: \"http://localhost:8080\"\n   }\n   ack_deadline_seconds: 10\n   message_retention_duration {\n     seconds: 604800\n   }\n   .\n   Endpoint for subscription is: http://localhost:8080\n\n   1\n   2\n   3\n   4\n   5\n   6\n   7\n   8\n   9\n   Published messages to projects/my-project/topics/my-topic.\n   ```\n\n   And in the terminal where the Functions Framework is running:\n\n   ```none\n    * Serving Flask app \"hello\" (lazy loading)\n    * Environment: production\n      WARNING: This is a development server. Do not use it in a production deployment.\n      Use a production WSGI server instead.\n    * Debug mode: on\n    * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n    * Restarting with fsevents reloader\n    * Debugger is active!\n    * Debugger PIN: 911-794-046\n   Received 1\n   127.0.0.1 - - [11/Aug/2021 14:42:22] \"POST / HTTP/1.1\" 200 -\n   Received 2\n   127.0.0.1 - - [11/Aug/2021 14:42:22] \"POST / HTTP/1.1\" 200 -\n   Received 5\n   127.0.0.1 - - [11/Aug/2021 14:42:22] \"POST / HTTP/1.1\" 200 -\n   Received 6\n   127.0.0.1 - - [11/Aug/2021 14:42:22] \"POST / HTTP/1.1\" 200 -\n   Received 7\n   127.0.0.1 - - [11/Aug/2021 14:42:22] \"POST / HTTP/1.1\" 200 -\n   Received 8\n   127.0.0.1 - - [11/Aug/2021 14:42:22] \"POST / HTTP/1.1\" 200 -\n   Received 9\n   127.0.0.1 - - [11/Aug/2021 14:42:39] \"POST / HTTP/1.1\" 200 -\n   Received 3\n   127.0.0.1 - - [11/Aug/2021 14:42:39] \"POST / HTTP/1.1\" 200 -\n   Received 4\n   127.0.0.1 - - [11/Aug/2021 14:42:39] \"POST / HTTP/1.1\" 200 -\n   ```\n\nFor more details on extracting data from a Pub/Sub event, see\nhttps://cloud.google.com/functions/docs/tutorials/pubsub#functions_helloworld_pubsub_tutorial-python\n\n### Quickstart: Build a Deployable Container\n\n1. Install [Docker](https://store.docker.com/search?type=edition&offering=community) and the [`pack` tool](https://buildpacks.io/docs/install-pack/).\n\n1. Build a container from your function using the Functions [buildpacks](https://github.com/GoogleCloudPlatform/buildpacks):\n\n        pack build \\\n            --builder gcr.io/buildpacks/builder:v1 \\\n            --env GOOGLE_FUNCTION_SIGNATURE_TYPE=http \\\n            --env GOOGLE_FUNCTION_TARGET=hello \\\n            my-first-function\n\n1. Start the built container:\n\n        docker run --rm -p 8080:8080 my-first-function\n        # Output: Serving function...\n\n1. Send requests to this function using `curl` from another terminal window:\n\n        curl localhost:8080\n        # Output: Hello World!\n\n## Run your function on serverless platforms\n\n### Google Cloud Run functions\n\nThis Functions Framework is based on the [Python Runtime on Google Cloud Functions](https://cloud.google.com/functions/docs/concepts/python-runtime).\n\nOn Cloud Functions, using the Functions Framework is not necessary: you don't need to add it to your `requirements.txt` file.\n\nAfter you've written your function, you can simply deploy it from your local machine using the `gcloud` command-line tool. [Check out the Cloud Functions quickstart](https://cloud.google.com/functions/docs/quickstart).\n\n### Container environments based on Knative\n\nCloud Run and Cloud Run on GKE both implement the [Knative Serving API](https://www.knative.dev/docs/). The Functions Framework is designed to be compatible with Knative environments. Just build and deploy your container to a Knative environment.\n\n## Configure the Functions Framework\n\nYou can configure the Functions Framework using command-line flags or environment variables. If you specify both, the environment variable will be ignored.\n\n| Command-line flag  | Environment variable      | Description                                                                                                                                                                                      |\n| ------------------ | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `--host`           | `HOST`                    | The host on which the Functions Framework listens for requests. Default: `0.0.0.0`                                                                                                               |\n| `--port`           | `PORT`                    | The port on which the Functions Framework listens for requests. Default: `8080`                                                                                                                  |\n| `--target`         | `FUNCTION_TARGET`         | The name of the exported function to be invoked in response to requests. Default: `function`                                                                                                     |\n| `--signature-type` | `FUNCTION_SIGNATURE_TYPE` | The signature used when writing your function. Controls unmarshalling rules and determines which arguments are used to invoke your function. Default: `http`; accepted values: `http`, `event` or `cloudevent` |\n| `--source`         | `FUNCTION_SOURCE`         | The path to the file containing your function. Default: `main.py` (in the current working directory)                                                                                             |\n| `--debug`          | `DEBUG`                   | A flag that allows to run functions-framework to run in debug mode, including live reloading. Default: `False`                                                                                   |\n\n## Enable Google Cloud Run function Events\n\nThe Functions Framework can unmarshall incoming\nGoogle Cloud Run functions [event](https://cloud.google.com/functions/docs/concepts/events-triggers#events) payloads to `event` and `context` objects.\nThese will be passed as arguments to your function when it receives a request.\nNote that your function must use the `event`-style function signature:\n\n```python\ndef hello(event, context):\n    print(event)\n    print(context)\n```\n\nTo enable automatic unmarshalling, set the function signature type to `event`\n using the `--signature-type` command-line flag or the `FUNCTION_SIGNATURE_TYPE` environment variable. By default, the HTTP\nsignature will be used and automatic event unmarshalling will be disabled.\n\nFor more details on this signature type, see the Google Cloud Functions\ndocumentation on\n[background functions](https://cloud.google.com/functions/docs/writing/background#cloud_pubsub_example).\n\nSee the [running example](examples/cloud_run_event).\n\n## Advanced Examples\n\nMore advanced guides can be found in the [`examples/`](examples/) directory.\nYou can also find examples on using the CloudEvent Python SDK [here](https://github.com/cloudevents/sdk-python).\n\n## Contributing\n\nContributions to this library are welcome and encouraged. See [CONTRIBUTING](CONTRIBUTING.md) for more information on how to get started.\n\n[ff_python_unit_img]: https://github.com/GoogleCloudPlatform/functions-framework-python/workflows/Python%20Unit%20CI/badge.svg\n[ff_python_unit_link]: https://github.com/GoogleCloudPlatform/functions-framework-python/actions?query=workflow%3A\"Python+Unit+CI\"\n[ff_python_lint_img]: https://github.com/GoogleCloudPlatform/functions-framework-python/workflows/Python%20Lint%20CI/badge.svg\n[ff_python_lint_link]: https://github.com/GoogleCloudPlatform/functions-framework-python/actions?query=workflow%3A\"Python+Lint+CI\"\n[ff_python_conformance_img]: https://github.com/GoogleCloudPlatform/functions-framework-python/workflows/Python%20Conformance%20CI/badge.svg\n[ff_python_conformance_link]: https://github.com/GoogleCloudPlatform/functions-framework-python/actions?query=workflow%3A\"Python+Conformance+CI\"\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Home-Page", "License-File", "Requires-Python"], "home_page": "https://github.com/googlecloudplatform/functions-framework-python", "keywords": "functions-framework", "license": "Apache-2.0", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": "Google LLC <googleapis-packages@google.com>", "name": "functions-framework", "package_url": "https://pypi.org/project/functions-framework/", "platform": null, "project_url": "https://pypi.org/project/functions-framework/", "project_urls": {"Homepage": "https://github.com/googlecloudplatform/functions-framework-python"}, "provides_extra": null, "release_url": "https://pypi.org/project/functions-framework/3.9.2/", "requires_dist": ["flask<4.0,>=2.0", "click<9.0,>=7.0", "watchdog>=1.0.0", "gunicorn>=22.0.0; platform_system != \"Windows\"", "cloudevents<=1.11.0,>=1.2.0", "Werkzeug<4.0.0,>=0.14", "starlette<1.0.0,>=0.37.0; python_version >= \"3.8\"", "uvicorn<1.0.0,>=0.18.0; python_version >= \"3.8\"", "uvicorn-worker<1.0.0,>=0.2.0; python_version >= \"3.8\""], "requires_python": "<4,>=3.5", "summary": "An open source FaaS (Function as a service) framework for writing portable Python functions -- brought to you by the Google Cloud Functions team.", "version": "3.9.2", "yanked": false, "yanked_reason": null}, "last_serial": 30388263, "urls": [{"comment_text": null, "digests": {"blake2b_256": "0e21c35bf131447a0da1319e7a8c90807564697b7c0c262695a15cc3d1d7668b", "md5": "07923f2dd19aa872339630102358e404", "sha256": "c3b26dff88168a7405f9ee56b2fe9c2b2cbea5c9fe4d3cacbef85089a1e22c93"}, "downloads": -1, "filename": "functions_framework-3.9.2-py3-none-any.whl", "has_sig": false, "md5_digest": "07923f2dd19aa872339630102358e404", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<4,>=3.5", "size": 41389, "upload_time": "2025-07-28T17:56:25", "upload_time_iso_8601": "2025-07-28T17:56:25.211224Z", "url": "https://files.pythonhosted.org/packages/0e/21/c35bf131447a0da1319e7a8c90807564697b7c0c262695a15cc3d1d7668b/functions_framework-3.9.2-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "dc8ae5d305b3a1d2838d9c33c2aeb7ab6f84124a1bc929af1b1ddd7a6d27822b", "md5": "7641e287a9db452dd5a6405af125e40c", "sha256": "4c9a25392b74f37fff45c679f48eb70849bfa59b7c191188794ffd0d46e7bc41"}, "downloads": -1, "filename": "functions_framework-3.9.2.tar.gz", "has_sig": false, "md5_digest": "7641e287a9db452dd5a6405af125e40c", "packagetype": "sdist", "python_version": "source", "requires_python": "<4,>=3.5", "size": 54078, "upload_time": "2025-07-28T17:56:26", "upload_time_iso_8601": "2025-07-28T17:56:26.174311Z", "url": "https://files.pythonhosted.org/packages/dc/8a/e5d305b3a1d2838d9c33c2aeb7ab6f84124a1bc929af1b1ddd7a6d27822b/functions_framework-3.9.2.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 17:56:09 GMT", "package": "parse-qwantz", "version": "2025.7.1", "json": {"info": {"author": null, "author_email": "Jan Szejko <jan.szejko@gmail.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Intended Audience :: End Users/Desktop", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 3.10", "Topic :: Multimedia :: Graphics :: Graphics Conversion", "Topic :: Scientific/Engineering :: Image Recognition", "Typing :: Typed"], "description": "# Dinosaur Comic Parser\n\nA transcript generator for [Ryan North](https://www.ryannorth.ca/)'s [Dinosaur Comics](https://qwantz.com)\n\n## Installation\n\nInstall `parse-qwantz` with `pip`\n\n```bash\n  pip install parse-qwantz\n```\n\n## Usage\n\nYou need to download the image file for the comic you want transcribed, for example https://qwantz.com/comics/comic2-02.png. Then run `parse-qwantz`:\n\n```\n$ parse-qwantz comic2-02.png\nT-Rex: Today is a beautiful day to be stomping on things! As a dinosaur, stomping is the best part of my day indeed!\n\nT-Rex: *gasp*\n\nT-Rex: What's that, little house? You wish you were back in your own time? THAT IS TOO BAD FOR YOU\n\nT-Rex: Perhaps you too will get a stomping, little girl!\nUtahraptor: WAIT!\n\nUtahraptor: Is stomping really the answer to your problem(s)?\nT-Rex: Problem(s)?\n\nT-Rex: My only problem(s) have to do with you interrupting my stomping!\nT-Rex: \u301asmall\u301b crazy utahraptor!\n```\n\nYou can also call it with\n```bash\npython -m parse_qwantz\n```\n\nThe argument can also be a directory path instead of a file path. In such case the program will run on all files in the specified directory.\n\n## Options\n\n### `--output-dir`\n\nBy default, the program outputs to stdout and logs to stderr. With this option, when processing file `image_name.png` it will output to `OUTPUT_DIR/image_name.png.txt` and log to `OUTPUT_DIR/image_name.log`.\n\n### `--generate-svg`\n\nInstead of transcribing the comic, generate a vectorized version in the SVG format and print it to the standard output.\n\n### `--parse-footer`\n\nInstead of transcribing the comic, transcribe just the footer.\n\n## Conventions\n\nBold and italics are marked with \"\u25d6\u25d7\" and \"\u25b9\u25c3\" respectively. This is to avoid ambiguity which may result from using characters like \"*\" or \"_\".\n\nAll descriptions are in \"\u301a\u301b\" brackets. Each line that isn't just description starts from a \"character\" name followed by a colon. That \"character\" might be one of the actual characters, but also \"Narrator\", \"Off panel\", \"Banner\", \"Book cover\" etc.\n\nWhen some text in a panel is obscured but can be reconstructed, it's in \"\u2983\u2984\" braces. So far this applies only to 2 comics: #59 and #61.\n\nWhen some text in a panel is obscured and not reconstructed, it's replaced either by the special \"\u2026\" character, or a description of how it's obscured in \"\u301a\u301b\" brackets.\n\n## Notes\n\nThis program still does not work on all DC strips, but at this point it should work correctly on pretty much all \"standard\" strips and some less-standard ones (thanks to the system of overrides). Eventually all existing strips should work, including the guest comics, with updates for new comics coming out regularly.\n\nAfter all comics are working, I might add some other features, like generating SVG images.\n\n## Running Tests\n\nTo run tests, run the following command:\n\n```bash\n  pytest test/\n```\n\n## Acknowledgments\n\nThis program would not be possible without the wonderful comics by Ryan North! Thanks, Ryan, and congratulations on the 20th anniversary of your comics! Btw [the anniversary comic](https://qwantz.com/?comic=4005) will totally not work with this script, haha! (at least until I add an override)\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "parse-qwantz", "package_url": "https://pypi.org/project/parse-qwantz/", "platform": null, "project_url": "https://pypi.org/project/parse-qwantz/", "project_urls": {"homepage": "https://github.com/janek37/parse_qwantz"}, "provides_extra": ["dev"], "release_url": "https://pypi.org/project/parse-qwantz/2025.7.1/", "requires_dist": ["Pillow", "typer", "pytest; extra == \"dev\""], "requires_python": ">=3.10", "summary": "Transcript generator for Dinosaur Comics", "version": "2025.7.1", "yanked": false, "yanked_reason": null}, "last_serial": 30388259, "urls": [{"comment_text": "", "digests": {"blake2b_256": "748be44503085f9421609d58d14fc080fdfe75538d9df6758eb3711f9b60b93a", "md5": "15ebaa35dac8c1bbd7d1fc63e64ee83c", "sha256": "850dc7893f90e250043d4c32f424b47cf88ddece6938ecffc86cd0c73e5f20ea"}, "downloads": -1, "filename": "parse_qwantz-2025.7.1-py3-none-any.whl", "has_sig": false, "md5_digest": "15ebaa35dac8c1bbd7d1fc63e64ee83c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.10", "size": 508165, "upload_time": "2025-07-28T17:56:09", "upload_time_iso_8601": "2025-07-28T17:56:09.784496Z", "url": "https://files.pythonhosted.org/packages/74/8b/e44503085f9421609d58d14fc080fdfe75538d9df6758eb3711f9b60b93a/parse_qwantz-2025.7.1-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "7a3833c8782117ab9cf031d1efbbd1b8c2693ca700ef4aeeaef088268e41b2b4", "md5": "52b1bb4fb3cc1fcaa255b4cccead350e", "sha256": "997ef9e289e9429feba985cd106d7e503a03eecca076b262a92d1754febeacfa"}, "downloads": -1, "filename": "parse_qwantz-2025.7.1.tar.gz", "has_sig": false, "md5_digest": "52b1bb4fb3cc1fcaa255b4cccead350e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.10", "size": 495636, "upload_time": "2025-07-28T17:56:12", "upload_time_iso_8601": "2025-07-28T17:56:12.189117Z", "url": "https://files.pythonhosted.org/packages/7a/38/33c8782117ab9cf031d1efbbd1b8c2693ca700ef4aeeaef088268e41b2b4/parse_qwantz-2025.7.1.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:04Z", "published": "Mon, 28 Jul 2025 17:55:59 GMT", "package": "GameSentenceMiner", "version": "2.13.0", "json": {"info": {"author": null, "author_email": "Beangate <bpwhelan95@gmail.com>", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": null, "license": "MIT License", "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "GameSentenceMiner", "package_url": "https://pypi.org/project/GameSentenceMiner/", "platform": null, "project_url": "https://pypi.org/project/GameSentenceMiner/", "project_urls": {"Homepage": "https://github.com/bpwhelan/GameSentenceMiner", "Repository": "https://github.com/bpwhelan/GameSentenceMiner"}, "provides_extra": null, "release_url": "https://pypi.org/project/GameSentenceMiner/2.13.0/", "requires_dist": ["requests~=2.32.3", "watchdog~=5.0.2", "DateTime~=5.5", "pyperclip~=1.9.0", "soundfile~=0.12.1", "toml~=0.10.2", "psutil~=6.0.0", "rapidfuzz~=3.9.7", "plyer~=2.1.0", "keyboard~=0.13.5", "websockets~=15.0.1", "openai-whisper", "stable-ts-whisperless", "silero-vad~=5.1.2", "ttkbootstrap~=1.10.1", "dataclasses_json~=0.6.7", "win10toast; sys_platform == \"win32\"", "numpy==2.2.6", "pystray", "pywin32; sys_platform == \"win32\"", "pygetwindow; sys_platform == \"win32\"", "flask", "groq", "obsws-python~=1.7.2", "matplotlib", "sounddevice", "google-genai"], "requires_python": ">=3.10", "summary": "A tool for mining sentences from games. Update: Overlay?", "version": "2.13.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388256, "urls": [{"comment_text": null, "digests": {"blake2b_256": "5e318d9d249075a0b1d5c3f17a9a4e54dc7d47054b28e97b8d8a871c204e0549", "md5": "d541220549341595edf2441615edf113", "sha256": "1a8c73101dfd72013d4e6059bb3c8e11c950ba5ce8566a2a67b8e4170bbbaa3b"}, "downloads": -1, "filename": "gamesentenceminer-2.13.0-py3-none-any.whl", "has_sig": false, "md5_digest": "d541220549341595edf2441615edf113", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.10", "size": 3198743, "upload_time": "2025-07-28T17:55:59", "upload_time_iso_8601": "2025-07-28T17:55:59.823595Z", "url": "https://files.pythonhosted.org/packages/5e/31/8d9d249075a0b1d5c3f17a9a4e54dc7d47054b28e97b8d8a871c204e0549/gamesentenceminer-2.13.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "ff3b7be2a8f1aa2d88662c7aca4e596f9be15c8e34615fa731c5de2f9f240a51", "md5": "005a06f6ac6a3afacff571455f11ea04", "sha256": "e57aa90e3e6abc37f31fa36ac8335893245650116f2ae60822d0d05aae547e66"}, "downloads": -1, "filename": "gamesentenceminer-2.13.0.tar.gz", "has_sig": false, "md5_digest": "005a06f6ac6a3afacff571455f11ea04", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.10", "size": 3184848, "upload_time": "2025-07-28T17:56:01", "upload_time_iso_8601": "2025-07-28T17:56:01.883688Z", "url": "https://files.pythonhosted.org/packages/ff/3b/7be2a8f1aa2d88662c7aca4e596f9be15c8e34615fa731c5de2f9f240a51/gamesentenceminer-2.13.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:05Z", "published": "Mon, 28 Jul 2025 17:55:50 GMT", "package": "eval-protocol", "version": "0.1.4", "json": {"info": {"author": null, "author_email": "Fireworks AI <info@fireworks.ai>", "bugtrack_url": null, "classifiers": ["Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Eval Protocol\n\n**Eval Protocol: Author, reproduce, and evaluate reward functions seamlessly on Fireworks, TRL, and your own infrastructure.**\n\n## Key Features\n\n*   **Easy-to-use Decorator**: Define reward functions with a simple `@reward_function` decorator.\n*   **Local Testing**: Quickly test your reward functions with sample data.\n*   **Flexible Evaluation**: Evaluate model outputs based on single or multiple custom metrics.\n*   **Seamless Deployment**: Deploy your reward functions to platforms like Fireworks AI.\n*   **Comprehensive CLI**: Manage reward functions, preview evaluations (`eval-protocol preview`), deploy (`eval-protocol deploy`), and run complex evaluation pipelines (`eval-protocol run`).\n*   **Simplified Dataset Integration**: Direct integration with HuggingFace datasets and on-the-fly format conversion.\n*   **Extensible**: Designed to be adaptable for various LLM evaluation scenarios.\n\n## Installation\n\n```bash\npip install eval-protocol\n```\n\n### Optional TRL Extras\n\nInstall the additional dependencies required for running the TRL-based training\nexamples:\n\n```bash\npip install \"eval-protocol[trl]\"\n```\n\n## Getting Started\n\nEval Protocol simplifies the creation and deployment of reward functions for evaluating AI model outputs.\n\n### 1. Creating a Reward Function for Tool Calling\n\nEval Protocol allows you to define custom logic to evaluate model responses. Here's an example of how you might use the built-in `exact_tool_match_reward` for evaluating tool/function calls. This reward function checks if the model's generated tool calls exactly match the expected ones.\n\n```python\n# This is a conceptual example of how exact_tool_match_reward is defined and used.\n# You would typically import it from eval_protocol.rewards.function_calling.\n# For actual usage, you configure it in your YAML files for `eval-protocol run`.\n\nfrom eval_protocol import reward_function\nfrom eval_protocol.models import EvaluateResult, Message, MetricResult\nfrom typing import List, Dict, Any, Optional, Union\n\n# Definition of exact_tool_match_reward (simplified for brevity, see source for full details)\n# from eval_protocol.rewards.function_calling import exact_tool_match_reward, eval_tool_call\n\n@reward_function\ndef exact_tool_match_reward(\n    messages: Union[List[Message], List[Dict[str, Any]]],\n    ground_truth: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -> EvaluateResult:\n    if not messages:\n        return EvaluateResult(\n            score=0.0, reason=\"No messages provided for evaluation.\", metrics={}\n        )\n\n    generation_message_obj = messages[-1]\n    generation_dict: Dict[str, Any]\n\n    if isinstance(generation_message_obj, Message):\n        generation_dict = {\n            \"role\": generation_message_obj.role,\n            \"content\": generation_message_obj.content,\n        }\n        if generation_message_obj.tool_calls:\n            generation_dict[\"tool_calls\"] = [\n                tc.model_dump() if hasattr(tc, \"model_dump\") else tc\n                for tc in generation_message_obj.tool_calls\n            ]\n    elif isinstance(generation_message_obj, dict):\n        generation_dict = generation_message_obj\n    else:\n        # Handle error for unexpected type\n        return EvaluateResult(score=0.0, reason=\"Unexpected generation message type.\", metrics={})\n\n    if ground_truth is None:\n        # Handle missing ground truth (e.g., score 0 if generation has tool calls, 1 if not)\n        # This logic is simplified here.\n        has_gen_tc = bool(generation_dict.get(\"tool_calls\") or \"<tool_call>\" in generation_dict.get(\"content\", \"\"))\n        score = 0.0 if has_gen_tc else 1.0\n        return EvaluateResult(score=score, reason=\"Ground truth not provided.\", metrics={})\n\n    # Ensure ground_truth is a dict (it might be a JSON string from some datasets)\n    if isinstance(ground_truth, str):\n        try:\n            ground_truth = json.loads(ground_truth)\n        except json.JSONDecodeError:\n            return EvaluateResult(score=0.0, reason=\"Ground truth string failed to parse.\", metrics={})\n\n    if not isinstance(ground_truth, dict):\n         return EvaluateResult(score=0.0, reason=\"Ground truth is not a dictionary.\", metrics={})\n\n    # This simplified check compares generated tool calls with the expected ones.\n    expected_tcs = ground_truth.get(\"tool_calls\", [])\n    generated_tcs = generation_dict.get(\"tool_calls\", [])\n\n    # This is a highly simplified check. The actual function is much more robust.\n    is_match = (len(expected_tcs) == len(generated_tcs)) # Placeholder\n    score = 1.0 if is_match else 0.0\n\n    reason = f\"Exact tool match evaluation score: {score}\"\n    return EvaluateResult(score=score, reason=reason, metrics={\n        \"tool_call_match\": MetricResult(score=score, success=is_match, reason=reason)\n    })\n\n```\nThis example illustrates the structure. The actual `exact_tool_match_reward` in `eval_protocol.rewards.function_calling` handles complex parsing and comparison of tool calls.\n\n### 2. Testing Your Reward Function with a Dataset\n\nEffective testing of a reward function involves evaluating it against a representative dataset. The key is the **dataset/reward function pair**: your dataset should provide the necessary `ground_truth` information that your reward function expects.\n\n**Crafting Your Dataset:**\n\n1.  **Define `ground_truth`**: For each sample in your dataset, the `ground_truth_for_eval` (or a similarly named field specified in your dataset configuration) must contain the information your reward function needs to make a judgment.\n    *   For `exact_tool_match_reward`, `ground_truth` should be a dictionary, often with a `tool_calls` key. This key would hold a list of expected tool calls, each specifying the `name` and `arguments` of the function call. Example:\n        ```json\n        {\n          \"role\": \"assistant\",\n          \"tool_calls\": [\n            {\n              \"name\": \"get_weather\",\n              \"arguments\": {\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}\n            }\n          ]\n        }\n        ```\n2.  **Format**: Datasets are typically JSONL files, where each line is a JSON object representing a sample. Each sample should include:\n    *   `messages`: The input conversation history for the model.\n    *   `tools` (optional, for tool calling): A list of available tools the model can use.\n    *   `ground_truth_for_eval`: The expected output or data for the reward function (e.g., the structure shown above for tool calling).\n    *   An `id` for tracking.\n\n**Example Test Snippet (Conceptual):**\n\nWhile `eval-protocol run` is the primary way to evaluate with datasets, here's a conceptual local test:\n\n```python\nfrom eval_protocol.rewards.function_calling import exact_tool_match_reward # Import the actual function\nfrom eval_protocol.models import Message\n\n# Sample 1: Correct tool call\ntest_messages_correct = [\n    Message(role=\"user\", content=\"What's the weather in SF?\"),\n    Message(role=\"assistant\", tool_calls=[ # Model's generated tool call\n        {\"id\": \"call_123\", \"type\": \"function\", \"function\": {\"name\": \"get_weather\", \"arguments\": '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}'}}\n    ])\n]\nground_truth_correct = { # Expected tool call for the reward function\n    \"tool_calls\": [\n        {\"name\": \"get_weather\", \"arguments\": {\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}}\n    ]\n}\n\n# Sample 2: Incorrect tool call\ntest_messages_incorrect = [\n    Message(role=\"user\", content=\"What's the weather in SF?\"),\n    Message(role=\"assistant\", tool_calls=[\n        {\"id\": \"call_456\", \"type\": \"function\", \"function\": {\"name\": \"get_current_time\", \"arguments\": '{}'}}\n    ])\n]\n# Ground truth remains the same as we expect get_weather\n\n# Test with the actual reward function\nresult_correct = exact_tool_match_reward(messages=test_messages_correct, ground_truth=ground_truth_correct)\nprint(f\"Correct Call - Score: {result_correct.score}, Reason: {result_correct.reason}\")\n\nresult_incorrect = exact_tool_match_reward(messages=test_messages_incorrect, ground_truth=ground_truth_correct)\nprint(f\"Incorrect Call - Score: {result_incorrect.score}, Reason: {result_incorrect.reason}\")\n```\nThis local test helps verify the reward function's logic with specific inputs. For comprehensive evaluation, use `eval-protocol run` with a full dataset (see next section).\n\n### 3. Running Local Evaluations with `eval-protocol run`\n\nFor comprehensive local evaluations, especially when working with datasets and complex configurations, the `eval-protocol run` command is the recommended tool. It leverages Hydra for configuration management, allowing you to define your evaluation pipeline (dataset, model, reward function, etc.) in YAML files.\n\n**Example: Math Evaluation using `codeparrot/gsm8k`**\n\nThe `examples/math_example` demonstrates evaluating models on math word problems.\n\n```bash\n# Ensure you are in the repository root\n# cd /path/to/eval-protocol\n\n# Run evaluation with the math configuration\neval-protocol run --config-name run_math_eval.yaml --config-path examples/math_example/conf\n\n# Override parameters directly from the command line:\neval-protocol run --config-name run_math_eval.yaml --config-path examples/math_example/conf \\\n  generation.model_name=\"accounts/fireworks/models/llama-v3p1-405b-instruct\" \\\n  evaluation_params.limit_samples=10\n```\n\n**What this command does (typically):**\n*   Loads the specified dataset (e.g., GSM8K directly from HuggingFace).\n*   Applies any dataset-specific prompts or preprocessing defined in the configuration.\n*   Generates model responses (e.g., using the Fireworks API or other configured providers).\n*   Evaluates the generated responses using the specified reward function(s).\n*   Saves detailed evaluation results to `<config_output_name>.jsonl` (e.g., `math_example_results.jsonl`) in a timestamped output directory (e.g., under `outputs/`).\n*   Saves generated prompt/response pairs to `preview_input_output_pairs.jsonl` in the same output directory, suitable for inspection or re-evaluation with `eval-protocol preview`.\n\n**Example: APPS Coding Evaluation**\n\nThe `examples/apps_coding_example` shows evaluation on code generation tasks using the `codeparrot/apps` dataset.\n\n```bash\n# Run evaluation with the APPS coding configuration\neval-protocol run --config-path examples/apps_coding_example/conf --config-name run_eval\n\n# Example: Limit samples for a quick test\neval-protocol run --config-path examples/apps_coding_example/conf --config-name run_eval evaluation_params.limit_samples=2\n\n# Example: Disable generation to test reward function on cached responses\neval-protocol run --config-path examples/apps_coding_example/conf --config-name run_eval generation.enabled=false\n```\n\nThese examples showcase how `eval-protocol run` can be adapted for different tasks and datasets through configuration files.\n\nFor more details on this command, Hydra configuration, and advanced usage, see the [CLI Overview](docs/cli_reference/cli_overview.mdx) and [Hydra Configuration Guide](docs/developer_guide/hydra_configuration.mdx).\n\n### Fireworks Authentication Setup (Required for Preview/Deploy with Fireworks)\n\nTo interact with the Fireworks AI platform for deploying and managing evaluations (including some preview scenarios that might use remote evaluators or if `eval-protocol run` uses a Fireworks-hosted model), Eval Protocol needs your Fireworks AI credentials. You can configure these in two ways:\n\n**A. Environment Variables (Highest Priority)**\n\nSet the following environment variables:\n\n*   `FIREWORKS_API_KEY`: Your Fireworks AI API key. This is required for all interactions with the Fireworks API.\n*   `FIREWORKS_ACCOUNT_ID`: Your Fireworks AI Account ID. This is often required for operations like creating or listing evaluators under your account.\n\n```bash\nexport FIREWORKS_API_KEY=\"your_fireworks_api_key\"\nexport FIREWORKS_ACCOUNT_ID=\"your_fireworks_account_id\"\n```\n\n**B. Configuration File (Lower Priority)**\n\nAlternatively, you can store your credentials in a configuration file located at `~/.fireworks/auth.ini`. If environment variables are not set, Eval Protocol will look for this file.\n\nCreate the file with the following format:\n\n```ini\n[fireworks]\napi_key = YOUR_FIREWORKS_API_KEY\naccount_id = YOUR_FIREWORKS_ACCOUNT_ID\n```\n\nReplace `YOUR_FIREWORKS_API_KEY` and `YOUR_FIREWORKS_ACCOUNT_ID` with your actual credentials.\n\n**Credential Sourcing Order:**\n\nEval Protocol will prioritize credentials in the following order:\n1.  Environment Variables (`FIREWORKS_API_KEY`, `FIREWORKS_ACCOUNT_ID`)\n2.  `~/.fireworks/auth.ini` configuration file\n\nEnsure that the `auth.ini` file has appropriate permissions to protect your sensitive credentials.\n\nThe `FIREWORKS_API_KEY` is essential for authenticating your requests to the Fireworks AI service. The `FIREWORKS_ACCOUNT_ID` is used to identify your specific account context for operations that are account-specific, such as managing your evaluators. While the API key authenticates *who* you are, the account ID often specifies *where* (under which account) an operation should take place. Some Fireworks API endpoints may require both.\n\n### 4. Evaluating with Sample Data (Preview)\n\nCreate a JSONL file with sample conversations to evaluate:\n\n```json\n{\"messages\": [{\"role\": \"user\", \"content\": \"Tell me about AI\"}, {\"role\": \"assistant\", \"content\": \"AI refers to systems designed to mimic human intelligence.\"}]}\n{\"messages\": [{\"role\": \"user\", \"content\": \"What is machine learning?\"}, {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of AI that focuses on building systems that can learn from data.\"}]}\n```\n\nPreview your evaluation using the CLI:\n\n```bash\neval-protocol preview --metrics-folders \"word_count=./path/to/metrics\" --samples ./path/to/samples.jsonl\n```\n\nFor example\n```\neval-protocol preview --metrics-folders \"word_count=examples/metrics/word_count\" --samples development/CODING_DATASET.jsonl\n```\n\n### 5. Deploying Your Reward Function\n\nDeploy your reward function to use in training workflows:\n\n```bash\neval-protocol deploy --id my-evaluator --metrics-folders \"word_count=./path/to/metrics\" --force\n```\n\n#### Local Development Server\n\nFor local development and testing, you can deploy a reward function as a local server with external tunnel access:\n\n```bash\n# Deploy as local server with automatic tunnel (ngrok/serveo)\neval-protocol deploy --id test-local-serve-eval --target local-serve --function-ref examples.row_wise.dummy_example.dummy_rewards.simple_echo_reward --verbose --force\n```\n\n**What this does:**\n- Starts a local HTTP server on port 8001 serving your reward function\n- Creates an external tunnel (using ngrok or serveo.net) to make the server publicly accessible\n- Registers the tunnel URL with Fireworks AI for remote evaluation\n- Keeps the server running indefinitely in the background\n\n**Key points:**\n- The CLI returns to prompt after deployment, but the server continues running in background\n- Check running processes: `ps aux | grep -E \"(generic_server|ngrok)\"`\n- Test locally: `curl -X POST http://localhost:8001/evaluate -H \"Content-Type: application/json\" -d '{\"messages\": [{\"role\": \"user\", \"content\": \"test\"}]}'`\n- Monitor logs: `tail -f logs/eval-protocol-local/generic_server_*.log`\n- Stop server: Kill the background processes manually when done\n\nThis is ideal for development, testing webhook integrations, or accessing your reward function from remote services without full cloud deployment.\n\nOr deploy programmatically:\n\n```python\nfrom eval_protocol.evaluation import create_evaluation\n\nevaluator = create_evaluation(\n    evaluator_id=\"my-evaluator\",\n    metric_folders=[\"word_count=./path/to/metrics\"],\n    display_name=\"My Word Count Evaluator\",\n    description=\"Evaluates responses based on word count\",\n    force=True  # Update if already exists\n)\n```\n\n## Advanced Usage\n\n### Multiple Metrics\n\nCombine multiple metrics in a single reward function:\n\n```python\nfrom eval_protocol import reward_function\nfrom eval_protocol.models import EvaluateResult, MetricResult, Message # Assuming models are here\nfrom typing import List, Dict, Any, Optional\n\n@reward_function\ndef combined_reward(\n    messages: List[Dict[str, Any]], # Or List[Message]\n    original_messages: Optional[List[Dict[str, Any]]] = None, # Or List[Message]\n    **kwargs: Any\n) -> EvaluateResult:\n    \"\"\"Evaluate with multiple metrics.\"\"\"\n    response = messages[-1].get(\"content\", \"\")\n\n    # Word count metric\n    word_count = len(response.split())\n    word_score = min(word_count / 100.0, 1.0)\n    word_metric_success = word_count > 10\n\n    # Specificity metric\n    specificity_markers = [\"specifically\", \"for example\", \"such as\"]\n    marker_count = sum(1 for marker in specificity_markers if marker.lower() in response.lower())\n    specificity_score = min(marker_count / 2.0, 1.0)\n    specificity_metric_success = marker_count > 0\n\n    # Combined score with weighted components\n    final_score = word_score * 0.3 + specificity_score * 0.7\n\n    return EvaluateResult(\n        score=final_score,\n        reason=f\"Combined score based on word count ({word_count}) and specificity markers ({marker_count})\",\n        metrics={\n            \"word_count\": MetricResult(\n                score=word_score,\n                success=word_metric_success,\n                reason=f\"Word count: {word_count}\"\n            ),\n            \"specificity\": MetricResult(\n                score=specificity_score,\n                success=specificity_metric_success,\n                reason=f\"Found {marker_count} specificity markers\"\n            )\n        }\n    )\n```\n\n### Custom Model Providers\n\nDeploy your reward function with a specific model provider:\n\n```python\n# Deploy with a custom provider\nmy_function.deploy(\n    name=\"my-evaluator-anthropic\",\n    description=\"My evaluator using Claude model\",\n    providers=[\n        {\n            \"providerType\": \"anthropic\",\n            \"modelId\": \"claude-3-sonnet-20240229\"\n        }\n    ],\n    force=True\n)\n```\n\n## Dataset Integration\n\nEval Protocol provides seamless integration with popular datasets through a simplified configuration system:\n\n### Direct HuggingFace Integration\n\nLoad datasets directly from HuggingFace Hub without manual preprocessing:\n\n```bash\n# Evaluate using GSM8K dataset with math-specific prompts\neval-protocol run --config-name run_math_eval.yaml --config-path examples/math_example/conf\n```\n\n### Derived Datasets\n\nCreate specialized dataset configurations that reference base datasets and apply transformations:\n\n```yaml\n# conf/dataset/gsm8k_math_prompts.yaml\ndefaults:\n  - base_derived_dataset\n  - _self_\n\nbase_dataset: \"gsm8k\"\nsystem_prompt: \"Solve the following math problem. Show your work clearly. Put the final numerical answer between <answer> and </answer> tags.\"\noutput_format: \"evaluation_format\"\nderived_max_samples: 5\n```\n\n### Key Benefits\n\n- **No Manual Conversion**: Datasets are converted to evaluation format on-the-fly\n- **System Prompt Integration**: Prompts are part of dataset configuration, not evaluation logic\n- **Flexible Column Mapping**: Automatically adapts different dataset formats\n- **Reusable Configurations**: Base datasets can be extended for different use cases\n\nSee the [math example](examples/math_example/) for a complete demonstration of the dataset system.\n\n## Detailed Documentation\n\nFor more comprehensive information, including API references, tutorials, and advanced guides, please see our [full documentation](docs/documentation_home.mdx).\n\n## Examples\n\nCheck the `examples` directory for complete examples:\n\n- `evaluation_preview_example.py`: How to preview an evaluator.\n- `deploy_example.py`: How to deploy a reward function to Fireworks.\n- `math_example/`: Demonstrates CLI-based evaluation (`eval-protocol run`) and TRL GRPO training for math problems (GSM8K dataset).\n- `apps_coding_example/`: Shows CLI-based evaluation (`eval-protocol run`) for code generation tasks (APPS dataset).\n - `apps_coding_example/`: Shows CLI-based evaluation (`eval-protocol run`) for code generation tasks (APPS dataset).\n\nThe OpenEvals project provides a suite of evaluators that can be used directly within Eval Protocol. The helper `eval_protocol.integrations.openeval.adapt` converts any OpenEvals evaluator into a reward function returning an `EvaluateResult`.\n\n```python\nfrom openevals import exact_match\nfrom eval_protocol.integrations.openeval import adapt\n\nexact_match_reward = adapt(exact_match)\nresult = exact_match_reward(\n    messages=[{\"role\": \"assistant\", \"content\": \"hello\"}],\n    ground_truth=\"hello\",\n)\nprint(result.score)\n```\n\nThe [deepeval](https://github.com/confident-ai/deepeval) project also offers a\nvariety of metrics. The helper `eval_protocol.integrations.deepeval.adapt_metric`\nconverts a deepeval metric instance into a reward function returning an\n`EvaluateResult`.\n\n```python\nfrom deepeval.metrics import FaithfulnessMetric\nfrom eval_protocol.integrations.deepeval import adapt_metric\n\nfaithfulness_reward = adapt_metric(FaithfulnessMetric())\nresult = faithfulness_reward(\n    messages=[{\"role\": \"assistant\", \"content\": \"hello\"}],\n    ground_truth=\"hello\",\n)\nprint(result.score)\n```\n\nThe GEval metric family uses an LLM-as-a-judge to score outputs based on\ncustom criteria. You can construct a `GEval` metric and adapt it in the same\nway:\n\n```python\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCaseParams\nfrom eval_protocol.integrations.deepeval import adapt_metric\n\ncorrectness_metric = GEval(\n    name=\"Correctness\",\n    criteria=\"Determine whether the answer is factually correct\",\n    evaluation_params=[\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT,\n        LLMTestCaseParams.EXPECTED_OUTPUT,\n    ],\n)\n\ncorrectness_reward = adapt_metric(correctness_metric)\nresult = correctness_reward(\n    messages=[{\"role\": \"user\", \"content\": \"Who wrote 1984?\"}, {\"role\": \"assistant\", \"content\": \"George Orwell\"}],\n    ground_truth=\"George Orwell\",\n)\nprint(result.score)\n```\n\n## Command Line Interface\n\nEval Protocol includes a CLI for common operations:\n\n```bash\n# Show help\neval-protocol --help\n\n# Preview an evaluator\neval-protocol preview --metrics-folders \"metric=./path\" --samples ./samples.jsonl\n\n# Deploy an evaluator\neval-protocol deploy --id my-evaluator --metrics-folders \"metric=./path\" --force\n```\n\n## Community and Support\n\n*   **GitHub Issues**: For bug reports and feature requests, please use [GitHub Issues](https://github.com/eval-protocol/python-sdk/issues).\n*   **GitHub Discussions**: (If enabled) For general questions, ideas, and discussions.\n*   Please also review our [Contributing Guidelines](development/CONTRIBUTING.md) and [Code of Conduct](CODE_OF_CONDUCT.md).\n\n## Development\n\n### Type Checking\n\nThe codebase uses mypy for static type checking. To run type checking:\n\n```bash\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Run mypy\nmypy eval_protocol\n```\n\nOur CI pipeline enforces type checking, so please ensure your code passes mypy checks before submitting PRs.\n\n### Running Tests\n\n```bash\n# Install test dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n```\n\n## Code of Conduct\n\nWe are dedicated to providing a welcoming and inclusive experience for everyone. Please review and adhere to our [Code of Conduct](CODE_OF_CONDUCT.md).\n\n## License\n\nEval Protocol is released under the Apache License 2.0.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["License-File"], "home_page": null, "keywords": null, "license": null, "license_expression": "Apache-2.0", "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "eval-protocol", "package_url": "https://pypi.org/project/eval-protocol/", "platform": null, "project_url": "https://pypi.org/project/eval-protocol/", "project_urls": {"Homepage": "https://github.com/fireworks-ai/eval-protocol"}, "provides_extra": ["dev", "trl", "openevals", "fireworks"], "release_url": "https://pypi.org/project/eval-protocol/0.1.4/", "requires_dist": ["requests>=2.25.0", "pydantic>=2.0.0", "dataclasses-json>=0.5.7", "fastapi>=0.68.0", "uvicorn>=0.15.0", "python-dotenv>=0.19.0", "openai==1.78.1", "aiosqlite", "aiohttp", "mcp>=1.9.2", "PyYAML>=5.0", "datasets", "fsspec", "hydra-core>=1.3.2", "omegaconf>=2.3.0", "gymnasium>=0.29.0", "httpx>=0.24.0", "anthropic>=0.59.0", "ipykernel>=6.30.0", "jupyter>=1.1.1", "build; extra == \"dev\"", "twine; extra == \"dev\"", "pytest>=6.0.0; extra == \"dev\"", "pytest-asyncio; extra == \"dev\"", "pytest-httpserver; extra == \"dev\"", "werkzeug>=2.0.0; extra == \"dev\"", "black>=21.5b2; extra == \"dev\"", "mypy>=0.812; extra == \"dev\"", "flake8>=3.9.2; extra == \"dev\"", "autopep8>=1.5.0; extra == \"dev\"", "transformers>=4.0.0; extra == \"dev\"", "types-setuptools; extra == \"dev\"", "types-requests; extra == \"dev\"", "types-PyYAML; extra == \"dev\"", "types-docker; extra == \"dev\"", "versioneer>=0.20; extra == \"dev\"", "openai==1.78.1; extra == \"dev\"", "pre-commit; extra == \"dev\"", "e2b; extra == \"dev\"", "pytest-cov; extra == \"dev\"", "pytest-xdist; extra == \"dev\"", "docker==7.1.0; extra == \"dev\"", "ipykernel>=6.30.0; extra == \"dev\"", "jupyter>=1.1.1; extra == \"dev\"", "pip>=25.1.1; extra == \"dev\"", "torch>=1.9; extra == \"trl\"", "trl>=0.7.0; extra == \"trl\"", "peft>=0.7.0; extra == \"trl\"", "transformers>=4.0.0; extra == \"trl\"", "accelerate>=0.28.0; extra == \"trl\"", "openevals>=0.1.0; extra == \"openevals\"", "fireworks-ai>=0.19.10; extra == \"fireworks\""], "requires_python": ">=3.10", "summary": "A Python library for defining, testing, and using reward functions", "version": "0.1.4", "yanked": false, "yanked_reason": null}, "last_serial": 30388253, "urls": [{"comment_text": null, "digests": {"blake2b_256": "d072a2cb9cea4e2d29e8532d6795775588f88e6e2315c31375b37423844613bc", "md5": "ab5906c89b366cb015dfd514b43b4989", "sha256": "80cdad6d4e6054b4e388f84230af0a0d9bd8f67c2eaffb052d215d8e4f1df1cd"}, "downloads": -1, "filename": "eval_protocol-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "ab5906c89b366cb015dfd514b43b4989", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.10", "size": 371143, "upload_time": "2025-07-28T17:55:50", "upload_time_iso_8601": "2025-07-28T17:55:50.383217Z", "url": "https://files.pythonhosted.org/packages/d0/72/a2cb9cea4e2d29e8532d6795775588f88e6e2315c31375b37423844613bc/eval_protocol-0.1.4-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "12b3baa5253a0f30be5a831cdf8e27bf5b104a1f1f82648dff077f7ca3069dfd", "md5": "e4c2831e20f1d4dced03f8de5bdadd2b", "sha256": "a3ee3ca4de5302c4d85314edb125309fcdc3b49086b78ea2ef9fab8ebc0a609e"}, "downloads": -1, "filename": "eval_protocol-0.1.4.tar.gz", "has_sig": false, "md5_digest": "e4c2831e20f1d4dced03f8de5bdadd2b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.10", "size": 475578, "upload_time": "2025-07-28T17:55:51", "upload_time_iso_8601": "2025-07-28T17:55:51.738156Z", "url": "https://files.pythonhosted.org/packages/12/b3/baa5253a0f30be5a831cdf8e27bf5b104a1f1f82648dff077f7ca3069dfd/eval_protocol-0.1.4.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:05Z", "published": "Mon, 28 Jul 2025 17:55:35 GMT", "package": "vacancycalculator", "version": "0.3.1.2", "json": {"info": {"author": "E.Bringa-S.Bergamin-SiMaF", "author_email": "santiagobergamin@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Home-Page", "License", "Requires-Dist", "Summary"], "home_page": "https://github.com/TiagoBe0/VFScript-SiMaF", "keywords": null, "license": "MIT", "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "vacancycalculator", "package_url": "https://pypi.org/project/vacancycalculator/", "platform": null, "project_url": "https://pypi.org/project/vacancycalculator/", "project_urls": {"Homepage": "https://github.com/TiagoBe0/VFScript-SiMaF"}, "provides_extra": null, "release_url": "https://pypi.org/project/vacancycalculator/0.3.1.2/", "requires_dist": ["scikit-learn", "pandas", "xgboost", "ovito", "numpy"], "requires_python": null, "summary": "Defect analysis and vacancy calculation for materials science", "version": "0.3.1.2", "yanked": false, "yanked_reason": null}, "last_serial": 30388504, "urls": [{"comment_text": null, "digests": {"blake2b_256": "fdbd522727b1a4192b7b89c741365dd28fd09e21d8f1534ccbd86de851ba10dc", "md5": "5860b807b1b1342296866ba6d9ba8507", "sha256": "5f03850004260449ee3e2f0c0365110c573aa0be6a396d1bdc02523c1056ce27"}, "downloads": -1, "filename": "vacancycalculator-0.3.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "5860b807b1b1342296866ba6d9ba8507", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 59246, "upload_time": "2025-07-28T17:55:35", "upload_time_iso_8601": "2025-07-28T17:55:35.725134Z", "url": "https://files.pythonhosted.org/packages/fd/bd/522727b1a4192b7b89c741365dd28fd09e21d8f1534ccbd86de851ba10dc/vacancycalculator-0.3.1.2-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "b831a6ecf8a7591f21966d44d4ae066b74616166f8b0a18a5bdc0a402ef72dfc", "md5": "be875cfbdc8d36d5b5f1568abcd2ad94", "sha256": "334242ecac4cbd0e4647e5a672b6c4891c6f7311d67bba3c552113d506bc5f5e"}, "downloads": -1, "filename": "vacancycalculator-0.3.1.2.tar.gz", "has_sig": false, "md5_digest": "be875cfbdc8d36d5b5f1568abcd2ad94", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 36358, "upload_time": "2025-07-28T17:55:37", "upload_time_iso_8601": "2025-07-28T17:55:37.456892Z", "url": "https://files.pythonhosted.org/packages/b8/31/a6ecf8a7591f21966d44d4ae066b74616166f8b0a18a5bdc0a402ef72dfc/vacancycalculator-0.3.1.2.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:05Z", "published": "Mon, 28 Jul 2025 17:55:34 GMT", "package": "pan-aisecurity", "version": "0.5.0.5.post2", "json": {"info": {"author": null, "author_email": "Palo Alto Networks AI Runtime Security SDK Team <dl-airs-api-sdk@paloaltonetworks.com>", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: Other/Proprietary License", "Natural Language :: English", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Typing :: Typed"], "description": "Palo Alto Networks Prisma AIRS Scan API Python SDK\n=============================================\n\nThis Python SDK provides convenient access to the\nPalo Alto Networks AI Runtime Security: API Intercept\nfor Python applications. This SDK includes type\ndefinitions for all request params and response fields and offers both\nsynchronous and asynchronous (asyncio) operations.\n\n<!--TOC-->\n\n- [API Documentation](#api-documentation)\n- [Installation](#installation)\n- [SDK Configuration](#sdk-configuration)\n  - [API Key](#api-key)\n  - [AI Profile](#ai-profile)\n- [Example: SDK Configuration](#example-sdk-configuration)\n  - [Using AI Profile Name](#using-ai-profile-name)\n  - [Using AI Profile ID](#using-ai-profile-id)\n- [Examples: Traditional Python (non-asyncio)](#examples-traditional-python-non-asyncio)\n  - [Inline (Synchronous) Scan Example](#inline-synchronous-scan-example)\n  - [Batch (Asynchronous) Scan Example](#batch-asynchronous-scan-example)\n  - [Scan Results Example](#scan-results-example)\n  - [Scan Reports Example](#scan-reports-example)\n- [Examples: Concurrent Python (asyncio)](#examples-concurrent-python-asyncio)\n  - [Inline (Synchronous) Scan Example (asyncio)](#inline-synchronous-scan-example-asyncio)\n  - [Batch (Asynchronous) Scan Example (asyncio)](#batch-asynchronous-scan-example-asyncio)\n  - [Scan Results Example (asyncio)](#scan-results-example-asyncio)\n  - [Scan Reports Example (asyncio)](#scan-reports-example-asyncio)\n- [Examples: Model Context Protocol](#examples-model-context-protocol)\n  - [Model Context Protocol Server Example](#model-context-protocol-server-example)\n- [Error Handling & Exceptions](#error-handling--exceptions)\n- [Compatability Policy](#compatability-policy)\n- [Legal](#legal)\n\n<!--TOC-->\n\n\n<a id=\"api-documentation\" aria-hidden=\"true\" href=\"#api-documentation\">\n\n# API Documentation\n\n</a>\n\nThe reference API documentation for Palo Alto Networks AI Runtime Security:\nAPI Intercept can be found at [https://pan.dev/ai-runtime-security/scan/api/](https://pan.dev/ai-runtime-security/scan/api/)\n\n<a id=\"installation\" href=\"#installation\">\n\n# Installation\n\n</a>\n\n```sh\n# Create and activate a virtual environment\npython3 -m venv --prompt ${PWD##*/} .venv && source .venv/bin/activate\n\n# Install most recent release version of aisecurity package\npython3 -m pip install \"pan-aisecurity\"\n```\n<a id=\"sdk-configuration\" href=\"#sdk-configuration\">\n\n# SDK Configuration\n\n</a>\n\nThe `aisecurity.init()` function accepts the following _**optional**_ parameters:\n\n- `api_key` (optional): Provide your API key through configuration or an environment variable.\n  - If `api_key` is not set, the environment variable `PANW_AI_SEC_API_KEY` will be used, if available.\n- `num_retries` (optional): Default value is 5.\n\n<a id=\"api-key\" href=\"#api-key\">\n\n## API Key\n\n</a>\n\nThere are two ways to specify your API key:\n\n1. Using an environment variable:\n\n```sh\nexport PANW_AI_SEC_API_KEY=YOUR_API_KEY\n```\n\n2. Specify your API key in `aisecurity.init()` with the `api_key` parameter:\n\n```python\napi_key = get_api_key_from_somewhere() # Fetch from Vault, Secrets Manager, etc.\naisecurity.init(api_key=api_key)\n```\n\n<a id=\"ai-profile\" href=\"#ai-profile\">\n\n## AI Profile\n\n</a>\n\nYou must provide ONE of: `profile_name` or `profile_id`\n\n```python\nai_profile = AiProfile(profile_id=\"YOUR_AI_PROFILE_ID\")\n```\n\nor\n\n```python\nai_profile = AiProfile(profile_name=\"YOUR_AI_PROFILE_NAME\")\n```\n\n<a id=\"example-sdk-configuration\" href=\"#example-sdk-configuration\">\n\n# Example: SDK Configuration\n\n</a>\n\n<a id=\"using-ai-profile-name\" href=\"#using-ai-profile-name\">\n\n## Using AI Profile Name\n\n</a>\n\n```python\nimport aisecurity\nAI_PROFILE_NAME = \"YOUR_AI_PROFILE_NAME\"\nAPI_KEY = os.getenv(\"PANW_AI_SEC_API_KEY\")\n\naisecurity.init(api_key=API_KEY)\nai_profile = AiProfile(profile_name=AI_PROFILE_NAME)\n```\n\n<a id=\"using-ai-profile-id\" href=\"#using-ai-profile-id\">\n\n## Using AI Profile ID\n\n</a>\n\n```python\nimport aisecurity\nAI_PROFILE_ID = \"YOUR_AI_PROFILE_ID\"\nAPI_KEY = os.getenv(\"PANW_AI_SEC_API_KEY\")\n\naisecurity.init(api_key=API_KEY)\nai_profile = AiProfile(profile_id=AI_PROFILE_ID)\n```\n\n<a id=\"examples-traditional-python-non-asyncio\" href=\"#examples-traditional-python-non-asyncio\">\n\n# Examples: Traditional Python (non-asyncio)\n\n</a>\n\n**Important**: You must properly configure an API Key and AI Profile ID or Name\nbefore using the SDK examples.\n\n<a id=\"inline-synchronous-scan-example\" href=\"#inline-synchronous-scan-example\">\n\n## Inline (Synchronous) Scan Example\n\n</a>\n\nAPI Reference: https://pan.dev/ai-runtime-security/api/scan-sync-request/\n\n<!-- source: examples/traditional/inline_sync_scan.py -->\n\n```python\n\nimport os\nfrom pprint import pprint\n\nimport aisecurity\nfrom aisecurity.generated_openapi_client.models.ai_profile import AiProfile\n\n# IMPORTANT: For traditional (non-asyncio), import Scanner from aisecurity.scan.inline.scanner\nfrom aisecurity.scan.inline.scanner import Scanner\nfrom aisecurity.scan.models.content import Content\n\nAI_PROFILE_NAME = \"YOUR_AI_PROFILE_NAME\"\nAPI_KEY = os.getenv(\"PANW_AI_SEC_API_KEY\")\n\n# Initialize the SDK with your API Key\naisecurity.init(api_key=API_KEY)\n\n# Configure an AI Profile\nai_profile = AiProfile(profile_name=AI_PROFILE_NAME)\n\n# Create a Scanner\nscanner = Scanner()\n\nscan_response = scanner.sync_scan(\n    ai_profile=ai_profile,\n    content=Content(\n        prompt=\"Questionable User Prompt Text\",\n        response=\"Questionable Model Response Text\",\n    ),\n)\npprint(scan_response)\n```\n\n<a id=\"batch-asynchronous-scan-example\" href=\"#batch-asynchronous-scan-example\">\n\n## Batch (Asynchronous) Scan Example\n\n</a>\n\nAPI Reference: https://pan.dev/ai-runtime-security/api/scan-async-request/\n\n<!-- source: examples/traditional/batch_async_scan.py -->\n\n```python\nimport os\nfrom pprint import pprint\n\nimport aisecurity\nfrom aisecurity.generated_openapi_client.models.ai_profile import AiProfile\n\n# IMPORTANT: For traditional (non-asyncio), import Scanner from aisecurity.scan.inline.scanner\nfrom aisecurity.scan.inline.scanner import Scanner\nfrom aisecurity.scan.models.content import Content\n\nAI_PROFILE_NAME = \"YOUR_AI_PROFILE_NAME\"\nAPI_KEY = os.getenv(\"PANW_AI_SEC_API_KEY\")\n\n# Initialize the SDK with your API Key\naisecurity.init(api_key=API_KEY)\n\n# Configure an AI Profile\nai_profile = AiProfile(profile_name=AI_PROFILE_NAME)\n\n# Create a Scanner\nscanner = Scanner()\n\nscan_response = scanner.sync_scan(\n    ai_profile=ai_profile,\n    content=Content(\n        prompt=\"Questionable User Prompt Text\",\n        response=\"Questionable Model Response Text\",\n    ),\n)\n# See API documentation for response structure\n# https://pan.dev/ai-runtime-security/api/scan-sync-request/\npprint(scan_response)\n```\n\n<a id=\"scan-results-example\" href=\"#scan-results-example\">\n\n## Scan Results Example\n\n</a>\n\nAPI Reference: https://pan.dev/ai-runtime-security/api/get-scan-results-by-scan-i-ds/\n\n<!-- source: examples/traditional/scan_results.py -->\n\n```python\n\nimport aisecurity\n\n# IMPORTANT: For traditional (non-asyncio), import Scanner from aisecurity.scan.inline.scanner\nfrom aisecurity.scan.inline.scanner import Scanner\n\naisecurity.init()\n\nscanner = Scanner()\n\n# See API documentation for response structure\n# https://pan.dev/ai-runtime-security/api/get-scan-results-by-scan-i-ds/\nexample_scan_id = \"020e7c31-0000-4e0d-a2a6-215a0d5c56d9\"\nscan_by_ids_response = scanner.query_by_scan_ids(scan_ids=[example_scan_id])\n```\n\n<a id=\"scan-reports-example\" href=\"#scan-reports-example\">\n\n## Scan Reports Example\n\n</a>\n\nAPI Reference: https://pan.dev/ai-runtime-security/api/get-threat-scan-reports/\n\n<!-- source: examples/traditional/scan_reports.py -->\n\n```python\nimport aisecurity\n\n# IMPORTANT: For traditional (non-asyncio), import Scanner from aisecurity.scan.inline.scanner\nfrom aisecurity.scan.inline.scanner import Scanner\n\naisecurity.init()\n\nscanner = Scanner()\n\n# See API documentation for response structure\n# https://pan.dev/ai-runtime-security/api/get-threat-scan-reports/\nexample_report_id = \"020e7c31-0000-4e0d-a2a6-215a0d5c56d9\"\nthreat_scan_reports = scanner.query_by_report_ids(report_ids=[example_report_id])\n```\n\n<a id=\"examples-concurrent-python-asyncio\" href=\"#examples-concurrent-python-asyncio\">\n\n# Examples: Concurrent Python (asyncio)\n\n</a>\n\n**Important**: You must properly configure an API Key and AI Profile ID or Name\nbefore using the SDK examples.\n\n<a id=\"inline-synchronous-scan-example-asyncio\" href=\"#inline-synchronous-scan-example-asyncio\">\n\n## Inline (Synchronous) Scan Example (asyncio)\n\n</a>\n\nAPI Reference: https://pan.dev/ai-runtime-security/api/scan-sync-request/\n\n<!-- source: examples/asyncio/inline_sync_scan.py -->\n\n```python\n\nimport asyncio\nimport os\nfrom pprint import pprint\n\nimport aisecurity\nfrom aisecurity.generated_openapi_client.models.ai_profile import AiProfile\n\n# IMPORTANT: For asyncio, import Scanner from aisecurity.scan.asyncio.scanner\nfrom aisecurity.scan.asyncio.scanner import Scanner\nfrom aisecurity.scan.models.content import Content\n\nAI_PROFILE_NAME = \"YOUR_AI_PROFILE_NAME\"\nAPI_KEY = os.getenv(\"PANW_AI_SEC_API_KEY\")\n\n# Initialize the SDK with your API Key\naisecurity.init(api_key=API_KEY)\n\n# Configure an AI Profile\nai_profile = AiProfile(profile_name=AI_PROFILE_NAME)\n\n# Create a Scanner\nscanner = Scanner()\n\n\nasync def main():\n    scan_response = await scanner.sync_scan(\n        ai_profile=ai_profile,\n        content=Content(\n            prompt=\"Questionable User Prompt Text\",\n            response=\"Questionable Model Response Text\",\n        ),\n    )\n    # See API documentation for response structure\n    # https://pan.dev/ai-runtime-security/api/scan-sync-request/\n    pprint(scan_response)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\n<a id=\"batch-asynchronous-scan-example-asyncio\" href=\"#batch-asynchronous-scan-example-asyncio\">\n\n## Batch (Asynchronous) Scan Example (asyncio)\n\n</a>\n\nAPI Reference: https://pan.dev/ai-runtime-security/api/scan-async-request/\n\n<!-- source: examples/asyncio/batch_async_scan.py -->\n\n```python\n\nimport asyncio\nimport os\nfrom pprint import pprint\n\nimport aisecurity\nfrom aisecurity.generated_openapi_client import AiProfile\nfrom aisecurity.generated_openapi_client import AsyncScanObject\nfrom aisecurity.generated_openapi_client import ScanRequest\nfrom aisecurity.generated_openapi_client import ScanRequestContentsInner\n\n# IMPORTANT: For asyncio, import Scanner from aisecurity.scan.asyncio.scanner\nfrom aisecurity.scan.asyncio.scanner import Scanner\n\nAI_PROFILE_NAME = \"YOUR_AI_PROFILE_NAME\"\nAPI_KEY = os.getenv(\"PANW_AI_SEC_API_KEY\")\n\n# Initialize the SDK with your API Key\naisecurity.init(api_key=API_KEY)\n\n# Configure an AI Profile\nai_profile = AiProfile(profile_name=AI_PROFILE_NAME)\n\n# Create a Scanner\nscanner = Scanner()\n\nreq_ids = 0\n# Batch (Asyncronous) Scan supports up to 5 Scan Request Objects\nasync_scan_objects = [\n    AsyncScanObject(\n        req_id=(req_ids := req_ids + 1),\n        scan_req=ScanRequest(\n            ai_profile=ai_profile,\n            contents=[\n                ScanRequestContentsInner(\n                    prompt=\"First Questionable User Prompt Text\",\n                    response=\"First Questionable Model Response Text\",\n                )\n            ],\n        ),\n    ),\n    AsyncScanObject(\n        req_id=(req_ids := req_ids + 1),\n        scan_req=ScanRequest(\n            ai_profile=ai_profile,\n            contents=[\n                ScanRequestContentsInner(\n                    prompt=\"Second Questionable User Prompt Text\",\n                    response=\"Second Questionable Model Response Text\",\n                )\n            ],\n        ),\n    ),\n]\n\n\nasync def main():\n    response = await scanner.async_scan(async_scan_objects)\n    # See API documentation for response structure\n    # https://pan.dev/ai-runtime-security/api/scan-async-request/\n    pprint(\n        {\n            \"received\": response.received,\n            \"scan_id\": response.scan_id,\n            \"report_id\": response.report_id,\n        }\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n<a id=\"scan-results-example-asyncio\" href=\"#scan-results-example-asyncio\">\n\n## Scan Results Example (asyncio)\n\n</a>\n\nAPI Reference: https://pan.dev/ai-runtime-security/api/get-scan-results-by-scan-i-ds/\n\n<!-- source: examples/asyncio/scan_results.py -->\n\n```python\nimport asyncio\nfrom pprint import pprint\n\nimport aisecurity\n\n# IMPORTANT: For asyncio, import Scanner from aisecurity.scan.asyncio.scanner\nfrom aisecurity.scan.asyncio.scanner import Scanner\n\naisecurity.init()\n\nscanner = Scanner()\n\n\nasync def main():\n    # See API documentation for response structure\n    # https://pan.dev/ai-runtime-security/api/get-scan-results-by-scan-i-ds/\n    example_scan_id = \"020e7c31-0000-4e0d-a2a6-215a0d5c56d9\"\n    scan_results = await scanner.query_by_scan_ids(scan_ids=[example_scan_id])\n    pprint(scan_results)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n<a id=\"scan-reports-example-asyncio\" href=\"#scan-reports-example-asyncio\">\n\n## Scan Reports Example (asyncio)\n\n</a>\n\nAPI Reference: https://pan.dev/ai-runtime-security/api/get-threat-scan-reports/\n\n<!-- source: examples/asyncio/scan_reports.py -->\n\n```python\nimport asyncio\nfrom pprint import pprint\n\nimport aisecurity\n\n# IMPORTANT: For asyncio, import Scanner from aisecurity.scan.asyncio.scanner\nfrom aisecurity.scan.asyncio.scanner import Scanner\n\naisecurity.init()\n\nscanner = Scanner()\n\n\nasync def main():\n    # See API documentation for response structur\n    # https://pan.dev/ai-runtime-security/api/get-threat-scan-reports/\n    example_report_id = \"020e7c31-0000-4e0d-a2a6-215a0d5c56d9\"\n    threat_scan_reports = await scanner.query_by_report_ids(\n        report_ids=[example_report_id]\n    )\n    pprint(threat_scan_reports)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n\n<a id=\"examples-model-context-protocol\" href=\"#examples-model-context-protocol\">\n\n# Examples: Model Context Protocol\n\n</a>\n\n<a id=\"model-context-protocol-server-example\" href=\"#model-context-protocol-server-example\">\n\n## Model Context Protocol Server Example\n\n</a>\n\n<!-- source examples/model_context_protocol/server.py -->\n\n```python\n#!/usr/bin/env -S uv run fastmcp run -t sse # noqa: CPY001\n\"\"\"\nPalo Alto Networks AI Runtime Security (AIRS) API - Model Context Protocol (MCP) Server Example\n\nThis is an example MCP Server demonstrating the use of the AI Runtime Security API Intercept as MCP Tools.\n\nThe server exposes the AIRS API functionality of as various MCP tools:\n- Inline Prompt/Response Scanning\n- Batch (Asynchronous) Scanning for collections of Prompts/Responses\n- Retrieval of Scan Results and Scan Threat Reports\n\"\"\"\n# PEP 723 Inline Script Metadata\n# /// script\n# requires-python = \">=3.10\"\n# dependencies = [\n#     \"pan-aisecurity\",\n#     \"fastmcp\",\n#     \"python-dotenv\",\n# ]#\n# ///\n\nimport asyncio\nimport itertools\nimport os\nimport sys\nfrom collections.abc import AsyncIterator\nfrom contextlib import asynccontextmanager\n\nimport dotenv\nfrom fastmcp import FastMCP\nfrom fastmcp.exceptions import ToolError\nfrom typing_extensions import Any, TypedDict\n\nimport aisecurity\nfrom aisecurity.constants.base import (\n    MAX_NUMBER_OF_BATCH_SCAN_OBJECTS,\n    MAX_NUMBER_OF_SCAN_IDS,\n)\nfrom aisecurity.generated_openapi_client import (\n    AsyncScanObject,\n    AsyncScanResponse,\n    ScanIdResult,\n    ScanRequest,\n    ScanRequestContentsInner,\n    ScanResponse,\n    ThreatScanReportObject,\n)\nfrom aisecurity.generated_openapi_client.models.ai_profile import AiProfile\nfrom aisecurity.scan.asyncio.scanner import Scanner\nfrom aisecurity.scan.models.content import Content\nfrom aisecurity.utils import safe_flatten\n\nai_profile: AiProfile\nscanner = Scanner()\n\n\n@asynccontextmanager\nasync def mcp_lifespan_manager(*args, **kwargs) -> AsyncIterator[Any]:\n    \"\"\"Starlette Lifespan Context Manager\n\n    This is required to close the shared aiohttp connection pool on server shutdown.\n    \"\"\"\n    yield\n    await scanner.close()\n\n\n# Create the MCP Server with the lifespan context manager\nmcp = FastMCP(\"aisecurity-scan-server\", lifespan=mcp_lifespan_manager)\n\n\nclass SimpleScanContent(TypedDict):\n    \"\"\"SimpleScanContent is a TypedDict representing a greatly simplified ScanRequestContentsInner object.\"\"\"\n\n    prompt: str | None\n    response: str | None\n\n\ndef pan_init():\n    \"\"\"Initialize the AI Runtime Security SDK (e.g. with your API Key).\n\n    NOTE: You probably DON'T want to run aisecurity.init() at the module top-level\n    to ensure the MCP Server Runtime Environment has a chance to set up environment\n    variables _before_ this function is run.\n    \"\"\"\n    global ai_profile\n\n    # Load Environment variables from .env if available\n    dotenv.load_dotenv()\n    # Make this function run only once\n    if getattr(pan_init, \"__completed__\", False):\n        return\n    if ai_profile_name := os.getenv(\"PANW_AI_PROFILE_NAME\"):\n        ai_profile = AiProfile(profile_name=ai_profile_name)\n    elif ai_profile_id := os.getenv(\"PANW_AI_PROFILE_ID\"):\n        ai_profile = AiProfile(profile_id=ai_profile_id)\n    else:\n        raise ToolError(\"Missing AI Profile Name (PANW_AI_PROFILE_NAME) or AI Profile ID (PANW_AI_PROFILE_ID)\")\n    aisecurity.init(\n        api_key=os.getenv(\"PANW_AI_SEC_API_KEY\"),  # Optional - shows default fallback behavior\n        api_endpoint=os.getenv(\"PANW_AI_SEC_API_ENDPOINT\"),  # Optional - shows default fallback behavior\n    )\n    setattr(pan_init, \"__completed__\", True)\n\n\n@mcp.tool()\nasync def pan_inline_scan(prompt: str | None = None, response: str | None = None) -> ScanResponse:\n    \"\"\"Submit a single Prompt and/or Model-Response (Scan Content) to be scanned synchronously.\n\n    This is a blocking operation - the function will not return until the scan is complete\n    or a timeout, (e.g. as configured in the AI Profile), is breached.\n\n    Returns a complete Scan Response, notably the category (benign/malicious) and action (allow/block).\n\n    See also: https://pan.dev/ai-runtime-security/api/scan-sync-request/\n    \"\"\"\n    pan_init()\n    if not prompt and not response:\n        raise ToolError(f\"Must provide at least one of prompt ({prompt}) and/or response ({response}).\")\n    scan_response = await scanner.sync_scan(\n        ai_profile=ai_profile,\n        content=Content(\n            prompt=prompt,\n            response=response,\n        ),\n    )\n    return scan_response\n\n\n@mcp.tool()\nasync def pan_batch_scan(\n    scan_contents: list[SimpleScanContent],\n) -> list[AsyncScanResponse]:\n    \"\"\"Submit multiple Scan Contents containing prompts/model-responses for asynchronous (batch) scanning.\n\n    Automatically splits requests into batches of 5, which are submitted concurrently.\n\n    Returns a list of AsyncScanResponse objects, each includes a scan_id and report_id,\n    which can be used to retrieve scan results after the asynchronous scans are complete.\n\n    See also: https://pan.dev/ai-runtime-security/api/scan-async-request/\n    \"\"\"\n    global ai_profile\n\n    pan_init()\n    # build the AsyncScanContent object\n    async_scan_batches: list[list[AsyncScanObject]] = []\n\n    req_id = 0\n    # Split into batches\n    for batch in itertools.batched(scan_contents, MAX_NUMBER_OF_BATCH_SCAN_OBJECTS):\n        async_scan_batches.append([\n            AsyncScanObject(\n                req_id=(req_id := req_id + 1),\n                scan_req=ScanRequest(\n                    ai_profile=ai_profile,\n                    contents=[\n                        ScanRequestContentsInner(\n                            prompt=sc.get(\"prompt\"),\n                            response=sc.get(\"response\"),\n                        )\n                    ],\n                ),\n            )\n            for sc in batch\n        ])\n\n    # Process each batch concurrently via asyncio\n    scan_coros = [scanner.async_scan(batch) for batch in async_scan_batches]\n    bulk_scan_results: list[AsyncScanResponse] = await asyncio.gather(*scan_coros)\n\n    return bulk_scan_results\n\n\n@mcp.tool()\nasync def pan_get_scan_results(scan_ids: list[str]) -> list[ScanIdResult]:\n    \"\"\"Retrieve Scan Results with a list of Scan IDs.\n\n    A Scan ID is a UUID string.\n\n    See also: https://pan.dev/ai-runtime-security/api/get-scan-results-by-scan-i-ds/\n    \"\"\"\n    pan_init()\n    request_batches: list[list[str]] = []\n    for batch in itertools.batched(scan_ids, MAX_NUMBER_OF_SCAN_IDS):\n        request_batches.append(list(batch))\n\n    # Process each batch concurrently via asyncio\n    tasks = [scanner.query_by_scan_ids(batch) for batch in request_batches]\n    batch_results: list[list[ScanIdResult]] = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # flatten nested list\n    return safe_flatten(batch_results)\n\n\n@mcp.tool()\nasync def pan_get_scan_reports(report_ids: list[str]) -> list[ThreatScanReportObject]:\n    \"\"\"Retrieve Scan Reports with a list of Scan Report IDs.\n\n    A Scan Report ID is a Scan ID (UUID) prefixed with \"R\".\n\n    See also: https://pan.dev/ai-runtime-security/api/get-scan-results-by-scan-i-ds/\n    \"\"\"\n    pan_init()\n\n    request_batches: list[list[str]] = []\n    for batch in itertools.batched(report_ids, MAX_NUMBER_OF_SCAN_IDS):\n        request_batches.append(list(batch))\n\n    # Process each batch concurrently via asyncio\n    tasks = [scanner.query_by_scan_ids(batch) for batch in request_batches]\n    await asyncio.gather(*tasks, return_exceptions=True)\n\n    threat_scan_reports = await scanner.query_by_report_ids(report_ids=report_ids)\n    return threat_scan_reports\n\n\ndef maybe_monkeypatch_itertools_batched():\n    # monkeypatch itertools on python < 3.12\n    # This is required for python versions before 3.12, since itertools.batched was\n    # added in python 3.12, and is required for the above functions to work.\n    if sys.version_info.minor < 12:\n\n        def batched(iterable, n, *, strict=False):\n            if n < 1:\n                raise ValueError(\"n must be at least one\")\n            iterator = iter(iterable)\n            while batch := tuple(itertools.islice(iterator, n)):\n                if strict and len(batch) != n:\n                    raise ValueError(\"batched(): incomplete batch\")\n                yield batch\n\n        setattr(itertools, \"batched\", batched)\n\n\nif __name__ == \"__main__\":\n    pan_init()\n    maybe_monkeypatch_itertools_batched()\n    asyncio.run(mcp.run_async())\n```\n\n<a id=\"error-handling--exceptions\" href=\"#error-handling--exceptions\">\n\n# Error Handling & Exceptions\n\n</a>\n\nWhen the client is unable to fetch the expected response from the API server, a\nsubclass of `aisecurity.exceptions.AISecSDKException` is raised.\n\nThere are five types of Exceptions defined in `aisecurity/exceptions.py`:\n\n- **AISEC_SERVER_SIDE_ERROR**: Errors returned by the API server. For example, an invalid API key.\n- **AISEC_CLIENT_SIDE_ERROR**: Errors that occur on the client side. For example, a network connection issue.\n- **AISEC_USER_REQUEST_PAYLOAD_ERROR**: Errors related to the user's request payload. For example, an empty scan object.\n- **AISEC_MISSING_VARIABLE**: Errors related to missing variables. For example, missing API key environment variable.\n- **AISEC_SDK_ERROR**: Other uncategorized errors that occur in the SDK.\n\n<a id=\"compatibility-policy\" href=\"#compatibility-policy\">\n\n# Compatability Policy\n\n</a>\n\nThis package generally follows [SemVer v2](https://semver.org/spec/v2.0.0.html)\nconventions, though certain backwards-incompatible changes may be released as\nminor versions:\n\n1. Changes that only affect static types, without breaking runtime behavior.\n2. Changes to library internals which are technically public but not intended or\n   documented for external use.\n3. Changes that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely\non a smooth upgrade experience. The major version number will be consistent with\nAPI major version.\n\n<a id=\"legal\" href=\"#legal\">\n\n# Legal\n\n</a>\n\nCopyright (c) 2025, Palo Alto Networks\n\nLicensed under the [Polyform Internal Use License 1.0.0](https://polyformproject.org/licenses/internal-use/1.0.0)\n(the \"License\"); you may not use this file except in compliance with the License.\n\nYou may obtain a copy of the License at:\n\nhttps://polyformproject.org/licenses/internal-use/1.0.0\n\n(or)\n\nhttps://github.com/polyformproject/polyform-licenses/blob/76a278c4/PolyForm-Internal-Use-1.0.0.md\n\nAs far as the law allows, the software comes as is, without any warranty\nor condition, and the licensor will not be liable to you for any damages\narising out of these terms or the use or nature of the software, under\nany kind of legal claim.\n\n<!---Protected_by_PANW_Code_Armor_2024 - Y3ByfC9haWZ3L3B5dGhvbi1haXNlY3wxODI3MXxtYWlu --->\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "AI Runtime Security, AI Security, Palo Alto Networks, PaloAltoNetworks", "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "pan-aisecurity", "package_url": "https://pypi.org/project/pan-aisecurity/", "platform": null, "project_url": "https://pypi.org/project/pan-aisecurity/", "project_urls": {"Documentation": "https://pan.dev/ai-runtime-security/scan/api/", "Homepage": "https://www.paloaltonetworks.com/prisma/prisma-ai-runtime-security"}, "provides_extra": ["docs", "mcp", "test"], "release_url": "https://pypi.org/project/pan-aisecurity/0.5.0.5.post2/", "requires_dist": ["aiodns>=1.1", "aiohttp-retry~=2.9", "aiohttp~=3.11", "arrow~=1.3", "pydantic>=2", "python-dateutil~=2.8", "singleton-decorator~=1.0", "typing-extensions~=4.7", "urllib3~=2.2", "readme-renderer[md]>=44.0; extra == \"docs\"", "fastmcp>=2.5.2; python_full_version >= \"3.10\" and extra == \"mcp\"", "mcp>=1.9.2; python_full_version >= \"3.10\" and extra == \"mcp\"", "coverage-enable-subprocess>=1.0; extra == \"test\"", "coverage[toml]; extra == \"test\"", "mypy>=1.5; extra == \"test\"", "pyright[nodejs]>=1.1.401; extra == \"test\"", "pytest-asyncio>=1.0.0; extra == \"test\"", "pytest-cov>=6.0; extra == \"test\"", "pytest-mock>=3.14.1; extra == \"test\"", "pytest-randomly>=3.16.0; extra == \"test\"", "pytest-rerunfailures>=15.1; extra == \"test\"", "pytest-xdist[psutil]>=3.7.0; extra == \"test\"", "pytest>=8.3.5; extra == \"test\""], "requires_python": ">=3.9", "summary": "Palo Alto Networks AI Runtime Security: API Intercept Python SDK", "version": "0.5.0.5.post2", "yanked": false, "yanked_reason": null}, "last_serial": 30388248, "urls": [{"comment_text": null, "digests": {"blake2b_256": "46333186de60d0a964d25a88da22b8667e92a69d5d25c42954dc7498664468d6", "md5": "614fe94fa757b61a9708f6effc01f081", "sha256": "ae75e359c3e9f3a821d08dfea21756fb22cc4c139bb1c74fadd976d4f32aa98e"}, "downloads": -1, "filename": "pan_aisecurity-0.5.0.5.post2-py3-none-any.whl", "has_sig": false, "md5_digest": "614fe94fa757b61a9708f6effc01f081", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 143414, "upload_time": "2025-07-28T17:55:34", "upload_time_iso_8601": "2025-07-28T17:55:34.854977Z", "url": "https://files.pythonhosted.org/packages/46/33/3186de60d0a964d25a88da22b8667e92a69d5d25c42954dc7498664468d6/pan_aisecurity-0.5.0.5.post2-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "fe9e589d4ecec37f05f2c58ff4686c127525e9a49a695d625685435a2e006edf", "md5": "a6f5ebbcdfa5996fc188ae7db2a85683", "sha256": "15ec3361788e44000bbc04aea1ed6e6ca6d98401f5a4778cc01f10270604bcc3"}, "downloads": -1, "filename": "pan_aisecurity-0.5.0.5.post2.tar.gz", "has_sig": false, "md5_digest": "a6f5ebbcdfa5996fc188ae7db2a85683", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 70923, "upload_time": "2025-07-28T17:55:36", "upload_time_iso_8601": "2025-07-28T17:55:36.280210Z", "url": "https://files.pythonhosted.org/packages/fe/9e/589d4ecec37f05f2c58ff4686c127525e9a49a695d625685435a2e006edf/pan_aisecurity-0.5.0.5.post2.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:05Z", "published": "Mon, 28 Jul 2025 17:54:59 GMT", "package": "orthoxml-tools", "version": "1.1.0", "json": {"info": {"author": null, "author_email": "Ali Yazdizadeh <aliyzd1379@gmail.com>", "bugtrack_url": null, "classifiers": [], "description": "# orthoxml-tools\n\nTools for working with OrthoXML files.\n\n## What is OrthoXML Format?\n\n> OrthoXML is a standard for sharing and exchanging orthology predictions. OrthoXML is designed broadly to allow the storage and comparison of orthology data from any ortholog database. It establishes a structure for describing orthology relationships while still allowing flexibility for database-specific information to be encapsulated in the same format.  \n> [OrthoXML](https://github.com/qfo/orthoxml/tree/main)\n\n# Installation\n\n```\npip install orthoxml-tools\n```\n\n# Usage\n\n```bash\northoxml-tools [options] <subcommand> [options]\n```\n\n> Note: Input OrthoXML files can be in plain text or compressed format. Both gzip (.gz) and bzip2 (.bz2) compression are supported.\n\n## Subcommands\n\n### \ud83d\udee0\ufe0f **validate**\nValidate an OrthoXML file against the schema version specified in the file itself.\n\n```bash\northoxml-tools validate --infile path/to/file.orthoxml\n```\n\n**Options:**\n- `--infile <file>`: Specify the input file (required).\n\n**Example:**\n```bash\northoxml-tools validate --infile examples/data/ex1.orthoxml\n```\n\n### \ud83d\udee0\ufe0f **stats**\nDisplay basic statistics.\n\n```bash\northoxml-tools stats --infile path/to/file.orthoxml [--outfile <file>] \n```\n\n**Options:**\n- `--infile <file>`: Specify the input file (required).\n\n**Example:**\n```bash\northoxml-tools stats --infile examples/data/ex3-int-taxon.orthoxml\n```\n\n### \ud83d\udee0\ufe0f **gene-stats**\nDisplay statistics for gene count per taxon.\n\n```bash\northoxml-tools gene-stats --infile path/to/file.orthoxml [--outfile <file>]\n```\n\n**Options:**\n- `--infile <file>`: Specify the input file (required).\n- `--outfile <file>`: Write stats to a txt file.\n\n**Example:**\n```bash\northoxml-tools gene-stats --infile examples/data/ex3-int-taxon.orthoxml --outfile gene_stats.txt\n```\n\n### \ud83d\udee0\ufe0f **filter**\nFilter orthology groups based on CompletenessScore score and a threshold and strategy.\n\n```bash\northoxml-tools filter --infile path/to/file.orthoxml --threshold <value> --strategy <cascade-remove|extract|reparent> --outfile <file>\n```\n\n**Options:**\n- `--infile <file>`: Specify the input file. (required)\n- `--threshold <value>`: Set the threshold for filtering. value below this will be removed. (required)\n- `--strategy <cascade-remove|extract|reparent>`: Choose the filtering strategy (default is `cascade-remove`).\n- `--outfile <file>`: Save output to a file. if not specified, the output will be printed to stdout. (required)\n\n\n**Examples:**\n```bash\n orthoxml-tools filter --infile examples/data/sample-for-filter.orthoxml --score-name CompletenessScore --strategy top-down --threshold 0.24 --outfile tests_output/filtered_stream.orthoxml\n```\n\n### \ud83d\udee0\ufe0f **taxonomy**\nPrint a human-readable taxonomy tree from the OrthoXML file.\n\n```bash\northoxml-tools taxonomy --infile path/to/file.orthoxml\n```\n\n**Example:**\n```bash\n>>> orthoxml-tools taxonomy --infile examples/data/ex3-int-taxon.orthoxml\nRoot\n\u251c\u2500\u2500 Mus musculus\n\u2514\u2500\u2500 Primates\n    \u251c\u2500\u2500 Homo sapiens\n    \u2514\u2500\u2500 Pan troglodytes\n```\n\n### \ud83d\udee0\ufe0f **export-pairs**\nExport pairs (orthologs or paralogs) in TSV form, with configurable chunking and buffering.\n\n```bash\northoxml-tools export-pairs <ortho|para> \\\n    --infile <file> \\\n    --outfile <file> \\\n    [--id <tag>] \\\n    [--chunk-size <number>] \\\n    [--buffer-size <bytes>]\n```\n\n**Positional arguments:**\n<ortho|para>\nChoose which pair type to export:\n- `ortho`: orthologous pairs\n- `para`: paralogous pairs\n\n**Options:**\n- `--infile <file>`: Input OrthoXML file (required).\n- `--outfile <file>`: Write output CSV to this file (required).\n- `--id <tag>`: Gene attribute to use as identifier (default: id).\n- `--chunk-size <number>`: Number of pairs to process per chunk (default: 20_000).\n- `--buffer-size <bytes>`: I/O buffer size in bytes (default: 4194304).\n\n**Examples:**\n\n```bash\n# [5.1] Export ortholog pairs with default chunk & buffer sizes\northoxml-tools export-pairs ortho \\\n    --infile examples/data/ex1-int-taxon.orthoxml \\\n    --outfile orthos.csv\n\n# [5.2] Export paralog pairs with default chunk & buffer sizes\northoxml-tools export-pairs para \\\n    --infile examples/data/ex1-int-taxon.orthoxml \\\n    --outfile paras.csv\n\n# [5.3] Export ortholog pairs using `geneId` as the identifier column\northoxml-tools export-pairs ortho \\\n    --infile examples/data/ex1-int-taxon.orthoxml \\\n    --outfile orthos_geneid.csv \\\n    --id geneId\n\n# [5.4] Export ortholog pairs with custom chunk and buffer sizes\northoxml-tools export-pairs ortho \\\n    --infile examples/data/ex1-int-taxon.orthoxml \\\n    --outfile orthos_custom.csv \\\n    --chunk-size 5000 \\\n    --buffer-size 1048576\n```\n\n\n### \ud83d\udee0\ufe0f **export-ogs**\nExport Orthologous Groups as TSV file.\n\n```bash\northoxml-tools export-ogs --infile path/to/file.orthoxml --outfile path/to/output.tsv [--id <tag>]\n```\n\n**Options:**\n- `--infile <file>`: Input OrthoXML file (required).\n- `--outfile <file>`: Write output CSV to this file (required).\n- `--id <tag>`: Gene attribute to use as identifier (default: id).\n\n**Examples:**\n```bash\northoxml-tools export-ogs --infile examples/data/sample-for-og.orthoxml --outfile tests_output/ogs.tsv --id protId\n```\n\n### \ud83d\udee0\ufe0f **split**\nSplit the tree into multiple trees based on rootHOGs.\n\n```bash\northoxml-tools split --infile path/to/file.orthoxml --outdir path/to/output_folder\n```\n\n**Options:**\n- `--infile <file>`: Specify the input OrthoXML file (required).\n- `--outdir <folder>`: Specify the output folder where the trees will be saved.\n- \n**Examples:**\n```bash\northoxml-tools split --infile examples/data/ex4-int-taxon-multiple-rhogs.orthoxml --outdir tests_output/splits\n```\n\n## File Conversions\n\n### \ud83d\udee0\ufe0f **OrthoXML to Newick Tree (NHX)**\nConvert OrthoXML to Newick (NHX) format.\n\n```bash\northoxml-tools to-nhx --infile path/to/file.orthoxml --outdir path/to/output_folder --xref-tag [geneId,protId,...]    \n```\n\n**Options:**\n- `--infile <file>`: Specify the input OrthoXML file (required).\n- `--outdir <folder>`: Specify the output folder where the NHX files will be saved (required).\n- `--xref-tag <tag>`: Specify the attribute of the `<gene>` element to use as the label for the leaves. Default is `protId`.\n- `--encode-levels`: If set, encode group levels as NHX comments in the output tree. This is useful for visualizing the hierarchy of orthologous groups.\n  \n**Example:**\n```bash\northoxml-tools to-nhx --infile examples/data/sample-for-nhx.orthoxml --outdir ./tests_output/trees --xref-tag protId --encode-levels\n```\n\n### \ud83d\udee0\ufe0f **Newick Tree (NHX) to OrthoXML**\nConvert Newick (NHX) format to OrthoXML.\n\n```bash\northoxml-tools from-nhx --infile path/to/file.nhx --outfile path/to/file.orthoxml\n```\n\n**Options:**\n- `--infile <file>`: Specify the input nhx file or files. (at least one file is required).\n  - You can specify multiple files by providing them as a space-separated list.\n  - If you provide multiple files, they will be combined into a single OrthoXML output.\n- `--outfile <folder>`: Specify the output OrthoXML file (required).\n\n**Example:**\n```bash\northoxml-tools from-nhx --infile examples/data/sample.nhx --outfile ./tests_output/from_nhx.orthoxml\northoxml-tools from-nhx --infile examples/data/sample2.nhx examples/data/sample.nhx --outfile ./tests_output/from_nhx21.orthoxml \n```\n\n### \ud83d\udee0\ufe0f CSV to OrthoXML (exploratory feature)\nConvert a CSV file to OrthoXML. The CSV file is structured such that each row represents an orthogroup (OG), each column corresponds to a species, and each cell contains a gene name. This format is generated by OrthoFinder e.g. `examples/data/InputOrthogroups.csv`.\n\n> [!WARNING]\n> Note that since the CSV does not contain the full information required to represent the hierarchical structure of HOGs, the output OrthoXML file is reported at the root level. It should not be considered a full-fledged OrthoXML file.\n\n```bash\northoxml-tools from-csv --infile path/to/file.csv --outfile path/to/file.orthoxml\n```\n\n**Options:**\n- `--infile <file>`: Specify the input orthogroups.csv file (required).\n- `--outfile <folder>`: Specify the output OrthoXML file (required).\n\n**Example:**\n```bash\northoxml-tools from-csv --infile examples/data/InputOrthogroups.csv --outfile tests_output/orthofinder.orthoxml\n```\n\n\n### \ud83d\udee0\ufe0f **filter**\nFilter the OrthoXML tree by a completeness score. \n\n- `--score-name <str>`: Name of the field for completeness score annotation (e.g. 'CompletenessScore') \n- `--threshold <float>`: Threshold value for the completeness score\n- `--strategy <bottomup|topdown>`: Filtering strategy. Bottom-up will keep complete subHOGs even if they parents are incomplete.\n- `--outfile <file>`: If provided, write the filtered OrthoXML to this file; otherwise, print to stdout\n\n```bash\northoxml-tools tests/test-data/case_filtering.orthoxml filter --score-name CompletenessScore \\\n                                                        --threshold 0.75 \\\n                                                        --strategy bottomup \\\n                                                        --outfile output-oxml.orthoxml \n```\n\n### **Help**\nTo see help for any command:\n\n```bash\northoxml-tools --help\northoxml-tools -h\northoxml-tools stats --help\northoxml-tools stats -h\n```\n\n## Legacy API\n\nThe `orthoxml-tools` package used to provides a object oriented interface for working with OrthoXML files. This API is deprecated and will be removed in v1.0.0. Please use the new streaming CLI method. The documentation on it can be found [here](LEGACY-README.md).\n\n## Testing\n\n```\nuv install `.[test]`\npytest -vv\n\n# test cli\ntests/test_cli.sh\n```\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": null, "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "orthoxml-tools", "package_url": "https://pypi.org/project/orthoxml-tools/", "platform": null, "project_url": "https://pypi.org/project/orthoxml-tools/", "project_urls": null, "provides_extra": ["test"], "release_url": "https://pypi.org/project/orthoxml-tools/1.1.0/", "requires_dist": ["dendropy>=5.0.8", "lxml>=5.3.0", "pytest-cov>=3.0.0; extra == \"test\"", "pytest>=7.0.0; extra == \"test\""], "requires_python": ">=3.9", "summary": "Tools for working with OrthoXML files.", "version": "1.1.0", "yanked": false, "yanked_reason": null}, "last_serial": 30388401, "urls": [{"comment_text": null, "digests": {"blake2b_256": "c43931a1e24328bea386bae720f63821fafcad6fb5b1db56fc41f5273d7d89f1", "md5": "8831eb2cc272881276a3ed27d3a2424e", "sha256": "78527fc380d1ab40ecfc0677c14c7568c6cd134d8a180e48ef45b75bab1bd804"}, "downloads": -1, "filename": "orthoxml_tools-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8831eb2cc272881276a3ed27d3a2424e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 62525, "upload_time": "2025-07-28T17:54:59", "upload_time_iso_8601": "2025-07-28T17:54:59.203368Z", "url": "https://files.pythonhosted.org/packages/c4/39/31a1e24328bea386bae720f63821fafcad6fb5b1db56fc41f5273d7d89f1/orthoxml_tools-1.1.0-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "7772b32d81b8255c7533752c0d89dfd83e8cb769444d77201baf9076f3ebe4b3", "md5": "73b395c17ed522d8f5746ce85960ce7a", "sha256": "5186e5042b246498eb4abfed558c369a2cce48ade44396a9c12f00c7661f76b3"}, "downloads": -1, "filename": "orthoxml_tools-1.1.0.tar.gz", "has_sig": false, "md5_digest": "73b395c17ed522d8f5746ce85960ce7a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 15043881, "upload_time": "2025-07-28T17:55:01", "upload_time_iso_8601": "2025-07-28T17:55:01.054187Z", "url": "https://files.pythonhosted.org/packages/77/72/b32d81b8255c7533752c0d89dfd83e8cb769444d77201baf9076f3ebe4b3/orthoxml_tools-1.1.0.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:05Z", "published": "Mon, 28 Jul 2025 17:54:41 GMT", "package": "pageai-sdk", "version": "0.6.0.166", "json": {"info": {"author": "OpenAPI Generator community", "author_email": "team@openapitools.org", "bugtrack_url": null, "classifiers": [], "description": "    # Introduction The PageAI (short for Synthetic EPUB) API is capapble of transforming multi page image only PDF files into accessible EPUBs.   # noqa: E501\n    \n", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": "OpenAPI, OpenAPI-Generator, PageAI API", "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "pageai-sdk", "package_url": "https://pypi.org/project/pageai-sdk/", "platform": null, "project_url": "https://pypi.org/project/pageai-sdk/", "project_urls": null, "provides_extra": null, "release_url": "https://pypi.org/project/pageai-sdk/0.6.0.166/", "requires_dist": ["urllib3>=1.15", "six>=1.10", "certifi", "python-dateutil"], "requires_python": null, "summary": "PageAI API", "version": "0.6.0.166", "yanked": false, "yanked_reason": null}, "last_serial": 30388472, "urls": [{"comment_text": null, "digests": {"blake2b_256": "81c70f982cb2bd396a18243c43c65413f3092f0fbb5df3f19e788bdd4bb93638", "md5": "28fb71a253c8efc19e2ba67ca4560593", "sha256": "694be7c7b0a0cbc3da0c0cb94147bcfb1b0bc1be079ecb1de17e49f3e34bbb04"}, "downloads": -1, "filename": "pageai_sdk-0.6.0.166-py3-none-any.whl", "has_sig": false, "md5_digest": "28fb71a253c8efc19e2ba67ca4560593", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 20193, "upload_time": "2025-07-28T17:54:41", "upload_time_iso_8601": "2025-07-28T17:54:41.030335Z", "url": "https://files.pythonhosted.org/packages/81/c7/0f982cb2bd396a18243c43c65413f3092f0fbb5df3f19e788bdd4bb93638/pageai_sdk-0.6.0.166-py3-none-any.whl", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:05Z", "published": "Mon, 28 Jul 2025 17:53:25 GMT", "package": "kimina-client", "version": "0.1.6", "json": {"info": {"author": null, "author_email": "Kimi Team - Project Numina <contact@projectnumina.com>", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Kimina client\n\nClient SDK to interact with Kimina Lean server. \n\nExample use:\n```python\nfrom kimina_client import KiminaClient\n\n# Specify LEAN_SERVER_API_KEY in your .env or pass `api_key`.\n# Default `api_url` is https://projectnumina.ai\nclient = KiminaClient()\n\n# If running locally use:\n# client = KiminaClient(api_url=\"http://localhost:80\")\n\nclient.check(\"#check Nat\")\n```\n\n## Backward client\n\n```python\nfrom kimina_client import Lean4Client\n\nclient = Lean4Client()\n\nclient.verify(\"#check Nat\")\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": null, "home_page": null, "keywords": null, "license": null, "license_expression": "MIT", "license_files": ["LICENSE"], "maintainer": null, "maintainer_email": null, "name": "kimina-client", "package_url": "https://pypi.org/project/kimina-client/", "platform": null, "project_url": "https://pypi.org/project/kimina-client/", "project_urls": {"Homepage": "https://github.com/project-numina/kimina-lean-server", "Issues": "https://github.com/project-numina/kimina-lean-server/issues"}, "provides_extra": ["server"], "release_url": "https://pypi.org/project/kimina-client/0.1.6/", "requires_dist": ["colorama>=0.4.6", "datasets>=4.0.0", "httpx>=0.28.1", "loguru>=0.7.3", "pip>=25.1.1", "pydantic-settings>=2.10.0", "python-dotenv>=1.1.0", "requests>=2.31.0", "tabulate>=0.9.0", "tenacity>=9.1.2", "fastapi>=0.115.13; extra == \"server\"", "google-cloud-logging>=3.12.1; extra == \"server\"", "prisma>=0.15.0; extra == \"server\"", "psutil>=7.0.0; extra == \"server\"", "rich>=14.0.0; extra == \"server\"", "uvicorn>=0.34.3; extra == \"server\""], "requires_python": ">=3.9", "summary": "Client SDK to interact with Kimina Lean server.", "version": "0.1.6", "yanked": false, "yanked_reason": null}, "last_serial": 30388419, "urls": [{"comment_text": null, "digests": {"blake2b_256": "d81b0eb390c420df142626eb746e2b502509eeffd26a0787049203d43d24ccc5", "md5": "2a8622fc7eebc3ae38d19dfdb7719ab2", "sha256": "f652006bec7e2169311c0551400715fd18f47a958cda47094b29850c72d31f94"}, "downloads": -1, "filename": "kimina_client-0.1.6-py3-none-any.whl", "has_sig": false, "md5_digest": "2a8622fc7eebc3ae38d19dfdb7719ab2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.9", "size": 28870, "upload_time": "2025-07-28T17:53:25", "upload_time_iso_8601": "2025-07-28T17:53:25.962436Z", "url": "https://files.pythonhosted.org/packages/d8/1b/0eb390c420df142626eb746e2b502509eeffd26a0787049203d43d24ccc5/kimina_client-0.1.6-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "c5ea6904c09a83a95ffebba927f098413cfad25bf5d1b1da96d4d9cd4ccdb5e9", "md5": "706d82327e657d1e814f5538e5f15b57", "sha256": "5ce34a2e065c954b1da8a7e0a049da44bb8a35086fa8cc3f7aeb1122e6d175a9"}, "downloads": -1, "filename": "kimina_client-0.1.6.tar.gz", "has_sig": false, "md5_digest": "706d82327e657d1e814f5538e5f15b57", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.9", "size": 25163, "upload_time": "2025-07-28T17:53:26", "upload_time_iso_8601": "2025-07-28T17:53:26.884444Z", "url": "https://files.pythonhosted.org/packages/c5/ea/6904c09a83a95ffebba927f098413cfad25bf5d1b1da96d4d9cd4ccdb5e9/kimina_client-0.1.6.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:05Z", "published": "Mon, 28 Jul 2025 17:53:20 GMT", "package": "tinybird", "version": "0.0.1.dev272", "json": {"info": {"author": "Tinybird", "author_email": "support@tinybird.co", "bugtrack_url": null, "classifiers": [], "description": "Tinybird CLI\n=============\n\nThe Tinybird command-line tool allows you to use all the Tinybird functionality directly from the command line. Additionally, it includes several functions to create and manage data projects easily.\n\nChangelog\n----------\n\n0.0.1dev1\n***********\n\n* Initial release of the Tinybird CLI\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Description", "Description-Content-Type", "Home-Page", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "https://www.tinybird.co/docs/forward/commands", "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "tinybird", "package_url": "https://pypi.org/project/tinybird/", "platform": null, "project_url": "https://pypi.org/project/tinybird/", "project_urls": {"Homepage": "https://www.tinybird.co/docs/forward/commands"}, "provides_extra": null, "release_url": "https://pypi.org/project/tinybird/0.0.1.dev272/", "requires_dist": ["aiofiles==24.1.0", "anthropic==0.55.0", "boto3", "click<8.2,>=8.1.6", "clickhouse-toolset==0.34.dev0", "colorama==0.4.6", "confluent-kafka==2.8.0", "cryptography~=41.0.0", "croniter==1.3.15", "docker==7.1.0", "GitPython~=3.1.32", "humanfriendly~=8.2", "plotext==5.3.2", "prompt_toolkit==3.0.48", "pydantic~=2.11.7", "pydantic-ai-slim[anthropic]~=0.4.2", "pyperclip==1.8.2", "pyyaml<6.1,>=6.0", "requests<3,>=2.28.1", "shandy-sqlfmt==0.11.1", "shandy-sqlfmt[jinjafmt]==0.11.1", "toposort==1.10", "tornado~=6.0.0", "urllib3<2,>=1.26.14", "watchdog==6.0.0", "wheel", "packaging<24,>=23.1", "llm>=0.19", "thefuzz==0.22.1", "python-dotenv==1.1.0"], "requires_python": "<3.14,>=3.9", "summary": "Tinybird Command Line Tool", "version": "0.0.1.dev272", "yanked": false, "yanked_reason": null}, "last_serial": 30388228, "urls": [{"comment_text": null, "digests": {"blake2b_256": "84e32c862bdfcd478c61d839db6faee3129a0527a5a95bfff779435c248e3cc0", "md5": "7541c99371c436c7bfa3c0c1617011f7", "sha256": "48d246bc7d1e3514396960c632a6b6c88cea557f9f9b238a949f7048fc3e9482"}, "downloads": -1, "filename": "tinybird-0.0.1.dev272-py3-none-any.whl", "has_sig": false, "md5_digest": "7541c99371c436c7bfa3c0c1617011f7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<3.14,>=3.9", "size": 435391, "upload_time": "2025-07-28T17:53:20", "upload_time_iso_8601": "2025-07-28T17:53:20.369328Z", "url": "https://files.pythonhosted.org/packages/84/e3/2c862bdfcd478c61d839db6faee3129a0527a5a95bfff779435c248e3cc0/tinybird-0.0.1.dev272-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": null, "digests": {"blake2b_256": "7a46665d94753bb2b0bd8f3d0fd608f896ff0303290f95d56599fd6559e2a035", "md5": "78c2d740a4e20a791a9c7a5e6baf55d4", "sha256": "f8d7965ad4476844e104ad5352d19782d5a3055def2e34de48b756c4fe6c628f"}, "downloads": -1, "filename": "tinybird-0.0.1.dev272.tar.gz", "has_sig": false, "md5_digest": "78c2d740a4e20a791a9c7a5e6baf55d4", "packagetype": "sdist", "python_version": "source", "requires_python": "<3.14,>=3.9", "size": 440396, "upload_time": "2025-07-28T17:53:22", "upload_time_iso_8601": "2025-07-28T17:53:22.737160Z", "url": "https://files.pythonhosted.org/packages/7a/46/665d94753bb2b0bd8f3d0fd608f896ff0303290f95d56599fd6559e2a035/tinybird-0.0.1.dev272.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:05Z", "published": "Mon, 28 Jul 2025 17:52:55 GMT", "package": "cirq", "version": "1.7.0.dev20250728175235", "json": {"info": {"author": "The Cirq Developers", "author_email": "cirq-dev@googlegroups.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Operating System :: MacOS :: MacOS X", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Topic :: Scientific/Engineering :: Quantum Computing", "Topic :: Software Development :: Libraries :: Python Modules", "Typing :: Typed"], "description": "<!-- H1 title omitted because our logo acts as the title. -->\n<div align=\"center\">\n\n<img width=\"300px\" alt=\"Cirq logo\" src=\"https://raw.githubusercontent.com/quantumlib/Cirq/refs/heads/main/docs/images/Cirq_logo_color.svg\">\n\nPython package for writing, manipulating, and running [quantum\ncircuits](https://en.wikipedia.org/wiki/Quantum_circuit) on quantum computers\nand simulators.\n\n[![Licensed under the Apache 2.0\nlicense](https://img.shields.io/badge/License-Apache%202.0-3c60b1.svg?logo=opensourceinitiative&logoColor=white&style=flat-square)](https://github.com/quantumlib/Cirq/blob/main/LICENSE)\n[![Compatible with Python versions 3.11 and\nhigher](https://img.shields.io/badge/Python-3.11+-fcbc2c.svg?style=flat-square&logo=python&logoColor=white)](https://www.python.org/downloads/)\n[![OpenSSF Best Practices](https://img.shields.io/badge/dynamic/json?label=OpenSSF&logo=springsecurity&logoColor=white&style=flat-square&colorA=gray&colorB=d56420&suffix=%25&query=$.badge_percentage_0&uri=https://bestpractices.coreinfrastructure.org/projects/10063.json)](https://www.bestpractices.dev/projects/10063)\n[![Cirq project on\nPyPI](https://img.shields.io/pypi/v/cirq.svg?logo=python&logoColor=white&label=PyPI&style=flat-square&color=fcbc2c)](https://pypi.org/project/cirq)\n[![Archived in\nZenodo](https://img.shields.io/badge/10.5281%2Fzenodo.4062499-gray.svg?label=DOI&logo=doi&logoColor=white&style=flat-square&colorA=gray&colorB=3c60b1)](https://doi.org/10.5281/zenodo.4062499)\n\n[Features](#features) &ndash;\n[Installation](#installation) &ndash;\n[Quick Start](#quick-start--hello-qubit-example) &ndash;\n[Documentation](#cirq-documentation) &ndash;\n[Integrations](#integrations) &ndash;\n[Community](#community) &ndash;\n[Citing Cirq](#citing-cirq) &ndash;\n[Contact](#contact)\n\n</div>\n\n## Features\n\nCirq provides useful abstractions for dealing with today\u2019s [noisy\nintermediate-scale quantum](https://arxiv.org/abs/1801.00862) (NISQ) computers,\nwhere the details of quantum hardware are vital to achieving state-of-the-art\nresults. Some of its features include:\n\n*   Flexible gate definitions and custom gates\n*   Parameterized circuits with symbolic variables\n*   Circuit transformation, compilation and optimization\n*   Hardware device modeling\n*   Noise modeling\n*   Multiple built-in quantum circuit simulators\n*   Integration with [qsim](https://github.com/quantumlib/qsim) for\n    high-performance simulation\n*   Interoperability with [NumPy](https://numpy.org) and\n    [SciPy](https://scipy.org)\n*   Cross-platform compatibility\n\n## Installation\n\nCirq supports Python version 3.11 and later, and can be used on Linux, MacOS,\nand Windows, as well as [Google Colab](https://colab.google). For complete\ninstallation instructions, please refer to the\n[Install](https://quantumai.google/cirq/start/install) section of the online\nCirq documentation.\n\n## Quick Start \u2013 \u201cHello Qubit\u201d Example\n\nHere is a simple example to get you up and running with Cirq after you have\ninstalled it. Start a Python interpreter, and then type the following:\n\n```python\nimport cirq\n\n# Pick a qubit.\nqubit = cirq.GridQubit(0, 0)\n\n# Create a circuit.\ncircuit = cirq.Circuit(\n    cirq.X(qubit)**0.5,  # Square root of NOT.\n    cirq.measure(qubit, key='m')  # Measurement.\n)\nprint(\"Circuit:\")\nprint(circuit)\n\n# Simulate the circuit several times.\nsimulator = cirq.Simulator()\nresult = simulator.run(circuit, repetitions=20)\nprint(\"Results:\")\nprint(result)\n```\n\nPython should then print output similar to this:\n\n```text\nCircuit:\n(0, 0): \u2500\u2500\u2500X^0.5\u2500\u2500\u2500M('m')\u2500\u2500\u2500\nResults:\nm=11000111111011001000\n```\n\nCongratulations! You have run your first quantum simulation in Cirq. You can\ncontinue to learn more by exploring the [many Cirq tutorials](#tutorials)\ndescribed below.\n\n## Cirq Documentation\n\nThe primary documentation site for Cirq is the [Cirq home page on the Quantum\nAI website](https://quantumai.google/cirq). There and elsewhere, a variety of\ndocumentation for Cirq is available.\n\n### Tutorials\n\n*   [Video tutorials] on YouTube are an engaging way to learn Cirq.\n*   [Jupyter notebook-based tutorials] let you learn Cirq from your browser \u2013 no\n    installation needed.\n*   [Text-based tutorials] on the Cirq home page are great when combined with a\n    local [installation] of Cirq on your computer. After starting with the\n    [basics], you'll be ready to dive into tutorials on circuit building and\n    circuit simulation under the [Build] and [Simulate] tabs, respectively. Check\n    out the other tabs for more!\n\n[Video tutorials]: https://www.youtube.com/playlist?list=PLpO2pyKisOjLVt_tDJ2K6ZTapZtHXPLB4\n[Jupyter notebook-based tutorials]: https://colab.research.google.com/github/quantumlib/Cirq\n[Text-based tutorials]: https://quantumai.google/cirq\n[installation]: https://quantumai.google/cirq/start/install\n[basics]: https://quantumai.google/cirq/start/basics\n[Build]: https://quantumai.google/cirq/build\n[Simulate]: https://quantumai.google/cirq/simula\n\n### Reference Documentation\n\n*   Docs for the [current stable release] correspond to what you get with\n    `pip install cirq`.\n*   Docs for the [pre-release] correspond to what you get with\n    `pip install --upgrade cirq~=1.0.dev`.\n\n[current stable release]: https://quantumai.google/reference/python/cirq/all_symbols\n[pre-release]: https://quantumai.google/reference/python/cirq/all_symbols?version=nightly\n\n### Examples\n\n*   The [examples subdirectory](./examples/) of the Cirq GitHub repo has many\n    programs illustrating the application of Cirq to everything from common\n    textbook algorithms to more advanced methods.\n*   The [Experiments page](https://quantumai.google/cirq/experiments/) on the\n    Cirq documentation site has yet more examples, from simple to advanced.\n\n### Change log\n\n*   The [Cirq releases](https://github.com/quantumlib/cirq/releases) page on\n    GitHub lists the changes in each release.\n\n## Integrations\n\nGoogle Quantum AI has a suite of open-source software that lets you do more\nwith Cirq. From high-performance simulators, to novel tools for expressing and\nanalyzing fault-tolerant quantum algorithms, our software stack lets you\ndevelop quantum programs for a variety of applications.\n\n<div align=\"center\">\n\n| Your interests                                  | Software to explore  |\n|-------------------------------------------------|----------------------|\n| Quantum algorithms?<br>Fault-tolerant quantum computing (FTQC)? | [Qualtran] |\n| Large circuits and/or a lot of simulations?     | [qsim] |\n| Circuits with thousands of qubits and millions of Clifford operations? | [Stim] |\n| Quantum error correction (QEC)?                 | [Stim] |\n| Chemistry and/or material science?              | [OpenFermion]<br>[OpenFermion-FQE]<br>[OpenFermion-PySCF]<br>[OpenFermion-Psi4] |\n| Quantum machine learning (QML)?                 | [TensorFlow Quantum] |\n| Real experiments using Cirq?                    | [ReCirq] |\n\n</div>\n\n[Qualtran]: https://github.com/quantumlib/qualtran\n[qsim]: https://github.com/quantumlib/qsim\n[Stim]: https://github.com/quantumlib/stim\n[OpenFermion]: https://github.com/quantumlib/openfermion\n[OpenFermion-FQE]: https://github.com/quantumlib/OpenFermion-FQE\n[OpenFermion-PySCF]: https://github.com/quantumlib/OpenFermion-PySCF\n[OpenFermion-Psi4]: https://github.com/quantumlib/OpenFermion-Psi4\n[TensorFlow Quantum]: https://github.com/tensorflow/quantum\n[ReCirq]: https://github.com/quantumlib/ReCirq\n\n## Community\n\n<a href=\"https://github.com/quantumlib/Cirq/graphs/contributors\"><img\nwidth=\"150em\" alt=\"Total number of contributors to Cirq\"\nsrc=\"https://img.shields.io/github/contributors/quantumlib/cirq?label=Contributors&logo=github&color=ccc&style=flat-square\"/></a>\n\nCirq has benefited from [contributions] by over 200 people and\ncounting. We are dedicated to cultivating an open and inclusive community to\nbuild software for quantum computers, and have a community [code of conduct].\n\n[contributions]: https://github.com/quantumlib/Cirq/graphs/contributors\n[code of conduct]: https://github.com/quantumlib/cirq/blob/main/CODE_OF_CONDUCT.md\n\n### Announcements\n\nStay on top of Cirq developments using the approach that best suits your needs:\n\n*   For releases and major announcements: sign up to the low-volume mailing list\n    [`cirq-announce`].\n*   For releases only:\n    *   Via GitHub notifications: configure [repository notifications] for Cirq.\n    *   Via Atom/RSS from GitHub: subscribe to the GitHub [Cirq releases Atom feed].\n    *   Via RSS from PyPI: subscribe to the [PyPI releases RSS feed] for Cirq.\n\nCirq releases take place approximately every quarter.\n\n[`cirq-announce`]: https://groups.google.com/forum/#!forum/cirq-announce\n[repository notifications]: https://docs.github.com/github/managing-subscriptions-and-notifications-on-github/configuring-notifications\n[Cirq releases Atom feed]: https://github.com/quantumlib/Cirq/releases.atom\n[PyPI releases RSS feed]: https://pypi.org/rss/project/cirq/releases.xml\n\n### Questions and Discussions\n\n*   Have questions about Cirq? Post them to the [Quantum Computing\n    Stack Exchange] and tag them with [`cirq`]. You can also search past\n    questions using that tag \u2013 it's a great way to learn!\n*   Want meet other Cirq developers and participate in discussions? Join\n    _Cirq Cynq_, our biweekly virtual meeting of contributors. Sign up\n    to [_cirq-dev_] to get an automatic meeting invitation!\n\n[Quantum Computing Stack Exchange]: https://quantumcomputing.stackexchange.com\n[`cirq`]: https://quantumcomputing.stackexchange.com/questions/tagged/cirq\n[_cirq-dev_]: https://groups.google.com/forum/#!forum/cirq-dev\n\n### Contributions\n\n*   Have a feature request or bug report? [Open an issue on GitHub]!\n*   Want to develop Cirq code? Look at the [list of good first issues] to\n    tackle, read our [contribution guidelines], and then start opening\n    [pull requests]!\n\n[Open an issue on GitHub]: https://github.com/quantumlib/Cirq/issues/new/choose\n[list of good first issues]: https://github.com/quantumlib/Cirq/contribute\n[contribution guidelines]: https://github.com/quantumlib/cirq/blob/main/CONTRIBUTING.md\n[pull requests]: https://help.github.com/articles/about-pull-requests\n\n## Citing Cirq<a name=\"how-to-cite-cirq\"></a><a name=\"how-to-cite\"></a>\n\nWhen publishing articles or otherwise writing about Cirq, please cite the Cirq\nversion you use \u2013 it will help others reproduce your results. We use Zenodo to\npreserve releases. The following links let you download the bibliographic\nrecord for the latest stable release of Cirq in some popular formats:\n\n<div align=\"center\">\n\n[![Download BibTeX bibliography record for latest Cirq\nrelease](https://img.shields.io/badge/Download%20record-e0e0e0.svg?style=flat-square&logo=LaTeX&label=BibTeX&labelColor=106f6e)](https://citation.doi.org/format?doi=10.5281/zenodo.4062499&style=bibtex)&nbsp;&nbsp;\n[![Download CSL JSON bibliography record for latest Cirq\nrelease](https://img.shields.io/badge/Download%20record-e0e0e0.svg?style=flat-square&label=CSL&labelColor=2d98e0&logo=json)](https://citation.doi.org/metadata?doi=10.5281/zenodo.4062499)\n\n</div>\n\nFor formatted citations and records in other formats, as well as records for\nall releases of Cirq past and present, please visit the [Cirq page on\nZenodo](https://doi.org/10.5281/zenodo.4062499).\n\n## Contact\n\nFor any questions or concerns not addressed here, please email\nquantum-oss-maintainers@google.com.\n\n## Disclaimer\n\nThis is not an officially supported Google product. This project is not\neligible for the [Google Open Source Software Vulnerability Rewards\nProgram](https://bughunters.google.com/open-source-security).\n\nCopyright 2019 The Cirq Developers.\n\n<div align=\"center\">\n  <a href=\"https://quantumai.google\">\n    <img width=\"15%\" alt=\"Google Quantum AI\"\n         src=\"https://raw.githubusercontent.com/quantumlib/Cirq/refs/heads/main/docs/images/quantum-ai-vertical.svg\">\n  </a>\n</div>\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "Keywords", "License", "License-File", "Maintainer", "Maintainer-Email", "Provides-Extra", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "http://github.com/quantumlib/cirq", "keywords": "algorithms, api, cirq, google, google quantum, nisq, python, quantum, quantum algorithms, quantum circuit, quantum circuit simulator, quantum computer simulator, quantum computing, quantum development kit, quantum information, quantum programming, quantum programming language, quantum simulation, sdk, simulation", "license": "Apache 2", "license_expression": null, "license_files": ["LICENSE", "AUTHORS"], "maintainer": "Google Quantum AI open-source maintainers", "maintainer_email": "quantum-oss-maintainers@google.com", "name": "cirq", "package_url": "https://pypi.org/project/cirq/", "platform": null, "project_url": "https://pypi.org/project/cirq/", "project_urls": {"Homepage": "http://github.com/quantumlib/cirq"}, "provides_extra": ["dev-env"], "release_url": "https://pypi.org/project/cirq/1.7.0.dev20250728175235/", "requires_dist": ["cirq-aqt==1.7.0.dev20250728175235", "cirq-core==1.7.0.dev20250728175235", "cirq-google==1.7.0.dev20250728175235", "cirq-ionq==1.7.0.dev20250728175235", "cirq-pasqal==1.7.0.dev20250728175235", "cirq-web==1.7.0.dev20250728175235", "mypy==1.16.1; extra == \"dev-env\"", "types-protobuf<6.0dev,>=5.26.1; extra == \"dev-env\"", "types-requests~=2.32; extra == \"dev-env\"", "pytest; extra == \"dev-env\"", "pytest-asyncio; extra == \"dev-env\"", "pytest-cov; extra == \"dev-env\"", "pytest-randomly; extra == \"dev-env\"", "coverage~=7.4; extra == \"dev-env\"", "pytest-xdist; extra == \"dev-env\"", "filelock~=3.1; extra == \"dev-env\"", "freezegun; extra == \"dev-env\"", "importlib-metadata; extra == \"dev-env\"", "codeowners; platform_system != \"Windows\" and extra == \"dev-env\"", "virtualenv~=20.23; extra == \"dev-env\"", "virtualenv-clone; extra == \"dev-env\"", "black[jupyter]==25.1.0; extra == \"dev-env\"", "isort[colors]~=6.0.1; extra == \"dev-env\"", "pylint~=3.3.1; extra == \"dev-env\"", "grpcio-tools~=1.71.2; extra == \"dev-env\"", "mypy-protobuf~=3.6; extra == \"dev-env\"", "typedunits; extra == \"dev-env\"", "ipython>=8.15; extra == \"dev-env\"", "notebook~=7.0; extra == \"dev-env\"", "ipykernel~=6.29; extra == \"dev-env\"", "papermill~=2.6; extra == \"dev-env\"", "ply>=3.6; extra == \"dev-env\"", "pylatex~=1.4; extra == \"dev-env\"", "quimb>=1.8; extra == \"dev-env\"", "opt_einsum; extra == \"dev-env\"", "seaborn~=0.12; extra == \"dev-env\"", "tensorflow-docs==2025.2.19.33219; extra == \"dev-env\"", "virtualenv; extra == \"dev-env\"", "setuptools>=70; extra == \"dev-env\"", "wheel; extra == \"dev-env\"", "twine; extra == \"dev-env\"", "ruff~=0.12.1; extra == \"dev-env\"", "asv; extra == \"dev-env\"", "qiskit-aer~=0.17.0; extra == \"dev-env\"", "stimcirq; extra == \"dev-env\""], "requires_python": ">=3.11.0", "summary": "A framework for creating, editing, and invoking Noisy Intermediate Scale Quantum (NISQ) circuits.", "version": "1.7.0.dev20250728175235", "yanked": false, "yanked_reason": null}, "last_serial": 30388224, "urls": [{"comment_text": null, "digests": {"blake2b_256": "dfec6395eb2f886d06ba8e93194703bb5aafc25ff163c66af46bf243b98efcc6", "md5": "b125d208d2dde6823012cee080b1839b", "sha256": "9ae9ce55f072b0ff096440d5908f1a9ff58b5a81ff85dbb826eb68de421227e3"}, "downloads": -1, "filename": "cirq-1.7.0.dev20250728175235-py3-none-any.whl", "has_sig": false, "md5_digest": "b125d208d2dde6823012cee080b1839b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.11.0", "size": 11326, "upload_time": "2025-07-28T17:52:55", "upload_time_iso_8601": "2025-07-28T17:52:55.167689Z", "url": "https://files.pythonhosted.org/packages/df/ec/6395eb2f886d06ba8e93194703bb5aafc25ff163c66af46bf243b98efcc6/cirq-1.7.0.dev20250728175235-py3-none-any.whl", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:05Z", "published": "Mon, 28 Jul 2025 17:52:54 GMT", "package": "bigsur", "version": "0.0.4", "json": {"info": {"author": "Emmanuel Dollinger", "author_email": "Emmanuel Dollinger <edolling@uci.edu>", "bugtrack_url": null, "classifiers": ["Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# BigSur\nBigSur is a package for principled, robust scRNAseq normalization. Currently we can perform feature selection, see [BigSurR](https://github.com/landerlabcode/BigSurR) for correlations.\n\n# What is BigSur?\nBasic Informatics and Gene Statistics from Unnormalized Reads (BigSur) is a principled pipeline allowing for feature selection, correlation and clustering in scRNAseq.\n* The feature selection derivations are detailed in [the BioRxiv preprint Dollinger et al. 2023](https://www.biorxiv.org/content/10.1101/2024.10.11.617709v1).\n* The correlation are detailed in [Silkwood et al. 2023](https://doi.org/10.1186/s12859-024-05926-z).\n\n\n# Installation\nThe only way to install BigSur currently is to clone the GitHub repo. We've included a environment file for [conda environment installation](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#building-identical-conda-environments); the only package we require that isn't installed with scanpy is [mpmath](https://github.com/mpmath/mpmath) and [numexpr](https://github.com/pydata/numexpr). For example:\n\nIn terminal:\n\n    cd bigsur_dir #directory to clone to\n\n    git clone https://github.com/landerlabcode/BigSur.git\n\n    conda create -f environment.yml -n bigsur\n\n## A note about the virtual environment\nThis environment contains all packages that are required to reproduce any result of the paper. If you want a lightweight conda enviroment (or alternatively, if the environment file is causing issues), you can create a sufficient conda environment as follows:\n\nIn terminal:\n\n    conda create -n bigsur -c conda-forge scanpy mpmath numexpr ipykernel python-igraph leidenalg\n\n# Usage\nUsage for feature selection is detailed in the [example notebook](https://github.com/landerlabcode/BigSur/blob/main/feature_selection_example_usage.ipynb). \n\nTL;DR:\n\n    import sys\n    \n    sys.path.append(bigsur_dir) # directory where git repo was cloned\n    \n    from BigSur.feature_selection import mcfano_feature_selection as mcfano\n\nReplace <code>sc.pp.highly_variable_genes(adata)</code> in your pipeline with <code>mcfano(adata, layer='counts')</code>, where the UMI counts are in <code>adata.layers['counts']</code>.\n\nAnd that's it! You can read more about how to use BigSur for feature selection, and in particular how to optimize cutoffs for a given dataset, in the [example notebook](https://github.com/landerlabcode/BigSur/blob/main/feature_selection_example_usage.ipynb). \n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Home-Page", "License-File", "Requires-Python"], "home_page": "https://github.com/landerlabcode/BigSur", "keywords": null, "license": null, "license_expression": null, "license_files": null, "maintainer": null, "maintainer_email": null, "name": "bigsur", "package_url": "https://pypi.org/project/bigsur/", "platform": null, "project_url": "https://pypi.org/project/bigsur/", "project_urls": {"Homepage": "https://github.com/landerlabcode/BigSur/", "Issues": "https://github.com/landerlabcode/BigSur/issues"}, "provides_extra": null, "release_url": "https://pypi.org/project/bigsur/0.0.4/", "requires_dist": null, "requires_python": "<4,>=3.9", "summary": "Basic Informatics and Gene Statistics from Unnormalized Reads, a feature selection tool for scRNAseq", "version": "0.0.4", "yanked": false, "yanked_reason": null}, "last_serial": 30388314, "urls": [{"comment_text": "", "digests": {"blake2b_256": "4c32f25fac8ad77e2903dda96e1504a5c95a608a6424030f0852b42f4cf2d4b4", "md5": "e7181fc42c2e48144cf5ad30de125761", "sha256": "d6579f22438a26ec391df6d7287cae1874ba4139edeff77f4df21ca5a8e5d9af"}, "downloads": -1, "filename": "bigsur-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "e7181fc42c2e48144cf5ad30de125761", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "<4,>=3.9", "size": 11429, "upload_time": "2025-07-28T17:52:54", "upload_time_iso_8601": "2025-07-28T17:52:54.042394Z", "url": "https://files.pythonhosted.org/packages/4c/32/f25fac8ad77e2903dda96e1504a5c95a608a6424030f0852b42f4cf2d4b4/bigsur-0.0.4-py3-none-any.whl", "yanked": false, "yanked_reason": null}, {"comment_text": "", "digests": {"blake2b_256": "ff1a05a2303623dad0010eb2c3c426fc4d6af37f6a6c79a789e1af2b9ef73be1", "md5": "35be9f9d8aa00c033a00501e2ecd29eb", "sha256": "dd58e0f3d1282dba53a1873df13dd8867af2b684e8dcbad824df96585180199c"}, "downloads": -1, "filename": "bigsur-0.0.4.tar.gz", "has_sig": false, "md5_digest": "35be9f9d8aa00c033a00501e2ecd29eb", "packagetype": "sdist", "python_version": "source", "requires_python": "<4,>=3.9", "size": 12443, "upload_time": "2025-07-28T17:52:55", "upload_time_iso_8601": "2025-07-28T17:52:55.761303Z", "url": "https://files.pythonhosted.org/packages/ff/1a/05a2303623dad0010eb2c3c426fc4d6af37f6a6c79a789e1af2b9ef73be1/bigsur-0.0.4.tar.gz", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
{"fetched_at": "2025-07-28T18:20:05Z", "published": "Mon, 28 Jul 2025 17:52:52 GMT", "package": "cirq-web", "version": "1.7.0.dev20250728175235", "json": {"info": {"author": "The Cirq Developers", "author_email": "cirq-dev@googlegroups.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Operating System :: MacOS :: MacOS X", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.11", "Programming Language :: Python :: 3.12", "Programming Language :: Python :: 3.13", "Topic :: Scientific/Engineering :: Quantum Computing", "Topic :: Software Development :: Libraries :: Python Modules", "Typing :: Typed"], "description": "<div align=\"center\">\n<img width=\"200px\" alt=\"Cirq logo\"\nsrc=\"https://raw.githubusercontent.com/quantumlib/Cirq/refs/heads/main/docs/images/Cirq_logo_color.svg\">\n</div>\n\n# cirq-web\n\n[Cirq] is a Python package for writing, manipulating, and running [quantum\ncircuits](https://en.wikipedia.org/wiki/Quantum_circuit) on quantum computers\nand simulators. Cirq provides useful abstractions for dealing with today\u2019s\n[noisy intermediate-scale quantum](https://arxiv.org/abs/1801.00862) (NISQ)\ncomputers, where the details of quantum hardware are vital to achieving\nstate-of-the-art results. For more information about Cirq, please visit the\n[Cirq documentation site].\n\nThis Python module is `cirq-web`, which allows users to take advantage of\nbrowser-based 3D visualization tools and features in Cirq. `cirq-web` also\nprovides a development environment for contributors to create and add their\nown visualizations to the module.\n\n[Cirq]: https://github.com/quantumlib/cirq\n[Cirq documentation site]: https://quantumai.google/cirq\n\n## Installation\n\nThis module is built on top of [Cirq]; installing this module will\nautomatically install the `cirq-core` module and other dependencies. There are\ntwo installation options for the `cirq-web` module:\n\n*   To install the stable version of `cirq-web`, use\n\n    ```shell\n    pip install cirq-web\n    ```\n\n*   To install the latest pre-release version of `cirq-web`, use\n\n    ```shell\n    pip install --upgrade cirq-web~=1.0.dev\n    ```\n\n    (The `~=` has a special meaning to `pip` of selecting the latest version\n    compatible with the `1.*` and `dev` in the name. Despite appearances,\n    this will not install an old version 1.0 release!)\n\nIf you would like to install Cirq with all the optional modules, not just\n`cirq-web`, then instead of the above commands, use `pip install cirq` for the\nstable release or `pip install --upgrade cirq~=1.0.dev` for the latest pre-release\nversion.\n\n## Documentation\n\nDocumentation for `cirq-web` can be found in the `README` file located in the\nmodule's subdirectory in the [Cirq repository on GitHub]. To get started\nwith using Cirq in general, please refer to the [Cirq documentation site].\n\nBelow is a quick example of using `cirq-web` to generate a portable 3D\nrendering of the Bloch sphere:\n\n```python\nimport cirq\nfrom cirq_web import BlochSphere\n\n# Prepare a state\nzero_state = [1+0j, 0+0j]\nstate_vector = cirq.to_valid_state_vector(zero_state)\n\n# Create and display the Bloch sphere\nsphere = BlochSphere(state_vector=state_vector)\nsphere.generate_html_file()\n```\n\nThis will create an HTML file in the current working directory. There are\nadditional options to specify the output directory or to open the\nvisualization in a browser, for example.\n\nYou can also view and interact with a Bloch sphere in a [Google\nColab](https://colab.google.com) notebook or Jupyter notebook. Here is an\nexample:\n\n```python\nimport cirq\nfrom cirq_web import BlochSphere\n\n# Prepare a state\nzero_state = [1+0j, 0+0j]\nstate_vector = cirq.to_valid_state_vector(zero_state)\n\n# Create and display the Bloch sphere\nsphere = BlochSphere(state_vector=state_vector)\ndisplay(sphere)\n```\n\nYou can find more example Jupyter notebooks in the `cirq-web` subdirectory of\nthe [Cirq repository on GitHub].\n\nFor more information about getting help, reporting bugs, and other matters\nrelated to Cirq and the Cirq-Web integration module, please visit the [Cirq\nrepository on GitHub].\n\n[Cirq repository on GitHub]: https://github.com/quantumlib/Cirq\n\n## Disclaimer\n\nCirq is not an official Google product. Copyright 2019 The Cirq Developers.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "dynamic": ["Author", "Author-Email", "Classifier", "Description", "Description-Content-Type", "Home-Page", "Keywords", "License", "License-File", "Maintainer", "Maintainer-Email", "Requires-Dist", "Requires-Python", "Summary"], "home_page": "http://github.com/quantumlib/cirq", "keywords": "algorithms, api, cirq, google, google quantum, nisq, python, quantum, quantum algorithms, quantum circuit, quantum circuit simulator, quantum computer simulator, quantum computing, quantum development kit, quantum information, quantum programming, quantum programming language, quantum simulation, sdk, simulation", "license": "Apache 2", "license_expression": null, "license_files": ["LICENSE"], "maintainer": "Google Quantum AI open-source maintainers", "maintainer_email": "quantum-oss-maintainers@google.com", "name": "cirq-web", "package_url": "https://pypi.org/project/cirq-web/", "platform": null, "project_url": "https://pypi.org/project/cirq-web/", "project_urls": {"Homepage": "http://github.com/quantumlib/cirq"}, "provides_extra": null, "release_url": "https://pypi.org/project/cirq-web/1.7.0.dev20250728175235/", "requires_dist": ["cirq-core==1.7.0.dev20250728175235"], "requires_python": ">=3.11.0", "summary": "Web-based 3D visualization tools for Cirq.", "version": "1.7.0.dev20250728175235", "yanked": false, "yanked_reason": null}, "last_serial": 30388220, "urls": [{"comment_text": null, "digests": {"blake2b_256": "305fa496d256b1ba28b5c976dd47bdff8e2f20116e5d2f00b84051cdd6f6f2b7", "md5": "66fe0c9dab006533db06efd44f7bd1c2", "sha256": "ca6c4c4625b84512d5c51df18477716ddb23a8c59c1160ee63671456c1fd0a8e"}, "downloads": -1, "filename": "cirq_web-1.7.0.dev20250728175235-py3-none-any.whl", "has_sig": false, "md5_digest": "66fe0c9dab006533db06efd44f7bd1c2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.11.0", "size": 430762, "upload_time": "2025-07-28T17:52:52", "upload_time_iso_8601": "2025-07-28T17:52:52.614153Z", "url": "https://files.pythonhosted.org/packages/30/5f/a496d256b1ba28b5c976dd47bdff8e2f20116e5d2f00b84051cdd6f6f2b7/cirq_web-1.7.0.dev20250728175235-py3-none-any.whl", "yanked": false, "yanked_reason": null}], "vulnerabilities": []}}
